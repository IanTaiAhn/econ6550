{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d640f",
   "metadata": {},
   "source": [
    "Previous file was getting too chunky. This one has just the finalized complete pipeline for the ml feature engineered thang for 2SLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885348d4",
   "metadata": {},
   "source": [
    "##### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from linearmodels.iv import IV2SLS\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1d516",
   "metadata": {},
   "source": [
    "Thought vomit\n",
    "What are my knowns?\n",
    "\n",
    "I want to show the data initially, and then also show that clicks is endogenous to conversion rate.\n",
    "\n",
    "Once i'm there I want to show my data isn't that clean and make a cleaned dataset.\n",
    "\n",
    "So there needs to be a preprocessing step that handles bad data, and logs stuff appropriately.\n",
    "\n",
    "Now. I want to make the interaction terms and such.\n",
    "\n",
    "Once that is all done I will create a machine learning instrument from enhanced features.\n",
    "\n",
    "Then it seems I'm foggy on how the instrument creation and 2sls will work with each other.\n",
    "\n",
    "Plus the whole point of this is to try to integrate Raj Chetty's 2014 forecast bias with value-added estimates into this somehow.\n",
    "\n",
    "I want to make sure my entire implementation is proper. Even if the results are null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee7a9e",
   "metadata": {},
   "source": [
    "##### Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63393a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data before analysis.\n",
    "    \n",
    "    Performs:\n",
    "    1. Handle negative income values\n",
    "    2. Impute missing income with median\n",
    "    3. Winsorize income at 1st and 99th percentiles\n",
    "    4. Filter age to plausible range (10-90 years)\n",
    "    5. Create logarithmic transformations for skewed variables\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. CLEAN INCOME\n",
    "    # =====================================================================\n",
    "    if 'Income' in df.columns:\n",
    "        # Convert negative income to missing\n",
    "        neg_income_count = (df['Income'] < 0).sum()\n",
    "        df.loc[df['Income'] < 0, 'Income'] = np.nan\n",
    "        \n",
    "        if neg_income_count > 0:\n",
    "            print(f\"âœ“ Converted {neg_income_count} negative income values to missing\")\n",
    "        \n",
    "        # Impute missing income with median\n",
    "        missing_income = df['Income'].isna().sum()\n",
    "        if missing_income > 0:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df['Income'] = imputer.fit_transform(df[['Income']])\n",
    "            print(f\"âœ“ Imputed {missing_income} missing income values with median\")\n",
    "        \n",
    "        # Winsorize: Cap extremes at 1st and 99th percentile\n",
    "        lower, upper = df['Income'].quantile([0.01, 0.99])\n",
    "        income_before = df['Income'].copy()\n",
    "        df['Income'] = df['Income'].clip(lower, upper)\n",
    "        winsorized = (income_before != df['Income']).sum()\n",
    "        print(f\"âœ“ Winsorized {winsorized} income values at 1st/99th percentiles\")\n",
    "        print(f\"  Income range: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. FILTER AGE\n",
    "    # =====================================================================\n",
    "    if 'Age' in df.columns:\n",
    "        age_before = len(df)\n",
    "        df = df[df['Age'].between(10, 90)]\n",
    "        age_filtered = age_before - len(df)\n",
    "        if age_filtered > 0:\n",
    "            print(f\"âœ“ Filtered {age_filtered} rows with implausible ages (keeping 10-90)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. CREATE LOGARITHMIC TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    print(f\"\\nðŸ“Š Creating logarithmic transformations:\")\n",
    "    \n",
    "    # Log of Income (if positive)\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'])\n",
    "        print(f\"  âœ“ Income_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of Clicks (if exists and positive)\n",
    "    if 'Clicks' in df.columns:\n",
    "        df['Clicks_log'] = np.log1p(df['Clicks'])\n",
    "        print(f\"  âœ“ Clicks_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of Age (for nonlinear age effects)\n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_log'] = np.log1p(df['Age'])\n",
    "        print(f\"  âœ“ Age_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of CTR (if exists and positive)\n",
    "    if 'CTR' in df.columns:\n",
    "        # Ensure CTR is positive before log\n",
    "        if (df['CTR'] > 0).all():\n",
    "            df['CTR_log'] = np.log(df['CTR'])\n",
    "            print(f\"  âœ“ CTR_log created (log transformation)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SUMMARY\n",
    "    # =====================================================================\n",
    "    final_rows = len(df)\n",
    "    rows_removed = initial_rows - final_rows\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLEANING SUMMARY:\")\n",
    "    print(f\"  Initial rows:        {initial_rows:,}\")\n",
    "    print(f\"  Final rows:          {final_rows:,}\")\n",
    "    print(f\"  Rows removed:        {rows_removed:,} ({rows_removed/initial_rows*100:.1f}%)\")\n",
    "    print(f\"  Log variables added: {len([col for col in df.columns if '_log' in col])}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def engineer_time_features(df):\n",
    "    \"\"\"Extract day of week and hour from Click_Time\"\"\"\n",
    "    if 'Click_Time' in df.columns:\n",
    "        df['Click_Time'] = pd.to_datetime(df['Click_Time'])\n",
    "        df['Day_of_Week'] = df['Click_Time'].dt.dayofweek\n",
    "        df['Hour'] = df['Click_Time'].dt.hour\n",
    "    return df\n",
    "    \n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical variables\"\"\"\n",
    "    categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            # df[col] = le # This line was wierd, idk why it did this, but i may have to revert things back.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# gonna hold off on this interaction term stuff for now.\n",
    "def engineer_instrument_features(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Create rich features that predict clicks but don't directly affect conversions.\n",
    "    \n",
    "    This is crucial for instrument strength. We create:\n",
    "    1. Interaction features between ad characteristics and demographics\n",
    "    2. Time-based features (weekend, business hours)\n",
    "    3. Nonlinear transformations\n",
    "    4. Complex interactions between multiple variables\n",
    "    \n",
    "    Key principle: These features should predict CLICKS well, but only affect\n",
    "    CONVERSIONS through clicks (exclusion restriction).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. AD CHARACTERISTICS Ã— DEMOGRAPHICS INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics respond differently to ad types\n",
    "    \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Type_encoded']):\n",
    "        df['Income_x_AdType'] = df['Income'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Income Ã— Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Topic_encoded']):\n",
    "        df['Age_x_AdTopic'] = df['Age'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Age Ã— Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded']):\n",
    "        df['Income_x_Placement'] = df['Income'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Income Ã— Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Placement_encoded']):\n",
    "        df['Age_x_Placement'] = df['Age'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Age Ã— Ad Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. TIME-BASED FEATURES AND INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Click patterns vary by time of day/week\n",
    "    \n",
    "    if 'Day_of_Week' in df.columns:\n",
    "        df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "        print(\"âœ“ Created Weekend indicator\")\n",
    "        \n",
    "    if 'Hour' in df.columns:\n",
    "        df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "        df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "        df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "        print(\"âœ“ Created time-of-day indicators\")\n",
    "    \n",
    "    # Time Ã— Ad interactions\n",
    "    if all(col in df.columns for col in ['Weekend', 'Ad_Type_encoded']):\n",
    "        df['Weekend_x_AdType'] = df['Weekend'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Weekend Ã— Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['BusinessHours', 'Ad_Placement_encoded']):\n",
    "        df['BusinessHours_x_Placement'] = df['BusinessHours'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Business Hours Ã— Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Evening', 'Ad_Topic_encoded']):\n",
    "        df['Evening_x_AdTopic'] = df['Evening'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Evening Ã— Ad Topic interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. DEMOGRAPHICS Ã— TIME INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics have different browsing patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Hour']):\n",
    "        df['Age_x_Hour'] = df['Age'] * df['Hour']\n",
    "        print(\"âœ“ Created Age Ã— Hour interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Weekend']):\n",
    "        df['Income_x_Weekend'] = df['Income'] * df['Weekend']\n",
    "        print(\"âœ“ Created Income Ã— Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'BusinessHours']):\n",
    "        df['Age_x_BusinessHours'] = df['Age'] * df['BusinessHours']\n",
    "        print(\"âœ“ Created Age Ã— Business Hours interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 4. NONLINEAR TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Relationships may be nonlinear (using log-transformed versions)\n",
    "    \n",
    "    if 'Age_log' in df.columns:\n",
    "        df['Age_squared'] = df['Age'] ** 2\n",
    "        print(\"âœ“ Created Age squared\")\n",
    "        \n",
    "    if 'Income_log' in df.columns:\n",
    "        df['Income_squared'] = df['Income'] ** 2\n",
    "        df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "        print(\"âœ“ Created Income squared and sqrt\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 5. COMPLEX CATEGORICAL INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Certain combinations may be particularly predictive\n",
    "    \n",
    "    # Location Ã— Demographics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Age']):\n",
    "        df['Location_x_Age'] = df['Location_encoded'] * df['Age']\n",
    "        print(\"âœ“ Created Location Ã— Age interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Income']):\n",
    "        df['Location_x_Income'] = df['Location_encoded'] * df['Income']\n",
    "        print(\"âœ“ Created Location Ã— Income interaction\")\n",
    "    \n",
    "    # Location Ã— Ad characteristics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Ad_Placement_encoded']):\n",
    "        df['Location_x_Placement'] = df['Location_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Location Ã— Placement interaction\")\n",
    "    \n",
    "    # Gender Ã— Ad characteristics\n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Topic_encoded']):\n",
    "        df['Gender_x_AdTopic'] = df['Gender_encoded'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Gender Ã— Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Type_encoded']):\n",
    "        df['Gender_x_AdType'] = df['Gender_encoded'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Gender Ã— Ad Type interaction\")\n",
    "    \n",
    "    # Ad Type Ã— Placement (different placements work for different types)\n",
    "    if all(col in df.columns for col in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "        df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Ad Type Ã— Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 6. THREE-WAY INTERACTIONS (most powerful)\n",
    "    # =====================================================================\n",
    "    # Rationale: Capture complex patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "        df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "        print(\"âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "        df['Income_x_Placement_x_BizHours'] = df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "        print(\"âœ“ Created Income Ã— Placement Ã— Business Hours interaction\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ccd05",
   "metadata": {},
   "source": [
    "##### Preprocessing End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415e8b1",
   "metadata": {},
   "source": [
    "##### Create ML Instrument Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b8602ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_instrument(df, model_type='stacking', cv_folds=5, use_enhanced_features=False):\n",
    "    \"\"\"\n",
    "    Generate ML-based instrument for Clicks using ensemble methods.\n",
    "    Returns a new DataFrame with 'Clicks_predicted' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Define instrument features (strictly policy-side, not outcomes!) ---\n",
    "    base_features = [\n",
    "        'Age', 'Income',\n",
    "        'Gender_encoded', 'Location_encoded',\n",
    "        'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "        'Day_of_Week', 'Hour'\n",
    "    ]\n",
    "\n",
    "    enhanced_features = [\n",
    "        # Interactions\n",
    "        'Income_x_AdType', 'Age_x_AdTopic', 'Income_x_Placement', 'Age_x_Placement',\n",
    "        'Weekend_x_AdType', 'BusinessHours_x_Placement', 'Evening_x_AdTopic',\n",
    "        'Age_x_Hour', 'Income_x_Weekend', 'Age_x_BusinessHours',\n",
    "        'Location_x_Age', 'Location_x_Income', 'Location_x_Placement',\n",
    "        'Gender_x_AdTopic', 'Gender_x_AdType', 'AdType_x_Placement',\n",
    "        'Age_x_AdType_x_Weekend', 'Income_x_Placement_x_BizHours',\n",
    "        # Time features\n",
    "        'Weekend', 'BusinessHours', 'Evening', 'Morning',\n",
    "        # Nonlinear (now using cleaned log versions) NOTE only created if i log age, and income...\n",
    "        'Age_squared', 'Age_log', 'Income_log', 'Income_squared', 'Income_sqrt',\n",
    "        'Clicks_log', 'CTR_log'\n",
    "    ]\n",
    "\n",
    "    if use_enhanced_features:\n",
    "        # Assume you have a separate function to engineer features\n",
    "        df = engineer_instrument_features(df)\n",
    "        instrument_features = base_features + enhanced_features\n",
    "    else:\n",
    "        instrument_features = base_features\n",
    "\n",
    "    # Filter available features\n",
    "    available_features = [f for f in instrument_features if f in df.columns]\n",
    "    X = df[available_features]\n",
    "    y = df['Clicks']\n",
    "\n",
    "    # --- Step 2: Build model ---\n",
    "    if model_type == 'stacking':\n",
    "        base_models = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=200, random_state=42))\n",
    "        ]\n",
    "        try:\n",
    "            from xgboost import XGBRegressor\n",
    "            base_models.append(('xgb', XGBRegressor(n_estimators=200, random_state=42, n_jobs=-1)))\n",
    "        except ImportError:\n",
    "            pass\n",
    "        model = StackingRegressor(estimators=base_models, final_estimator=Ridge(alpha=1.0), cv=cv_folds, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'rf':\n",
    "        model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'gb':\n",
    "        model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 'stacking', 'rf', or 'gb'.\")\n",
    "\n",
    "    # --- Step 3: Generate out-of-fold predictions ---\n",
    "    clicks_pred = cross_val_predict(model, X, y, cv=cv_folds, n_jobs=-1)\n",
    "    df = df.copy()\n",
    "    df['Clicks_predicted'] = clicks_pred\n",
    "\n",
    "    # --- Step 4: Fit final model (optional, for diagnostics) ---\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return df, X, model\n",
    "\n",
    "def enhanced_instrument_diagnostics(df, X, y, model):\n",
    "    \"\"\"\n",
    "    Functional: Comprehensive instrument strength testing with Stock-Yogo critical values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'Clicks' and 'Clicks_predicted' columns.\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix used in first-stage model.\n",
    "    y : pd.Series or np.array\n",
    "        True clicks (endogenous regressor).\n",
    "    model : fitted sklearn model\n",
    "        First-stage ML model used to generate instruments.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract instrument (Z) and endogenous regressor (D) ---\n",
    "    if 'Clicks_predicted' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Clicks_predicted' column\")\n",
    "    z = df['Clicks_predicted'].values\n",
    "    d = df['Clicks'].values\n",
    "\n",
    "    n = len(d)\n",
    "    k = X.shape[1]\n",
    "\n",
    "    # --- First-stage RÂ² and F-statistic ---\n",
    "    d_resid = d - d.mean()\n",
    "    ss_tot = np.sum(d_resid**2)\n",
    "    ss_res = np.sum((d - z)**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    f_stat = (r_squared / 1) / ((1 - r_squared) / (n - k - 1))\n",
    "\n",
    "    # --- Correlation ---\n",
    "    corr = np.corrcoef(z, d)[0, 1]\n",
    "\n",
    "    # --- Cragg-Donald statistic ---\n",
    "    cragg_donald = n * r_squared\n",
    "\n",
    "    # --- Display results ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nSAMPLE INFORMATION:\")\n",
    "    print(f\"  Sample size (n):              {n:,}\")\n",
    "    print(f\"  Number of features (k):       {k}\")\n",
    "    print(\"\\nFIRST-STAGE PERFORMANCE:\")\n",
    "    print(f\"  R-squared:                    {r_squared:.4f}\")\n",
    "    print(f\"  Correlation (Z, D):           {corr:.4f}\")\n",
    "    print(f\"  F-statistic:                  {f_stat:.2f}\")\n",
    "    print(f\"  Cragg-Donald statistic:       {cragg_donald:.2f}\")\n",
    "\n",
    "    print(\"\\nBENCHMARKS & INTERPRETATION:\")\n",
    "    print(f\"  {'Criterion':<35} {'Threshold':<12} {'Status'}\")\n",
    "    print(f\"  {'-'*35} {'-'*12} {'-'*20}\")\n",
    "    weak_status = \"âœ“ STRONG\" if f_stat > 10 else \"âœ— WEAK\"\n",
    "    print(f\"  {'Weak Instrument (F < 10)':<35} {'10.00':<12} {weak_status}\")\n",
    "    sy_10_status = \"âœ“âœ“ EXCELLENT\" if f_stat > 16.38 else \"âœ— Below threshold\"\n",
    "    sy_15_status = \"âœ“ GOOD\" if f_stat > 8.96 else \"âœ— Below threshold\"\n",
    "    print(f\"  {'Stock-Yogo 10% max bias':<35} {'16.38':<12} {sy_10_status}\")\n",
    "    print(f\"  {'Stock-Yogo 15% max bias':<35} {'8.96':<12} {sy_15_status}\")\n",
    "\n",
    "    print(\"\\nOVERALL ASSESSMENT:\")\n",
    "    if f_stat > 16.38:\n",
    "        print(\"  âœ“âœ“ VERY STRONG INSTRUMENT\")\n",
    "        print(\"     Maximum IV bias < 10% of OLS bias\")\n",
    "    elif f_stat > 10:\n",
    "        print(\"  âœ“ STRONG INSTRUMENT\")\n",
    "        print(\"     Acceptable for causal inference\")\n",
    "    elif f_stat > 5:\n",
    "        print(\"  âš  MODERATELY WEAK INSTRUMENT\")\n",
    "        print(\"     Proceed with caution\")\n",
    "    else:\n",
    "        print(\"  âœ— WEAK INSTRUMENT\")\n",
    "        print(\"     Results may be unreliable\")\n",
    "\n",
    "    # --- Feature importance (if available) ---\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING CLICKS:\")\n",
    "        importances = model.feature_importances_\n",
    "        top_features = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for i, (feat, imp) in enumerate(top_features, 1):\n",
    "            print(f\"  {i:2d}. {feat:35s} {imp:.4f}\")\n",
    "    elif hasattr(model, 'final_estimator_'):\n",
    "        print(\"\\nâ„¹ Stacking ensemble used - feature importances not directly available\")\n",
    "\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5b313",
   "metadata": {},
   "source": [
    "##### Create ML Instrument End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5d99",
   "metadata": {},
   "source": [
    "##### 2SLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "486a9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=None,\n",
    "    include_interactions=False,\n",
    "    add_constant=True,\n",
    "    cluster_col=None,\n",
    "    cov_type='robust'  # 'robust' for HC, or 'cluster' if cluster_col is set\n",
    "):\n",
    "    \"\"\"\n",
    "    Functional 2SLS using linearmodels.iv.IV2SLS.\n",
    "\n",
    "    Model:\n",
    "        First stage: D = Ï€0 + Ï€1 Z + Î“ X + Î½\n",
    "        Second stage: Y = Î± + Î² D + Î˜ X + Îµ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain y_col, d_col, z_col, and any control columns.\n",
    "    y_col : str\n",
    "        Outcome column (e.g., 'Conversion_Rate').\n",
    "    d_col : str\n",
    "        Endogenous regressor (e.g., 'Clicks').\n",
    "    z_col : str\n",
    "        Instrument column (e.g., 'Clicks_predicted' from ML first stage).\n",
    "    base_controls : list[str] or None\n",
    "        Exogenous controls. Do NOT include outcome-adjacent variables like CTR.\n",
    "        Recommended: ['Age', 'Income', 'Gender_encoded', 'Location_encoded',\n",
    "                      'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'].\n",
    "    include_interactions : bool\n",
    "        If True, include Ad_Type Ã— Ad_Placement interaction (exogenous).\n",
    "    add_constant : bool\n",
    "        If True, add a constant term automatically.\n",
    "    cluster_col : str or None\n",
    "        Column name for cluster-robust SEs (e.g., 'UserID', 'CampaignID').\n",
    "    cov_type : str\n",
    "        'robust' (HC), 'cluster' (requires cluster_col), or 'unadjusted'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : IV2SLSResults\n",
    "        Fitted IV results object from linearmodels.\n",
    "    data_used : pd.DataFrame\n",
    "        DataFrame with columns actually used in estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Validate required columns ---\n",
    "    required = [y_col, d_col, z_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # --- Controls: enforce exogeneity discipline ---\n",
    "    if base_controls is None:\n",
    "        base_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "        ]\n",
    "\n",
    "    # Strict: exclude CTR or any post-click/outcome-adjacent metrics from controls\n",
    "    invalid_controls = [c for c in base_controls if c not in df.columns]\n",
    "    if invalid_controls:\n",
    "        # Warn strictly but proceed with available subset\n",
    "        print(f\"âš  Skipping missing controls: {invalid_controls}\")\n",
    "    controls = [c for c in base_controls if c in df.columns]\n",
    "\n",
    "    # --- Optional exogenous interaction ---\n",
    "    interaction_col = None\n",
    "    if include_interactions:\n",
    "        if ('Ad_Type_encoded' in df.columns) and ('Ad_Placement_encoded' in df.columns):\n",
    "            interaction_col = 'Ad_Type_x_Placement'\n",
    "            if interaction_col not in df.columns:\n",
    "                df = df.copy()\n",
    "                df[interaction_col] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "                print(\"âœ“ Added exogenous interaction: Ad_Type_x_Placement\")\n",
    "            controls.append(interaction_col)\n",
    "        else:\n",
    "            print(\"âš  Interaction requested but required columns not present; skipping.\")\n",
    "\n",
    "    # --- Build data frame used in estimation ---\n",
    "    cols_needed = [y_col, d_col, z_col] + controls\n",
    "    data = df[cols_needed].dropna().copy()\n",
    "    if data.empty:\n",
    "        raise ValueError(\"After dropping NA, no rows remain for estimation.\")\n",
    "\n",
    "    # --- Build formula for IV2SLS ---\n",
    "    # dependent ~ exog + [endog ~ instruments]\n",
    "    exog_formula = ' + '.join(controls) if controls else '1'\n",
    "    if add_constant and exog_formula != '1':\n",
    "        exog_formula = '1 + ' + exog_formula  # linearmodels adds constant via '1 +'\n",
    "    elif add_constant and exog_formula == '1':\n",
    "        # '1' already denotes constant in linearmodels formula\n",
    "        pass\n",
    "    else:\n",
    "        # No constant: use '-1' to suppress intercept if you have exog terms\n",
    "        if controls:\n",
    "            exog_formula = '-1 + ' + ' + '.join(controls)\n",
    "\n",
    "    formula = f\"{y_col} ~ {exog_formula} + [{d_col} ~ {z_col}]\"\n",
    "\n",
    "    # --- Fit IV2SLS ---\n",
    "    if cov_type == 'cluster' and (cluster_col is not None) and (cluster_col in df.columns):\n",
    "        clusters = df.loc[data.index, cluster_col]\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='clustered', clusters=clusters)\n",
    "    else:\n",
    "        if cov_type == 'cluster' and cluster_col is None:\n",
    "            print(\"âš  cov_type='cluster' requested but no cluster_col provided; defaulting to robust.\")\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='robust' if cov_type != 'unadjusted' else 'unadjusted')\n",
    "\n",
    "    # --- Strict reporting ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary)\n",
    "\n",
    "    # Optional first-stage diagnostics available via .first_stage (dict of RegressionResults)\n",
    "    # Example:\n",
    "    try:\n",
    "        fs = results.first_stage[d_col]\n",
    "        print(\"\\nFirst-stage summary (endogenous regressor: {}):\".format(d_col))\n",
    "        print(f\"  R-squared: {fs.rsquared:.4f}\")\n",
    "        print(f\"  F-statistic (excluded instrument): {getattr(fs, 'f_statistic', None)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return results, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f630374",
   "metadata": {},
   "source": [
    "##### 2SLS End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_data(n=2000):\n",
    "    \"\"\"Generate synthetic data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Age': np.random.randint(18, 65, n),\n",
    "        'Gender': np.random.choice(['M', 'F'], n),\n",
    "        'Income': np.random.randint(30000, 150000, n),\n",
    "        'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "        'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "        'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "        'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "        'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "    })\n",
    "    \n",
    "    # Normalize income to reasonable scale\n",
    "    data['Income'] = data['Income'] / 100000  # Scale to 0.3-1.5 range\n",
    "    \n",
    "    # Generate clicks with realistic structure\n",
    "    clicks_base = (\n",
    "        0.5 +  # baseline\n",
    "        0.3 * (data['Ad_Type'] == 'Video').astype(float) +\n",
    "        0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float) +\n",
    "        0.01 * data['Age'] +\n",
    "        0.2 * data['Income'] +\n",
    "        np.random.randn(n) * 0.5\n",
    "    )\n",
    "    data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "    # Generate CTR (correlated with clicks but not in instrument)\n",
    "    data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "    # Generate conversion rate with causal effect from clicks\n",
    "    # Plus confounding through unobserved factors\n",
    "    unobserved_confounder = np.random.randn(n) * 0.05\n",
    "    \n",
    "    conversion_base = (\n",
    "        0.05 +  # baseline\n",
    "        0.08 * data['Clicks'] +  # TRUE CAUSAL EFFECT\n",
    "        0.02 * data['Income'] +\n",
    "        0.005 * data['Age'] +\n",
    "        0.3 * data['CTR'] +\n",
    "        unobserved_confounder +\n",
    "        np.random.randn(n) * 0.03\n",
    "    )\n",
    "    data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "    # Add endogeneity: unobserved confounder affects clicks too\n",
    "    data['Clicks'] = data['Clicks'] + unobserved_confounder * 2\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d540b7",
   "metadata": {},
   "source": [
    "##### Start of Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be069528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORIGINAL DATASET\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  \n",
      "0 2024-01-01 00:00:00  0.607039  0.072258         0.390314  \n",
      "1 2024-01-01 01:00:00  0.501653  0.063577         0.271384  \n",
      "2 2024-01-01 02:00:00  1.000099  0.073307         0.322355  \n",
      "3 2024-01-01 03:00:00  1.599193  0.163070         0.535647  \n",
      "4 2024-01-01 04:00:00  2.066244  0.179659         0.373919  \n",
      "\n",
      "============================================================\n",
      "DATA CLEANING AND PREPROCESSING\n",
      "============================================================\n",
      "âœ“ Winsorized 100 income values at 1st/99th percentiles\n",
      "  Income range: [0, 1]\n",
      "\n",
      "ðŸ“Š Creating logarithmic transformations:\n",
      "  âœ“ Income_log created (log1p transformation)\n",
      "  âœ“ Clicks_log created (log1p transformation)\n",
      "  âœ“ Age_log created (log1p transformation)\n",
      "  âœ“ CTR_log created (log transformation)\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY:\n",
      "  Initial rows:        5,000\n",
      "  Final rows:          5,000\n",
      "  Rows removed:        0 (0.0%)\n",
      "  Log variables added: 4\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLEANED AND LOGGED DATASET\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  Income_log  \\\n",
      "0 2024-01-01 00:00:00  0.607039  0.072258         0.390314    0.629547   \n",
      "1 2024-01-01 01:00:00  0.501653  0.063577         0.271384    0.347553   \n",
      "2 2024-01-01 02:00:00  1.000099  0.073307         0.322355    0.611943   \n",
      "3 2024-01-01 03:00:00  1.599193  0.163070         0.535647    0.590383   \n",
      "4 2024-01-01 04:00:00  2.066244  0.179659         0.373919    0.899690   \n",
      "\n",
      "   Clicks_log   Age_log   CTR_log  \n",
      "0    0.474393  4.043051 -2.627509  \n",
      "1    0.406566  3.850148 -2.755507  \n",
      "2    0.693197  3.496508 -2.613103  \n",
      "3    0.955201  4.110874 -1.813575  \n",
      "4    1.120453  3.258097 -1.716694  \n",
      "\n",
      "============================================================\n",
      "TIME ENGINEERED COLUMN\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  Income_log  \\\n",
      "0 2024-01-01 00:00:00  0.607039  0.072258         0.390314    0.629547   \n",
      "1 2024-01-01 01:00:00  0.501653  0.063577         0.271384    0.347553   \n",
      "2 2024-01-01 02:00:00  1.000099  0.073307         0.322355    0.611943   \n",
      "3 2024-01-01 03:00:00  1.599193  0.163070         0.535647    0.590383   \n",
      "4 2024-01-01 04:00:00  2.066244  0.179659         0.373919    0.899690   \n",
      "\n",
      "   Clicks_log   Age_log   CTR_log  Day_of_Week  Hour  \n",
      "0    0.474393  4.043051 -2.627509            0     0  \n",
      "1    0.406566  3.850148 -2.755507            0     1  \n",
      "2    0.693197  3.496508 -2.613103            0     2  \n",
      "3    0.955201  4.110874 -1.813575            0     3  \n",
      "4    1.120453  3.258097 -1.716694            0     4  \n",
      "\n",
      "============================================================\n",
      "ENCODED CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  ...  Clicks_log   Age_log  \\\n",
      "0 2024-01-01 00:00:00  0.607039  0.072258  ...    0.474393  4.043051   \n",
      "1 2024-01-01 01:00:00  0.501653  0.063577  ...    0.406566  3.850148   \n",
      "2 2024-01-01 02:00:00  1.000099  0.073307  ...    0.693197  3.496508   \n",
      "3 2024-01-01 03:00:00  1.599193  0.163070  ...    0.955201  4.110874   \n",
      "4 2024-01-01 04:00:00  2.066244  0.179659  ...    1.120453  3.258097   \n",
      "\n",
      "    CTR_log  Day_of_Week  Hour  Gender_encoded  Location_encoded  \\\n",
      "0 -2.627509            0     0               1                 1   \n",
      "1 -2.755507            0     1               0                 0   \n",
      "2 -2.613103            0     2               1                 0   \n",
      "3 -1.813575            0     3               1                 1   \n",
      "4 -1.716694            0     4               1                 0   \n",
      "\n",
      "   Ad_Type_encoded  Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "0                1                 3                     1  \n",
      "1                1                 3                     0  \n",
      "2                0                 2                     0  \n",
      "3                1                 1                     2  \n",
      "4                1                 0                     0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "============================================================\n",
      "DESCRIPTION OF DF AFTER PREPROCESSING\n",
      "============================================================\n",
      "               Age Gender       Income  Location Ad_Type Ad_Topic  \\\n",
      "count   5000.00000   5000  5000.000000      5000    5000     5000   \n",
      "unique         NaN      2          NaN         3       3        4   \n",
      "top            NaN      F          NaN  Suburban  Banner  Fashion   \n",
      "freq           NaN   2526          NaN      1701    1686     1309   \n",
      "mean      41.16820    NaN     0.903536       NaN     NaN      NaN   \n",
      "min       18.00000    NaN     0.309159       NaN     NaN      NaN   \n",
      "25%       29.00000    NaN     0.594980       NaN     NaN      NaN   \n",
      "50%       41.00000    NaN     0.912505       NaN     NaN      NaN   \n",
      "75%       53.00000    NaN     1.205272       NaN     NaN      NaN   \n",
      "max       64.00000    NaN     1.486421       NaN     NaN      NaN   \n",
      "std       13.53105    NaN     0.348928       NaN     NaN      NaN   \n",
      "\n",
      "       Ad_Placement           Click_Time       Clicks          CTR  ...  \\\n",
      "count          5000                 5000  5000.000000  5000.000000  ...   \n",
      "unique            3                  NaN          NaN          NaN  ...   \n",
      "top         Display                  NaN          NaN          NaN  ...   \n",
      "freq           1720                  NaN          NaN          NaN  ...   \n",
      "mean            NaN  2024-04-14 03:30:00     1.262149     0.126370  ...   \n",
      "min             NaN  2024-01-01 00:00:00    -0.138999     0.005043  ...   \n",
      "25%             NaN  2024-02-22 01:45:00     0.886442     0.077800  ...   \n",
      "50%             NaN  2024-04-14 03:30:00     1.258780     0.117304  ...   \n",
      "75%             NaN  2024-06-05 05:15:00     1.625449     0.165359  ...   \n",
      "max             NaN  2024-07-27 07:00:00     3.343517     0.395278  ...   \n",
      "std             NaN                  NaN     0.537903     0.065565  ...   \n",
      "\n",
      "         Clicks_log      Age_log      CTR_log  Day_of_Week         Hour  \\\n",
      "count   5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
      "unique          NaN          NaN          NaN          NaN          NaN   \n",
      "top             NaN          NaN          NaN          NaN          NaN   \n",
      "freq            NaN          NaN          NaN          NaN          NaN   \n",
      "mean       0.786046     3.684198    -2.234789     2.979200    11.487200   \n",
      "min       -0.149659     2.944439    -5.289685     0.000000     0.000000   \n",
      "25%        0.634693     3.401197    -2.553614     1.000000     5.000000   \n",
      "50%        0.814825     3.737670    -2.142986     3.000000    11.000000   \n",
      "75%        0.965252     3.988984    -1.799638     5.000000    17.000000   \n",
      "max        1.468684     4.174387    -0.928165     6.000000    23.000000   \n",
      "std        0.252072     0.349952     0.644122     1.994082     6.925332   \n",
      "\n",
      "        Gender_encoded  Location_encoded  Ad_Type_encoded  Ad_Topic_encoded  \\\n",
      "count      5000.000000       5000.000000      5000.000000       5000.000000   \n",
      "unique             NaN               NaN              NaN               NaN   \n",
      "top                NaN               NaN              NaN               NaN   \n",
      "freq               NaN               NaN              NaN               NaN   \n",
      "mean          0.494800          1.000600         0.992400          1.489800   \n",
      "min           0.000000          0.000000         0.000000          0.000000   \n",
      "25%           0.000000          0.000000         0.000000          0.000000   \n",
      "50%           0.000000          1.000000         1.000000          1.000000   \n",
      "75%           1.000000          2.000000         2.000000          3.000000   \n",
      "max           1.000000          2.000000         2.000000          3.000000   \n",
      "std           0.500023          0.812362         0.816625          1.132675   \n",
      "\n",
      "        Ad_Placement_encoded  \n",
      "count            5000.000000  \n",
      "unique                   NaN  \n",
      "top                      NaN  \n",
      "freq                     NaN  \n",
      "mean                0.991600  \n",
      "min                 0.000000  \n",
      "25%                 0.000000  \n",
      "50%                 1.000000  \n",
      "75%                 2.000000  \n",
      "max                 2.000000  \n",
      "std                 0.824418  \n",
      "\n",
      "[11 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and preprocessing\n",
    "# df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "df = generate_example_data(n=5000)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ORIGINAL DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = clean_data(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CLEANED AND LOGGED DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = engineer_time_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('TIME ENGINEERED COLUMN')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = encode_categorical_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ENCODED CATEGORICAL VARIABLES')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('DESCRIPTION OF DF AFTER PREPROCESSING')\n",
    "print(\"=\"*60)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c214b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING ML INSTRUMENT\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\n",
      "============================================================\n",
      "âœ“ Created Income Ã— Ad Type interaction\n",
      "âœ“ Created Age Ã— Ad Topic interaction\n",
      "âœ“ Created Income Ã— Ad Placement interaction\n",
      "âœ“ Created Age Ã— Ad Placement interaction\n",
      "âœ“ Created Weekend indicator\n",
      "âœ“ Created time-of-day indicators\n",
      "âœ“ Created Weekend Ã— Ad Type interaction\n",
      "âœ“ Created Business Hours Ã— Ad Placement interaction\n",
      "âœ“ Created Evening Ã— Ad Topic interaction\n",
      "âœ“ Created Age Ã— Hour interaction\n",
      "âœ“ Created Income Ã— Weekend interaction\n",
      "âœ“ Created Age Ã— Business Hours interaction\n",
      "âœ“ Created Age squared\n",
      "âœ“ Created Income squared and sqrt\n",
      "âœ“ Created Location Ã— Age interaction\n",
      "âœ“ Created Location Ã— Income interaction\n",
      "âœ“ Created Location Ã— Placement interaction\n",
      "âœ“ Created Gender Ã— Ad Topic interaction\n",
      "âœ“ Created Gender Ã— Ad Type interaction\n",
      "âœ“ Created Ad Type Ã— Placement interaction\n",
      "âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\n",
      "âœ“ Created Income Ã— Placement Ã— Business Hours interaction\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML instrument creation took about 1m40secs with real data\n",
    "# ML instrument creation took about 3m8secs with synthetic data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CREATING ML INSTRUMENT')\n",
    "print(\"=\"*60)\n",
    "df, X, model = create_ml_instrument(df, use_enhanced_features=True)\n",
    "# NOTE Remember that the instrument is really weak when created not using the interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b41b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ML INSTRUMENT DIAGNOSTICS\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "SAMPLE INFORMATION:\n",
      "  Sample size (n):              5,000\n",
      "  Number of features (k):       38\n",
      "\n",
      "FIRST-STAGE PERFORMANCE:\n",
      "  R-squared:                    0.9998\n",
      "  Correlation (Z, D):           0.9999\n",
      "  F-statistic:                  30484680.67\n",
      "  Cragg-Donald statistic:       4999.19\n",
      "\n",
      "BENCHMARKS & INTERPRETATION:\n",
      "  Criterion                           Threshold    Status\n",
      "  ----------------------------------- ------------ --------------------\n",
      "  Weak Instrument (F < 10)            10.00        âœ“ STRONG\n",
      "  Stock-Yogo 10% max bias             16.38        âœ“âœ“ EXCELLENT\n",
      "  Stock-Yogo 15% max bias             8.96         âœ“ GOOD\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  âœ“âœ“ VERY STRONG INSTRUMENT\n",
      "     Maximum IV bias < 10% of OLS bias\n",
      "\n",
      "â„¹ Stacking ensemble used - feature importances not directly available\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# diagnostics for ml instrument strength\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ML INSTRUMENT DIAGNOSTICS')\n",
    "print(\"=\"*60)\n",
    "enhanced_instrument_diagnostics(df, X, df['Clicks'], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added exogenous interaction: Ad_Type_x_Placement\n",
      "\n",
      "======================================================================\n",
      "2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\n",
      "======================================================================\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:        Conversion_Rate   R-squared:                      0.8133\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.8130\n",
      "No. Observations:                5000   F-statistic:                 2.179e+04\n",
      "Date:                Wed, Nov 12 2025   P-value (F-stat)                0.0000\n",
      "Time:                        11:47:13   Distribution:                  chi2(9)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                                  Parameter Estimates                                   \n",
      "========================================================================================\n",
      "                      Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                0.0393     0.0038     10.473     0.0000      0.0319      0.0466\n",
      "Age                      0.0049  5.484e-05     89.383     0.0000      0.0048      0.0050\n",
      "Income                   0.0172     0.0021     8.1586     0.0000      0.0131      0.0214\n",
      "Gender_encoded          -0.0002     0.0014    -0.1632     0.8704     -0.0030      0.0026\n",
      "Location_encoded     -3.467e-05     0.0009    -0.0393     0.9687     -0.0018      0.0017\n",
      "Ad_Type_encoded         -0.0021     0.0014    -1.4625     0.1436     -0.0049      0.0007\n",
      "Ad_Topic_encoded         0.0008     0.0006     1.2025     0.2292     -0.0005      0.0020\n",
      "Ad_Placement_encoded    -0.0012     0.0014    -0.8551     0.3925     -0.0039      0.0015\n",
      "Ad_Type_x_Placement  -6.697e-05     0.0011    -0.0627     0.9500     -0.0022      0.0020\n",
      "Clicks                   0.1252     0.0015     86.101     0.0000      0.1223      0.1280\n",
      "========================================================================================\n",
      "\n",
      "Endogenous: Clicks\n",
      "Instruments: Clicks_predicted\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# 2SLS with ml featured instrument\n",
    "# The r-squared value is super tiny with real data for whatever reason.\n",
    "# However, with my synthetic data I actually get a good r-squared score and a small p-value.\n",
    "# Perhpas the move is to end the real data stuff there, but continue with Raj Chetty methodlogies with the\n",
    "# synthetic data for reaserch sake.\n",
    "\n",
    "controls = [\n",
    "    'Age', 'Income',\n",
    "    'Gender_encoded', 'Location_encoded',\n",
    "    'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "]\n",
    "\n",
    "results, data_used = run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=controls,\n",
    "    include_interactions=True,   # optional\n",
    "    add_constant=True,\n",
    "    cov_type='robust'            # or 'cluster' with cluster_col='CampaignID'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8966eda",
   "metadata": {},
   "source": [
    "##### End of Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
