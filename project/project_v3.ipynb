{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d640f",
   "metadata": {},
   "source": [
    "Previous file was getting too chunky. This one has just the finalized complete pipeline for the ml feature engineered thang for 2SLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885348d4",
   "metadata": {},
   "source": [
    "##### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1e48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from linearmodels.iv import IV2SLS\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1d516",
   "metadata": {},
   "source": [
    "Thought vomit\n",
    "What are my knowns?\n",
    "\n",
    "I want to show the data initially, and then also show that clicks is endogenous to conversion rate.\n",
    "\n",
    "Once i'm there I want to show my data isn't that clean and make a cleaned dataset.\n",
    "\n",
    "So there needs to be a preprocessing step that handles bad data, and logs stuff appropriately.\n",
    "\n",
    "Now. I want to make the interaction terms and such.\n",
    "\n",
    "Once that is all done I will create a machine learning instrument from enhanced features.\n",
    "\n",
    "Then it seems I'm foggy on how the instrument creation and 2sls will work with each other.\n",
    "\n",
    "Plus the whole point of this is to try to integrate Raj Chetty's 2014 forecast bias with value-added estimates into this somehow.\n",
    "\n",
    "I want to make sure my entire implementation is proper. Even if the results are null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee7a9e",
   "metadata": {},
   "source": [
    "##### Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef0054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def clean_data_strict(df):\n",
    "    \"\"\"\n",
    "    Clean ONLY exogenous variables. Do NOT touch endogenous variables.\n",
    "    \n",
    "    Rule: If it's measured AFTER ad exposure, don't transform it here.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA CLEANING (EXOGENOUS VARIABLES ONLY)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== INCOME =====\n",
    "    if 'Income' in df.columns:\n",
    "        neg_income = (df['Income'] < 0).sum()\n",
    "        df.loc[df['Income'] < 0, 'Income'] = np.nan\n",
    "        if neg_income > 0:\n",
    "            print(f\"‚úì Converted {neg_income} negative income values to missing\")\n",
    "        \n",
    "        missing_income = df['Income'].isna().sum()\n",
    "        if missing_income > 0:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df['Income'] = imputer.fit_transform(df[['Income']])\n",
    "            print(f\"‚úì Imputed {missing_income} missing income values\")\n",
    "        \n",
    "        lower, upper = df['Income'].quantile([0.01, 0.99])\n",
    "        df['Income'] = df['Income'].clip(lower, upper)\n",
    "        print(f\"‚úì Winsorized income: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "    \n",
    "    # ===== AGE =====\n",
    "    if 'Age' in df.columns:\n",
    "        age_before = len(df)\n",
    "        df = df[df['Age'].between(10, 90)]\n",
    "        age_filtered = age_before - len(df)\n",
    "        if age_filtered > 0:\n",
    "            print(f\"‚úì Filtered {age_filtered} implausible ages\")\n",
    "    \n",
    "    # ===== LOG TRANSFORMS (EXOGENOUS ONLY) =====\n",
    "    print(f\"\\nüìä Log transformations:\")\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'])\n",
    "        print(f\"  ‚úì Income_log\")\n",
    "    \n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_log'] = np.log1p(df['Age'])\n",
    "        print(f\"  ‚úì Age_log\")\n",
    "    \n",
    "    # ===== SUMMARY =====\n",
    "    final_rows = len(df)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Rows: {initial_rows:,} ‚Üí {final_rows:,} (removed {initial_rows-final_rows:,})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_time_features_enhanced(df):\n",
    "    \"\"\"\n",
    "    Extract ALL temporal features that might predict clicks.\n",
    "    \"\"\"\n",
    "    if 'Click_Time' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TIME FEATURE ENGINEERING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df['Click_Time'] = pd.to_datetime(df['Click_Time'])\n",
    "    \n",
    "    # Basic time features\n",
    "    df['Day_of_Week'] = df['Click_Time'].dt.dayofweek\n",
    "    df['Hour'] = df['Click_Time'].dt.hour\n",
    "    df['Month'] = df['Click_Time'].dt.month\n",
    "    df['Day_of_Month'] = df['Click_Time'].dt.day\n",
    "    df['Week_of_Year'] = df['Click_Time'].dt.isocalendar().week\n",
    "    \n",
    "    # Binary indicators\n",
    "    df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "    df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "    df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "    df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "    df['Night'] = ((df['Hour'] >= 0) & (df['Hour'] <= 5)).astype(int)\n",
    "    df['Lunch'] = ((df['Hour'] >= 12) & (df['Hour'] <= 13)).astype(int)\n",
    "    \n",
    "    # Seasonal indicators\n",
    "    df['Is_Summer'] = df['Month'].isin([6, 7, 8]).astype(int)\n",
    "    df['Is_Winter'] = df['Month'].isin([12, 1, 2]).astype(int)\n",
    "    df['Is_Holiday_Season'] = df['Month'].isin([11, 12]).astype(int)\n",
    "    \n",
    "    # Paycheck cycles\n",
    "    df['Beginning_of_Month'] = (df['Day_of_Month'] <= 5).astype(int)\n",
    "    df['End_of_Month'] = (df['Day_of_Month'] >= 25).astype(int)\n",
    "    \n",
    "    # Specific high-traffic days\n",
    "    df['Is_Monday'] = (df['Day_of_Week'] == 0).astype(int)\n",
    "    df['Is_Friday'] = (df['Day_of_Week'] == 4).astype(int)\n",
    "    \n",
    "    print(f\"‚úì Created {len([c for c in df.columns if c not in ['Click_Time']])} time features\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical variables.\"\"\"\n",
    "    categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CATEGORICAL ENCODING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            print(f\"‚úì Encoded {col}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_instrument_features_comprehensive(df):\n",
    "    \"\"\"\n",
    "    Create ALL possible exogenous interactions for maximum instrument strength.\n",
    "    \n",
    "    Categories:\n",
    "    1. Demographics √ó Ad Characteristics\n",
    "    2. Demographics √ó Time\n",
    "    3. Ad Characteristics √ó Time\n",
    "    4. Location-specific interactions\n",
    "    5. Nonlinear transformations\n",
    "    6. Complex three-way interactions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE INSTRUMENT FEATURE ENGINEERING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_count = 0\n",
    "    \n",
    "    # ================================================================\n",
    "    # 1. DEMOGRAPHICS √ó AD CHARACTERISTICS\n",
    "    # ================================================================\n",
    "    print(\"\\n[1] Demographics √ó Ad Characteristics:\")\n",
    "    \n",
    "    demo_cols = ['Age', 'Income', 'Gender_encoded']\n",
    "    ad_cols = ['Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded']\n",
    "    \n",
    "    for demo in demo_cols:\n",
    "        for ad in ad_cols:\n",
    "            if all(c in df.columns for c in [demo, ad]):\n",
    "                feat_name = f'{demo}_x_{ad}'\n",
    "                df[feat_name] = df[demo] * df[ad]\n",
    "                feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count} interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 2. DEMOGRAPHICS √ó TIME\n",
    "    # ================================================================\n",
    "    print(\"\\n[2] Demographics √ó Time:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    time_cols = ['Weekend', 'BusinessHours', 'Evening', 'Morning', 'Hour', \n",
    "                 'Is_Monday', 'Is_Friday', 'Beginning_of_Month', 'End_of_Month']\n",
    "    \n",
    "    for demo in ['Age', 'Income']:\n",
    "        for time in time_cols:\n",
    "            if all(c in df.columns for c in [demo, time]):\n",
    "                feat_name = f'{demo}_x_{time}'\n",
    "                df[feat_name] = df[demo] * df[time]\n",
    "                feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 3. AD CHARACTERISTICS √ó TIME\n",
    "    # ================================================================\n",
    "    print(\"\\n[3] Ad Characteristics √ó Time:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    for ad in ad_cols:\n",
    "        for time in time_cols:\n",
    "            if all(c in df.columns for c in [ad, time]):\n",
    "                feat_name = f'{ad}_x_{time}'\n",
    "                df[feat_name] = df[ad] * df[time]\n",
    "                feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 4. LOCATION-SPECIFIC INTERACTIONS\n",
    "    # ================================================================\n",
    "    print(\"\\n[4] Location-specific interactions:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    if 'Location_encoded' in df.columns:\n",
    "        # Location √ó Demographics\n",
    "        for demo in ['Age', 'Income', 'Gender_encoded']:\n",
    "            if demo in df.columns:\n",
    "                df[f'Location_x_{demo}'] = df['Location_encoded'] * df[demo]\n",
    "                feature_count += 1\n",
    "        \n",
    "        # Location √ó Ad characteristics\n",
    "        for ad in ad_cols:\n",
    "            if ad in df.columns:\n",
    "                df[f'Location_x_{ad}'] = df['Location_encoded'] * df[ad]\n",
    "                feature_count += 1\n",
    "        \n",
    "        # Location √ó Time\n",
    "        for time in ['Weekend', 'BusinessHours', 'Evening']:\n",
    "            if time in df.columns:\n",
    "                df[f'Location_x_{time}'] = df['Location_encoded'] * df[time]\n",
    "                feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 5. NONLINEAR TRANSFORMATIONS\n",
    "    # ================================================================\n",
    "    print(\"\\n[5] Nonlinear transformations:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_squared'] = df['Age'] ** 2\n",
    "        df['Age_cubed'] = df['Age'] ** 3\n",
    "        df['Age_sqrt'] = np.sqrt(df['Age'])\n",
    "        feature_count += 3\n",
    "    \n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_squared'] = df['Income'] ** 2\n",
    "        df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "        feature_count += 2\n",
    "    \n",
    "    if 'Hour' in df.columns:\n",
    "        df['Hour_squared'] = df['Hour'] ** 2\n",
    "        # Cyclical encoding (hour 23 is close to hour 0)\n",
    "        df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "        df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "        feature_count += 3\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} transformations\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 6. THREE-WAY INTERACTIONS (HIGH-ORDER)\n",
    "    # ================================================================\n",
    "    print(\"\\n[6] Three-way interactions:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    # Age √ó Ad Type √ó Time\n",
    "    if all(c in df.columns for c in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "        df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "        feature_count += 1\n",
    "    \n",
    "    if all(c in df.columns for c in ['Age', 'Ad_Type_encoded', 'Evening']):\n",
    "        df['Age_x_AdType_x_Evening'] = df['Age'] * df['Ad_Type_encoded'] * df['Evening']\n",
    "        feature_count += 1\n",
    "    \n",
    "    # Income √ó Ad Placement √ó Time\n",
    "    if all(c in df.columns for c in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "        df['Income_x_Placement_x_BizHours'] = (\n",
    "            df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "        )\n",
    "        feature_count += 1\n",
    "    \n",
    "    if all(c in df.columns for c in ['Income', 'Ad_Placement_encoded', 'Weekend']):\n",
    "        df['Income_x_Placement_x_Weekend'] = (\n",
    "            df['Income'] * df['Ad_Placement_encoded'] * df['Weekend']\n",
    "        )\n",
    "        feature_count += 1\n",
    "    \n",
    "    # Location √ó Ad √ó Time\n",
    "    if all(c in df.columns for c in ['Location_encoded', 'Ad_Type_encoded', 'Evening']):\n",
    "        df['Location_x_AdType_x_Evening'] = (\n",
    "            df['Location_encoded'] * df['Ad_Type_encoded'] * df['Evening']\n",
    "        )\n",
    "        feature_count += 1\n",
    "    \n",
    "    # Gender √ó Ad Topic √ó Time\n",
    "    if all(c in df.columns for c in ['Gender_encoded', 'Ad_Topic_encoded', 'Weekend']):\n",
    "        df['Gender_x_Topic_x_Weekend'] = (\n",
    "            df['Gender_encoded'] * df['Ad_Topic_encoded'] * df['Weekend']\n",
    "        )\n",
    "        feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} three-way interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # 7. AD CHARACTERISTIC INTERACTIONS\n",
    "    # ================================================================\n",
    "    print(\"\\n[7] Ad characteristic interactions:\")\n",
    "    prev_count = feature_count\n",
    "    \n",
    "    if all(c in df.columns for c in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "        df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "        feature_count += 1\n",
    "    \n",
    "    if all(c in df.columns for c in ['Ad_Type_encoded', 'Ad_Topic_encoded']):\n",
    "        df['AdType_x_Topic'] = df['Ad_Type_encoded'] * df['Ad_Topic_encoded']\n",
    "        feature_count += 1\n",
    "    \n",
    "    if all(c in df.columns for c in ['Ad_Topic_encoded', 'Ad_Placement_encoded']):\n",
    "        df['AdTopic_x_Placement'] = df['Ad_Topic_encoded'] * df['Ad_Placement_encoded']\n",
    "        feature_count += 1\n",
    "    \n",
    "    print(f\"  Created {feature_count - prev_count} ad interactions\")\n",
    "    \n",
    "    # ================================================================\n",
    "    # SUMMARY\n",
    "    # ================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOTAL NEW FEATURES CREATED: {feature_count}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    # print(df.describe(include='all'))\n",
    "    # Select only object (string) columns\n",
    "    # string_cols = df.select_dtypes(include=['object']).columns\n",
    "    # print(string_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# def create_leave_one_out_aggregates(df):\n",
    "#     \"\"\"\n",
    "#     Create group-level aggregates using leave-one-out encoding.\n",
    "    \n",
    "#     These capture \"propensity to click\" at group level, which is exogenous\n",
    "#     for an individual observation.\n",
    "    \n",
    "#     WARNING: This is advanced and borderline. Use only if desperate for\n",
    "#     instrument strength.\n",
    "#     \"\"\"\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"LEAVE-ONE-OUT AGGREGATE FEATURES\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     if 'Clicks' not in df.columns:\n",
    "#         print(\"‚ö†Ô∏è  'Clicks' column not found, skipping aggregates\")\n",
    "#         return df\n",
    "    \n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "#     # Location-level average\n",
    "#     if 'Location' in df.columns:\n",
    "#         df['Location_AvgClicks'] = np.nan\n",
    "#         for train_idx, test_idx in kf.split(df):\n",
    "#             loc_avg = df.iloc[train_idx].groupby('Location')['Clicks'].mean()\n",
    "#             df.loc[test_idx, 'Location_AvgClicks'] = (\n",
    "#                 df.loc[test_idx, 'Location'].map(loc_avg)\n",
    "#             )\n",
    "#         print(\"‚úì Location_AvgClicks (leave-one-out)\")\n",
    "    \n",
    "#     # Age-group-level average\n",
    "#     if 'Age' in df.columns:\n",
    "#         df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 50, 100])\n",
    "#         df['AgeGroup_AvgClicks'] = np.nan\n",
    "#         for train_idx, test_idx in kf.split(df):\n",
    "#             age_avg = df.iloc[train_idx].groupby('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63393a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data before analysis.\n",
    "    \n",
    "    Performs:\n",
    "    1. Handle negative income values\n",
    "    2. Impute missing income with median\n",
    "    3. Winsorize income at 1st and 99th percentiles\n",
    "    4. Filter age to plausible range (10-90 years)\n",
    "    5. Create logarithmic transformations for skewed variables\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. CLEAN INCOME\n",
    "    # =====================================================================\n",
    "    if 'Income' in df.columns:\n",
    "        # Convert negative income to missing\n",
    "        neg_income_count = (df['Income'] < 0).sum()\n",
    "        df.loc[df['Income'] < 0, 'Income'] = np.nan\n",
    "        \n",
    "        if neg_income_count > 0:\n",
    "            print(f\"‚úì Converted {neg_income_count} negative income values to missing\")\n",
    "        \n",
    "        # Impute missing income with median\n",
    "        missing_income = df['Income'].isna().sum()\n",
    "        if missing_income > 0:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df['Income'] = imputer.fit_transform(df[['Income']])\n",
    "            print(f\"‚úì Imputed {missing_income} missing income values with median\")\n",
    "        \n",
    "        # Winsorize: Cap extremes at 1st and 99th percentile\n",
    "        lower, upper = df['Income'].quantile([0.01, 0.99])\n",
    "        income_before = df['Income'].copy()\n",
    "        df['Income'] = df['Income'].clip(lower, upper)\n",
    "        winsorized = (income_before != df['Income']).sum()\n",
    "        print(f\"‚úì Winsorized {winsorized} income values at 1st/99th percentiles\")\n",
    "        print(f\"  Income range: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. FILTER AGE\n",
    "    # =====================================================================\n",
    "    if 'Age' in df.columns:\n",
    "        age_before = len(df)\n",
    "        df = df[df['Age'].between(10, 90)]\n",
    "        age_filtered = age_before - len(df)\n",
    "        if age_filtered > 0:\n",
    "            print(f\"‚úì Filtered {age_filtered} rows with implausible ages (keeping 10-90)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. CREATE LOGARITHMIC TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    print(f\"\\nüìä Creating logarithmic transformations:\")\n",
    "    \n",
    "    # Log of Income (if positive)\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'])\n",
    "        print(f\"  ‚úì Income_log created (log1p transformation)\")\n",
    "    \n",
    "    # # Log of Clicks (if exists and positive)\n",
    "    # if 'Clicks' in df.columns:\n",
    "    #     df['Clicks_log'] = np.log1p(df['Clicks'])\n",
    "    #     print(f\"  ‚úì Clicks_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of Age (for nonlinear age effects)\n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_log'] = np.log1p(df['Age'])\n",
    "        print(f\"  ‚úì Age_log created (log1p transformation)\")\n",
    "    \n",
    "    # # Log of CTR (if exists and positive)\n",
    "    # if 'CTR' in df.columns:\n",
    "    #     # Ensure CTR is positive before log\n",
    "    #     if (df['CTR'] > 0).all():\n",
    "    #         df['CTR_log'] = np.log(df['CTR'])\n",
    "    #         print(f\"  ‚úì CTR_log created (log transformation)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SUMMARY\n",
    "    # =====================================================================\n",
    "    final_rows = len(df)\n",
    "    rows_removed = initial_rows - final_rows\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLEANING SUMMARY:\")\n",
    "    print(f\"  Initial rows:        {initial_rows:,}\")\n",
    "    print(f\"  Final rows:          {final_rows:,}\")\n",
    "    print(f\"  Rows removed:        {rows_removed:,} ({rows_removed/initial_rows*100:.1f}%)\")\n",
    "    print(f\"  Log variables added: {len([col for col in df.columns if '_log' in col])}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def engineer_time_features(df):\n",
    "    \"\"\"Extract day of week and hour from Click_Time\"\"\"\n",
    "    if 'Click_Time' in df.columns:\n",
    "        df['Click_Time'] = pd.to_datetime(df['Click_Time'])\n",
    "        df['Day_of_Week'] = df['Click_Time'].dt.dayofweek\n",
    "        df['Hour'] = df['Click_Time'].dt.hour\n",
    "    return df\n",
    "    \n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical variables\"\"\"\n",
    "    categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            # df[col] = le # This line was wierd, idk why it did this, but i may have to revert things back.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# gonna hold off on this interaction term stuff for now.\n",
    "def engineer_instrument_features(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Create rich features that predict clicks but don't directly affect conversions.\n",
    "    \n",
    "    This is crucial for instrument strength. We create:\n",
    "    1. Interaction features between ad characteristics and demographics\n",
    "    2. Time-based features (weekend, business hours)\n",
    "    3. Nonlinear transformations\n",
    "    4. Complex interactions between multiple variables\n",
    "    \n",
    "    Key principle: These features should predict CLICKS well, but only affect\n",
    "    CONVERSIONS through clicks (exclusion restriction).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. AD CHARACTERISTICS √ó DEMOGRAPHICS INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics respond differently to ad types\n",
    "    \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Type_encoded']):\n",
    "        df['Income_x_AdType'] = df['Income'] * df['Ad_Type_encoded']\n",
    "        print(\"‚úì Created Income √ó Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Topic_encoded']):\n",
    "        df['Age_x_AdTopic'] = df['Age'] * df['Ad_Topic_encoded']\n",
    "        print(\"‚úì Created Age √ó Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded']):\n",
    "        df['Income_x_Placement'] = df['Income'] * df['Ad_Placement_encoded']\n",
    "        print(\"‚úì Created Income √ó Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Placement_encoded']):\n",
    "        df['Age_x_Placement'] = df['Age'] * df['Ad_Placement_encoded']\n",
    "        print(\"‚úì Created Age √ó Ad Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. TIME-BASED FEATURES AND INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Click patterns vary by time of day/week\n",
    "    \n",
    "    if 'Day_of_Week' in df.columns:\n",
    "        df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "        print(\"‚úì Created Weekend indicator\")\n",
    "        \n",
    "    if 'Hour' in df.columns:\n",
    "        df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "        df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "        df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "        print(\"‚úì Created time-of-day indicators\")\n",
    "    \n",
    "    # Time √ó Ad interactions\n",
    "    if all(col in df.columns for col in ['Weekend', 'Ad_Type_encoded']):\n",
    "        df['Weekend_x_AdType'] = df['Weekend'] * df['Ad_Type_encoded']\n",
    "        print(\"‚úì Created Weekend √ó Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['BusinessHours', 'Ad_Placement_encoded']):\n",
    "        df['BusinessHours_x_Placement'] = df['BusinessHours'] * df['Ad_Placement_encoded']\n",
    "        print(\"‚úì Created Business Hours √ó Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Evening', 'Ad_Topic_encoded']):\n",
    "        df['Evening_x_AdTopic'] = df['Evening'] * df['Ad_Topic_encoded']\n",
    "        print(\"‚úì Created Evening √ó Ad Topic interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. DEMOGRAPHICS √ó TIME INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics have different browsing patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Hour']):\n",
    "        df['Age_x_Hour'] = df['Age'] * df['Hour']\n",
    "        print(\"‚úì Created Age √ó Hour interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Weekend']):\n",
    "        df['Income_x_Weekend'] = df['Income'] * df['Weekend']\n",
    "        print(\"‚úì Created Income √ó Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'BusinessHours']):\n",
    "        df['Age_x_BusinessHours'] = df['Age'] * df['BusinessHours']\n",
    "        print(\"‚úì Created Age √ó Business Hours interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 4. NONLINEAR TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Relationships may be nonlinear (using log-transformed versions)\n",
    "    \n",
    "    if 'Age_log' in df.columns:\n",
    "        df['Age_squared'] = df['Age'] ** 2\n",
    "        print(\"‚úì Created Age squared\")\n",
    "        \n",
    "    if 'Income_log' in df.columns:\n",
    "        df['Income_squared'] = df['Income'] ** 2\n",
    "        df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "        print(\"‚úì Created Income squared and sqrt\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 5. COMPLEX CATEGORICAL INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Certain combinations may be particularly predictive\n",
    "    \n",
    "    # Location √ó Demographics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Age']):\n",
    "        df['Location_x_Age'] = df['Location_encoded'] * df['Age']\n",
    "        print(\"‚úì Created Location √ó Age interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Income']):\n",
    "        df['Location_x_Income'] = df['Location_encoded'] * df['Income']\n",
    "        print(\"‚úì Created Location √ó Income interaction\")\n",
    "    \n",
    "    # Location √ó Ad characteristics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Ad_Placement_encoded']):\n",
    "        df['Location_x_Placement'] = df['Location_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"‚úì Created Location √ó Placement interaction\")\n",
    "    \n",
    "    # Gender √ó Ad characteristics\n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Topic_encoded']):\n",
    "        df['Gender_x_AdTopic'] = df['Gender_encoded'] * df['Ad_Topic_encoded']\n",
    "        print(\"‚úì Created Gender √ó Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Type_encoded']):\n",
    "        df['Gender_x_AdType'] = df['Gender_encoded'] * df['Ad_Type_encoded']\n",
    "        print(\"‚úì Created Gender √ó Ad Type interaction\")\n",
    "    \n",
    "    # Ad Type √ó Placement (different placements work for different types)\n",
    "    if all(col in df.columns for col in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "        df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"‚úì Created Ad Type √ó Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 6. THREE-WAY INTERACTIONS (most powerful)\n",
    "    # =====================================================================\n",
    "    # Rationale: Capture complex patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "        df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "        print(\"‚úì Created Age √ó Ad Type √ó Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "        df['Income_x_Placement_x_BizHours'] = df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "        print(\"‚úì Created Income √ó Placement √ó Business Hours interaction\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ccd05",
   "metadata": {},
   "source": [
    "##### Preprocessing End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415e8b1",
   "metadata": {},
   "source": [
    "##### Create ML Instrument Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b8602ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_instrument(df, model_type='stacking', cv_folds=5, use_enhanced_features=False):\n",
    "    \"\"\"\n",
    "    Generate ML-based instrument for Clicks using ensemble methods.\n",
    "    Returns a new DataFrame with 'Clicks_predicted' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Define instrument features (strictly policy-side, not outcomes!) ---\n",
    "    base_features = [\n",
    "        'Age', 'Income',\n",
    "        'Gender_encoded', 'Location_encoded',\n",
    "        'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "        'Day_of_Week', 'Hour'\n",
    "    ]\n",
    "\n",
    "    enhanced_features = [\n",
    "        # Interactions\n",
    "        'Income_x_AdType', 'Age_x_AdTopic', 'Income_x_Placement', 'Age_x_Placement',\n",
    "        'Weekend_x_AdType', 'BusinessHours_x_Placement', 'Evening_x_AdTopic',\n",
    "        'Age_x_Hour', 'Income_x_Weekend', 'Age_x_BusinessHours',\n",
    "        'Location_x_Age', 'Location_x_Income', 'Location_x_Placement',\n",
    "        'Gender_x_AdTopic', 'Gender_x_AdType', 'AdType_x_Placement',\n",
    "        'Age_x_AdType_x_Weekend', 'Income_x_Placement_x_BizHours',\n",
    "        # Time features\n",
    "        'Weekend', 'BusinessHours', 'Evening', 'Morning',\n",
    "        # Nonlinear (now using cleaned log versions) NOTE only created if i log age, and income...\n",
    "        'Age_squared', 'Age_log', 'Income_log', 'Income_squared', 'Income_sqrt'\n",
    "    ]\n",
    "\n",
    "    # NOTE so this totally failed! But! Let's just use Lasso regularization at the end! Boom.\n",
    "    # i dont want to rename variables and idk if this works so for now it is renamed to \n",
    "    # NOTE advanced_enhanced features, but is name to enhanced_features for ease of use.\n",
    "    # enhanced_features = [\n",
    "    #     'Income_log', 'Age_log', 'Month', 'Day_of_Month', 'Week_of_Year', \n",
    "    #     'Weekend', 'BusinessHours', \n",
    "    #     'Evening', 'Morning', 'Night', 'Lunch', 'Is_Summer', 'Is_Winter', 'Is_Holiday_Season', 'Beginning_of_Month', 'End_of_Month', \n",
    "    #     'Is_Monday', 'Is_Friday',\n",
    "    #     'Age_x_Ad_Type_encoded', 'Age_x_Ad_Topic_encoded', 'Age_x_Ad_Placement_encoded', 'Income_x_Ad_Type_encoded', \n",
    "    #     'Income_x_Ad_Topic_encoded', 'Income_x_Ad_Placement_encoded', 'Gender_encoded_x_Ad_Type_encoded', \n",
    "    #     'Gender_encoded_x_Ad_Topic_encoded', 'Gender_encoded_x_Ad_Placement_encoded', 'Age_x_Weekend', 'Age_x_BusinessHours', \n",
    "    #     'Age_x_Evening', 'Age_x_Morning', 'Age_x_Hour', 'Age_x_Is_Monday', 'Age_x_Is_Friday', 'Age_x_Beginning_of_Month', \n",
    "    #     'Age_x_End_of_Month', 'Income_x_Weekend', 'Income_x_BusinessHours', 'Income_x_Evening', 'Income_x_Morning', 'Income_x_Hour', \n",
    "    #     'Income_x_Is_Monday', 'Income_x_Is_Friday', 'Income_x_Beginning_of_Month', 'Income_x_End_of_Month', 'Ad_Type_encoded_x_Weekend', \n",
    "    #     'Ad_Type_encoded_x_BusinessHours', 'Ad_Type_encoded_x_Evening', 'Ad_Type_encoded_x_Morning', 'Ad_Type_encoded_x_Hour', \n",
    "    #     'Ad_Type_encoded_x_Is_Monday', 'Ad_Type_encoded_x_Is_Friday', 'Ad_Type_encoded_x_Beginning_of_Month', \n",
    "    #     'Ad_Type_encoded_x_End_of_Month', 'Ad_Topic_encoded_x_Weekend', 'Ad_Topic_encoded_x_BusinessHours', \n",
    "    #     'Ad_Topic_encoded_x_Evening', 'Ad_Topic_encoded_x_Morning', 'Ad_Topic_encoded_x_Hour', 'Ad_Topic_encoded_x_Is_Monday', \n",
    "    #     'Ad_Topic_encoded_x_Is_Friday', 'Ad_Topic_encoded_x_Beginning_of_Month', 'Ad_Topic_encoded_x_End_of_Month', \n",
    "    #     'Ad_Placement_encoded_x_Weekend', 'Ad_Placement_encoded_x_BusinessHours', 'Ad_Placement_encoded_x_Evening', \n",
    "    #     'Ad_Placement_encoded_x_Morning', 'Ad_Placement_encoded_x_Hour', 'Ad_Placement_encoded_x_Is_Monday', \n",
    "    #     'Ad_Placement_encoded_x_Is_Friday', 'Ad_Placement_encoded_x_Beginning_of_Month', 'Ad_Placement_encoded_x_End_of_Month', \n",
    "    #     'Location_x_Age', 'Location_x_Income', 'Location_x_Gender_encoded', 'Location_x_Ad_Type_encoded', 'Location_x_Ad_Topic_encoded',\n",
    "    #     'Location_x_Ad_Placement_encoded', 'Location_x_Weekend', 'Location_x_BusinessHours', 'Location_x_Evening', 'Age_squared', \n",
    "    #     'Age_cubed', 'Age_sqrt', 'Income_squared', 'Income_sqrt', 'Hour_squared', 'Hour_sin', 'Hour_cos', 'Age_x_AdType_x_Weekend', \n",
    "    #     'Age_x_AdType_x_Evening', 'Income_x_Placement_x_BizHours', 'Income_x_Placement_x_Weekend',\n",
    "    #     'Location_x_AdType_x_Evening', 'Gender_x_Topic_x_Weekend', 'AdType_x_Placement', 'AdType_x_Topic', 'AdTopic_x_Placement'\n",
    "    # ]\n",
    "    \n",
    "    if use_enhanced_features:\n",
    "        # Assume you have a separate function to engineer features\n",
    "        # df = engineer_instrument_features(df)\n",
    "        df = engineer_instrument_features_comprehensive(df)\n",
    "\n",
    "        instrument_features = base_features + enhanced_features\n",
    "    else:\n",
    "        instrument_features = base_features\n",
    "\n",
    "    # Filter available features\n",
    "    available_features = [f for f in instrument_features if f in df.columns]\n",
    "    # print('\\n FLAG', available_features)\n",
    "    # print(df.describe(include='all'))\n",
    "    print(df.dtypes)\n",
    "\n",
    "    X = df[available_features]\n",
    "    y = df['Clicks']\n",
    "\n",
    "    # --- Step 2: Build model ---\n",
    "    if model_type == 'stacking':\n",
    "        base_models = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=200, random_state=42))\n",
    "        ]\n",
    "        try:\n",
    "            from xgboost import XGBRegressor\n",
    "            base_models.append(('xgb', XGBRegressor(n_estimators=200, random_state=42, n_jobs=-1)))\n",
    "        except ImportError:\n",
    "            pass\n",
    "        model = StackingRegressor(estimators=base_models, final_estimator=Ridge(alpha=1.0), cv=cv_folds, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'rf':\n",
    "        model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'gb':\n",
    "        model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 'stacking', 'rf', or 'gb'.\")\n",
    "\n",
    "    # --- Step 3: Generate out-of-fold predictions ---\n",
    "    clicks_pred = cross_val_predict(model, X, y, cv=cv_folds, n_jobs=-1)\n",
    "    df = df.copy()\n",
    "    df['Clicks_predicted'] = clicks_pred\n",
    "\n",
    "    # --- Step 4: Fit final model (optional, for diagnostics) ---\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return df, X, model\n",
    "\n",
    "def enhanced_instrument_diagnostics(df, X, y, model):\n",
    "    \"\"\"\n",
    "    Functional: Comprehensive instrument strength testing with Stock-Yogo critical values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'Clicks' and 'Clicks_predicted' columns.\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix used in first-stage model.\n",
    "    y : pd.Series or np.array\n",
    "        True clicks (endogenous regressor).\n",
    "    model : fitted sklearn model\n",
    "        First-stage ML model used to generate instruments.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract instrument (Z) and endogenous regressor (D) ---\n",
    "    if 'Clicks_predicted' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Clicks_predicted' column\")\n",
    "    z = df['Clicks_predicted'].values\n",
    "    d = df['Clicks'].values\n",
    "\n",
    "    n = len(d)\n",
    "    k = X.shape[1]\n",
    "\n",
    "    # --- First-stage R¬≤ and F-statistic ---\n",
    "    d_resid = d - d.mean()\n",
    "    ss_tot = np.sum(d_resid**2)\n",
    "    ss_res = np.sum((d - z)**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    f_stat = (r_squared / 1) / ((1 - r_squared) / (n - k - 1))\n",
    "\n",
    "    # --- Correlation ---\n",
    "    corr = np.corrcoef(z, d)[0, 1]\n",
    "\n",
    "    # --- Cragg-Donald statistic ---\n",
    "    cragg_donald = n * r_squared\n",
    "\n",
    "    # --- Display results ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nSAMPLE INFORMATION:\")\n",
    "    print(f\"  Sample size (n):              {n:,}\")\n",
    "    print(f\"  Number of features (k):       {k}\")\n",
    "    print(\"\\nFIRST-STAGE PERFORMANCE:\")\n",
    "    print(f\"  R-squared:                    {r_squared:.4f}\")\n",
    "    print(f\"  Correlation (Z, D):           {corr:.4f}\")\n",
    "    print(f\"  F-statistic:                  {f_stat:.2f}\")\n",
    "    print(f\"  Cragg-Donald statistic:       {cragg_donald:.2f}\")\n",
    "\n",
    "    print(\"\\nBENCHMARKS & INTERPRETATION:\")\n",
    "    print(f\"  {'Criterion':<35} {'Threshold':<12} {'Status'}\")\n",
    "    print(f\"  {'-'*35} {'-'*12} {'-'*20}\")\n",
    "    weak_status = \"‚úì STRONG\" if f_stat > 10 else \"‚úó WEAK\"\n",
    "    print(f\"  {'Weak Instrument (F < 10)':<35} {'10.00':<12} {weak_status}\")\n",
    "    sy_10_status = \"‚úì‚úì EXCELLENT\" if f_stat > 16.38 else \"‚úó Below threshold\"\n",
    "    sy_15_status = \"‚úì GOOD\" if f_stat > 8.96 else \"‚úó Below threshold\"\n",
    "    print(f\"  {'Stock-Yogo 10% max bias':<35} {'16.38':<12} {sy_10_status}\")\n",
    "    print(f\"  {'Stock-Yogo 15% max bias':<35} {'8.96':<12} {sy_15_status}\")\n",
    "\n",
    "    print(\"\\nOVERALL ASSESSMENT:\")\n",
    "    if f_stat > 16.38:\n",
    "        print(\"  ‚úì‚úì VERY STRONG INSTRUMENT\")\n",
    "        print(\"     Maximum IV bias < 10% of OLS bias\")\n",
    "    elif f_stat > 10:\n",
    "        print(\"  ‚úì STRONG INSTRUMENT\")\n",
    "        print(\"     Acceptable for causal inference\")\n",
    "    elif f_stat > 5:\n",
    "        print(\"  ‚ö† MODERATELY WEAK INSTRUMENT\")\n",
    "        print(\"     Proceed with caution\")\n",
    "    else:\n",
    "        print(\"  ‚úó WEAK INSTRUMENT\")\n",
    "        print(\"     Results may be unreliable\")\n",
    "\n",
    "    # --- Feature importance (if available) ---\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING CLICKS:\")\n",
    "        importances = model.feature_importances_\n",
    "        top_features = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for i, (feat, imp) in enumerate(top_features, 1):\n",
    "            print(f\"  {i:2d}. {feat:35s} {imp:.4f}\")\n",
    "    elif hasattr(model, 'final_estimator_'):\n",
    "        print(\"\\n‚Ñπ Stacking ensemble used - feature importances not directly available\")\n",
    "\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5b313",
   "metadata": {},
   "source": [
    "##### Create ML Instrument End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167ab0a",
   "metadata": {},
   "source": [
    "##### OLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ca114d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ols(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    base_controls=None,\n",
    "    include_interactions=False,\n",
    "    add_constant=True,\n",
    "    cluster_col=None,\n",
    "    cov_type='robust'  # 'robust' for HC, or 'cluster' if cluster_col is set\n",
    "):\n",
    "    \"\"\"\n",
    "    Na√Øve OLS regression treating Clicks as exogenous.\n",
    "    Model:\n",
    "        Y = Œ± + Œ≤ D + Œò X + Œµ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain y_col, d_col, and any control columns.\n",
    "    y_col : str\n",
    "        Outcome column (e.g., 'Conversion_Rate').\n",
    "    d_col : str\n",
    "        Regressor (treated as exogenous here).\n",
    "    base_controls : list[str] or None\n",
    "        Exogenous controls.\n",
    "    include_interactions : bool\n",
    "        If True, include Ad_Type √ó Ad_Placement interaction (exogenous).\n",
    "    add_constant : bool\n",
    "        If True, add a constant term automatically.\n",
    "    cluster_col : str or None\n",
    "        Column name for cluster-robust SEs.\n",
    "    cov_type : str\n",
    "        'robust' (HC), 'cluster' (requires cluster_col), or 'unadjusted'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : RegressionResults\n",
    "        Fitted OLS results object from statsmodels.\n",
    "    data_used : pd.DataFrame\n",
    "        DataFrame with columns actually used in estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Controls ---\n",
    "    if base_controls is None:\n",
    "        base_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "        ]\n",
    "\n",
    "    invalid_controls = [c for c in base_controls if c not in df.columns]\n",
    "    if invalid_controls:\n",
    "        print(f\"‚ö† Skipping missing controls: {invalid_controls}\")\n",
    "    controls = [c for c in base_controls if c in df.columns]\n",
    "\n",
    "    # --- Optional interaction ---\n",
    "    if include_interactions:\n",
    "        if ('Ad_Type_encoded' in df.columns) and ('Ad_Placement_encoded' in df.columns):\n",
    "            interaction_col = 'Ad_Type_x_Placement'\n",
    "            if interaction_col not in df.columns:\n",
    "                df = df.copy()\n",
    "                df[interaction_col] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "                print(\"‚úì Added exogenous interaction: Ad_Type_x_Placement\")\n",
    "            controls.append(interaction_col)\n",
    "\n",
    "    # --- Build data ---\n",
    "    cols_needed = [y_col, d_col] + controls\n",
    "    data = df[cols_needed].dropna().copy()\n",
    "    if data.empty:\n",
    "        raise ValueError(\"After dropping NA, no rows remain for estimation.\")\n",
    "\n",
    "    y = data[y_col]\n",
    "    X = data[[d_col] + controls]\n",
    "\n",
    "    if add_constant:\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "    # --- Fit OLS ---\n",
    "    if cov_type == 'cluster' and (cluster_col is not None) and (cluster_col in df.columns):\n",
    "        clusters = df.loc[data.index, cluster_col]\n",
    "        results = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': clusters})\n",
    "    else:\n",
    "        if cov_type == 'cluster' and cluster_col is None:\n",
    "            print(\"‚ö† cov_type='cluster' requested but no cluster_col provided; defaulting to robust.\")\n",
    "            cov_type = 'robust'\n",
    "        results = sm.OLS(y, X).fit(cov_type='HC1' if cov_type == 'robust' else 'nonrobust')\n",
    "\n",
    "    # --- Reporting ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"OLS ESTIMATION SUMMARY (statsmodels.api.OLS)\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary())\n",
    "\n",
    "    return results, data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01c9b4",
   "metadata": {},
   "source": [
    "##### OLS End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5d99",
   "metadata": {},
   "source": [
    "##### 2SLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "486a9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=None,\n",
    "    include_interactions=False,\n",
    "    add_constant=True,\n",
    "    cluster_col=None,\n",
    "    cov_type='robust'  # 'robust' for HC, or 'cluster' if cluster_col is set\n",
    "):\n",
    "    \"\"\"\n",
    "    Functional 2SLS using linearmodels.iv.IV2SLS.\n",
    "\n",
    "    Model:\n",
    "        First stage: D = œÄ0 + œÄ1 Z + Œì X + ŒΩ\n",
    "        Second stage: Y = Œ± + Œ≤ D + Œò X + Œµ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain y_col, d_col, z_col, and any control columns.\n",
    "    y_col : str\n",
    "        Outcome column (e.g., 'Conversion_Rate').\n",
    "    d_col : str\n",
    "        Endogenous regressor (e.g., 'Clicks').\n",
    "    z_col : str\n",
    "        Instrument column (e.g., 'Clicks_predicted' from ML first stage).\n",
    "    base_controls : list[str] or None\n",
    "        Exogenous controls. Do NOT include outcome-adjacent variables like CTR.\n",
    "        Recommended: ['Age', 'Income', 'Gender_encoded', 'Location_encoded',\n",
    "                      'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'].\n",
    "    include_interactions : bool\n",
    "        If True, include Ad_Type √ó Ad_Placement interaction (exogenous).\n",
    "    add_constant : bool\n",
    "        If True, add a constant term automatically.\n",
    "    cluster_col : str or None\n",
    "        Column name for cluster-robust SEs (e.g., 'UserID', 'CampaignID').\n",
    "    cov_type : str\n",
    "        'robust' (HC), 'cluster' (requires cluster_col), or 'unadjusted'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : IV2SLSResults\n",
    "        Fitted IV results object from linearmodels.\n",
    "    data_used : pd.DataFrame\n",
    "        DataFrame with columns actually used in estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Validate required columns ---\n",
    "    required = [y_col, d_col, z_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # --- Controls: enforce exogeneity discipline ---\n",
    "    if base_controls is None:\n",
    "        base_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "        ]\n",
    "\n",
    "    # Strict: exclude CTR or any post-click/outcome-adjacent metrics from controls\n",
    "    invalid_controls = [c for c in base_controls if c not in df.columns]\n",
    "    if invalid_controls:\n",
    "        # Warn strictly but proceed with available subset\n",
    "        print(f\"‚ö† Skipping missing controls: {invalid_controls}\")\n",
    "    controls = [c for c in base_controls if c in df.columns]\n",
    "\n",
    "    # --- Optional exogenous interaction ---\n",
    "    interaction_col = None\n",
    "    if include_interactions:\n",
    "        if ('Ad_Type_encoded' in df.columns) and ('Ad_Placement_encoded' in df.columns):\n",
    "            interaction_col = 'Ad_Type_x_Placement'\n",
    "            if interaction_col not in df.columns:\n",
    "                df = df.copy()\n",
    "                df[interaction_col] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "                print(\"‚úì Added exogenous interaction: Ad_Type_x_Placement\")\n",
    "            controls.append(interaction_col)\n",
    "        else:\n",
    "            print(\"‚ö† Interaction requested but required columns not present; skipping.\")\n",
    "\n",
    "    # --- Build data frame used in estimation ---\n",
    "    cols_needed = [y_col, d_col, z_col] + controls\n",
    "    data = df[cols_needed].dropna().copy()\n",
    "    if data.empty:\n",
    "        raise ValueError(\"After dropping NA, no rows remain for estimation.\")\n",
    "\n",
    "    # --- Build formula for IV2SLS ---\n",
    "    # dependent ~ exog + [endog ~ instruments]\n",
    "    exog_formula = ' + '.join(controls) if controls else '1'\n",
    "    if add_constant and exog_formula != '1':\n",
    "        exog_formula = '1 + ' + exog_formula  # linearmodels adds constant via '1 +'\n",
    "    elif add_constant and exog_formula == '1':\n",
    "        # '1' already denotes constant in linearmodels formula\n",
    "        pass\n",
    "    else:\n",
    "        # No constant: use '-1' to suppress intercept if you have exog terms\n",
    "        if controls:\n",
    "            exog_formula = '-1 + ' + ' + '.join(controls)\n",
    "\n",
    "    formula = f\"{y_col} ~ {exog_formula} + [{d_col} ~ {z_col}]\"\n",
    "\n",
    "    # --- Fit IV2SLS ---\n",
    "    if cov_type == 'cluster' and (cluster_col is not None) and (cluster_col in df.columns):\n",
    "        clusters = df.loc[data.index, cluster_col]\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='clustered', clusters=clusters)\n",
    "    else:\n",
    "        if cov_type == 'cluster' and cluster_col is None:\n",
    "            print(\"‚ö† cov_type='cluster' requested but no cluster_col provided; defaulting to robust.\")\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='robust' if cov_type != 'unadjusted' else 'unadjusted')\n",
    "\n",
    "    # --- Strict reporting ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary)\n",
    "\n",
    "    # Optional first-stage diagnostics available via .first_stage (dict of RegressionResults)\n",
    "    # Example:\n",
    "    try:\n",
    "        fs = results.first_stage[d_col]\n",
    "        print(\"\\nFirst-stage summary (endogenous regressor: {}):\".format(d_col))\n",
    "        print(f\"  R-squared: {fs.rsquared:.4f}\")\n",
    "        print(f\"  F-statistic (excluded instrument): {getattr(fs, 'f_statistic', None)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return results, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f630374",
   "metadata": {},
   "source": [
    "##### 2SLS End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983708a",
   "metadata": {},
   "source": [
    "##### Stratified 2SLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2edefc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from linearmodels.iv import IV2SLS\n",
    "\n",
    "# def analyze_subgroup_effects_iv(\n",
    "#     df,\n",
    "#     subgroup_vars=None,\n",
    "#     min_subgroup_size=100,\n",
    "#     y_col='Conversion_Rate',\n",
    "#     d_col='Clicks',\n",
    "#     z_col='Clicks_predicted',\n",
    "#     base_controls=None,\n",
    "#     add_constant=True,\n",
    "#     cov_type='robust',         # 'robust', 'unadjusted', or 'cluster'\n",
    "#     cluster_col=None,\n",
    "#     verbose=True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Stratified IV2SLS with robust input validation and rank-deficiency pruning.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --- Required columns ---\n",
    "#     required = [y_col, d_col, z_col]\n",
    "#     missing = [c for c in required if c not in df.columns]\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "#     # --- Controls: strictly exogenous, no post-treatment metrics ---\n",
    "#     if base_controls is None:\n",
    "#         base_controls = [\n",
    "#             'Age', 'Income',\n",
    "#             'Gender_encoded', 'Location_encoded',\n",
    "#             'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "#         ]\n",
    "#     controls_all = [c for c in base_controls if c in df.columns]\n",
    "#     if verbose:\n",
    "#         missing_controls = [c for c in base_controls if c not in df.columns]\n",
    "#         if missing_controls:\n",
    "#             print(f\"‚ö† Skipping missing controls: {missing_controls}\")\n",
    "\n",
    "#     # --- Default subgroup spec ---\n",
    "#     if subgroup_vars is None:\n",
    "#         subgroup_vars = {\n",
    "#             'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "#             'Age': [0, 35, 50, 65, np.inf],\n",
    "#             'Location': None,\n",
    "#             'Ad_Type': None\n",
    "#         }\n",
    "\n",
    "#     # --- Helper: build formula ---\n",
    "#     def build_formula(controls_list):\n",
    "#         if controls_list:\n",
    "#             exog = ' + '.join(controls_list)\n",
    "#             exog = ('1 + ' + exog) if add_constant else ('-1 + ' + exog)\n",
    "#         else:\n",
    "#             exog = '1' if add_constant else '-1'\n",
    "#         return f\"{y_col} ~ {exog} + [{d_col} ~ {z_col}]\"\n",
    "\n",
    "#     # --- Helper: prune zero-variance and duplicate columns in subgroup ---\n",
    "#     def prune_controls(sub_df, ctrl_cols):\n",
    "#         pruned = []\n",
    "#         for c in ctrl_cols:\n",
    "#             if c not in sub_df.columns:\n",
    "#                 continue\n",
    "#             # drop if constant (zero variance)\n",
    "#             if sub_df[c].nunique() <= 1:\n",
    "#                 continue\n",
    "#             pruned.append(c)\n",
    "#         # Optional: drop perfectly duplicated columns\n",
    "#         # (cheap check: drop cols identical to the intercept vector)\n",
    "#         return pruned\n",
    "\n",
    "#     rows = []\n",
    "#     subgroup_keys = subgroup_vars if isinstance(subgroup_vars, list) else list(subgroup_vars.keys())\n",
    "\n",
    "#     for var in subgroup_keys:\n",
    "#         df_local = df.copy()\n",
    "\n",
    "#         # Build subgroup column\n",
    "#         if isinstance(subgroup_vars, dict) and subgroup_vars.get(var) is not None:\n",
    "#             bins = subgroup_vars[var]\n",
    "#             if not isinstance(bins, (list, tuple)) or len(bins) < 2:\n",
    "#                 if verbose: print(f\"‚ö† Invalid bins for {var}; skipping.\")\n",
    "#                 continue\n",
    "#             labels = [f\"{var}_{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "#             try:\n",
    "#                 df_local[f'{var}_subgroup'] = pd.cut(df_local[var], bins=bins, labels=labels, include_lowest=True)\n",
    "#                 subgroup_col = f'{var}_subgroup'\n",
    "#             except Exception as e:\n",
    "#                 if verbose: print(f\"‚ö† Failed to bin {var}: {e}; skipping.\")\n",
    "#                 continue\n",
    "#         else:\n",
    "#             subgroup_col = var\n",
    "#             if subgroup_col not in df_local.columns:\n",
    "#                 if verbose: print(f\"‚ö† Subgroup column {subgroup_col} missing; skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#         subgroups = df_local[subgroup_col].dropna().unique()\n",
    "#         if len(subgroups) == 0:\n",
    "#             if verbose: print(f\"‚ö† No valid subgroups for {var}; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         for sg in subgroups:\n",
    "#             dsg = df_local[df_local[subgroup_col] == sg]\n",
    "#             n_obs = len(dsg)\n",
    "#             if n_obs < min_subgroup_size:\n",
    "#                 continue\n",
    "\n",
    "#             # Ensure variation in instrument and endogenous regressor\n",
    "#             if dsg[z_col].nunique() <= 1 or dsg[d_col].nunique() <= 1:\n",
    "#                 # No first-stage or second-stage variation ‚Üí skip\n",
    "#                 continue\n",
    "\n",
    "#             # Build and prune controls for this subgroup\n",
    "#             controls = prune_controls(dsg, controls_all)\n",
    "\n",
    "#             # Build estimation data\n",
    "#             cols_needed = [y_col, d_col, z_col] + controls\n",
    "#             data = dsg[cols_needed].dropna()\n",
    "#             n_used = len(data)\n",
    "#             if n_used < min_subgroup_size:\n",
    "#                 continue\n",
    "\n",
    "#             # Final sanity: instrument/endog still vary after NA drop\n",
    "#             if data[z_col].nunique() <= 1 or data[d_col].nunique() <= 1:\n",
    "#                 continue\n",
    "\n",
    "#             formula = build_formula(controls)\n",
    "\n",
    "#             try:\n",
    "#                 if cov_type == 'cluster' and cluster_col and (cluster_col in dsg.columns):\n",
    "#                     clusters = dsg.loc[data.index, cluster_col]\n",
    "#                     res = IV2SLS.from_formula(formula, data=data).fit(\n",
    "#                         cov_type='clustered', clusters=clusters\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     if cov_type == 'cluster' and not cluster_col and verbose:\n",
    "#                         print(\"‚ö† cov_type='cluster' requested without cluster_col; defaulting to robust.\")\n",
    "#                     res = IV2SLS.from_formula(formula, data=data).fit(\n",
    "#                         cov_type='robust' if cov_type != 'unadjusted' else 'unadjusted'\n",
    "#                     )\n",
    "\n",
    "#                 # Extract estimates (second stage)\n",
    "#                 beta = res.params.get(d_col, np.nan)\n",
    "#                 se = res.std_errors.get(d_col, np.nan)\n",
    "#                 pval = res.pvalues.get(d_col, np.nan)\n",
    "#                 ci_lower = beta - 1.96 * se if pd.notnull(se) else np.nan\n",
    "#                 ci_upper = beta + 1.96 * se if pd.notnull(se) else np.nan\n",
    "#                 significant = bool(pd.notnull(pval) and pval < 0.05)\n",
    "\n",
    "#                 # First-stage diagnostics (single endogenous ‚Üí FirstStageResults object)\n",
    "#                 fs = res.first_stage\n",
    "#                 fs_r2 = getattr(fs, 'rsquared', np.nan)\n",
    "#                 fs_fstat_obj = getattr(fs, 'f_statistic', None)\n",
    "#                 if hasattr(fs_fstat_obj, 'stat'):\n",
    "#                     fs_f = float(fs_fstat_obj.stat)\n",
    "#                 elif isinstance(fs_fstat_obj, (int, float)):\n",
    "#                     fs_f = float(fs_fstat_obj)\n",
    "#                 else:\n",
    "#                     fs_f = np.nan\n",
    "#                 instrument_weak = bool(pd.notnull(fs_f) and fs_f < 10)\n",
    "\n",
    "#                 rows.append({\n",
    "#                     'Variable': var,\n",
    "#                     'Subgroup': str(sg),\n",
    "#                     'N': n_obs,\n",
    "#                     'N_Used': n_used,\n",
    "#                     'Controls_Used': ','.join(controls) if controls else '(none)',\n",
    "#                     'First_Stage_R2': fs_r2,\n",
    "#                     'First_Stage_F': fs_f,\n",
    "#                     'Instrument_Weak': instrument_weak,\n",
    "#                     'Beta': beta,\n",
    "#                     'Std_Error': se,\n",
    "#                     'P_Value': pval,\n",
    "#                     'CI_Lower': ci_lower,\n",
    "#                     'CI_Upper': ci_upper,\n",
    "#                     'Significant': significant\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 if verbose:\n",
    "#                     print(f\"‚úó Error in subgroup '{var}={sg}': {e}\")\n",
    "#                 continue\n",
    "\n",
    "#     if not rows:\n",
    "#         if verbose: print(\"‚ö† No subgroups estimated successfully.\")\n",
    "#         return None\n",
    "\n",
    "#     out = pd.DataFrame(rows)\n",
    "#     out['Abs_Effect'] = out['Beta'].abs()\n",
    "#     out = out.sort_values('Abs_Effect', ascending=False)\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97952f6c",
   "metadata": {},
   "source": [
    "##### Stratified 2SLS End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd5a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that I clicks is indeed endogenous.\n",
    "def generate_example_data(n=2000):\n",
    "    \"\"\"Generate synthetic data with endogenous clicks\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Age': np.random.randint(18, 65, n),\n",
    "        'Gender': np.random.choice(['M', 'F'], n),\n",
    "        'Income': np.random.randint(30000, 150000, n),\n",
    "        'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "        'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "        'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "        'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "        'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "    })\n",
    "    \n",
    "    # Normalize income\n",
    "    data['Income'] = data['Income'] / 100000\n",
    "    \n",
    "    # Unobserved confounder (correlated with both clicks and conversion error)\n",
    "    unobserved_confounder = np.random.randn(n)\n",
    "    \n",
    "    # Generate clicks (endogenous regressor)\n",
    "    clicks_base = (\n",
    "        0.5\n",
    "        + 0.3 * (data['Ad_Type'] == 'Video').astype(float)\n",
    "        + 0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float)\n",
    "        + 0.01 * data['Age']\n",
    "        + 0.2 * data['Income']\n",
    "        + 0.8 * unobserved_confounder   # <-- confounder drives clicks\n",
    "        + np.random.randn(n) * 0.5\n",
    "    )\n",
    "    data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "    # CTR (correlated with clicks)\n",
    "    data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "    # Conversion rate: causal effect + controls + error term\n",
    "    # Error term includes the SAME confounder that drives clicks ‚Üí endogeneity\n",
    "    epsilon = 0.05 * unobserved_confounder + np.random.randn(n) * 0.03\n",
    "    \n",
    "    conversion_base = (\n",
    "        0.05\n",
    "        + 0.08 * data['Clicks']        # true causal effect\n",
    "        + 0.02 * data['Income']\n",
    "        + 0.005 * data['Age']\n",
    "        + 0.3 * data['CTR']\n",
    "        + epsilon                      # endogenous error\n",
    "    )\n",
    "    data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# def generate_example_data(n=2000):\n",
    "#     \"\"\"Generate synthetic data for demonstration\"\"\"\n",
    "#     np.random.seed(42)\n",
    "    \n",
    "#     data = pd.DataFrame({\n",
    "#         'Age': np.random.randint(18, 65, n),\n",
    "#         'Gender': np.random.choice(['M', 'F'], n),\n",
    "#         'Income': np.random.randint(30000, 150000, n),\n",
    "#         'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "#         'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "#         'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "#         'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "#         'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "#     })\n",
    "    \n",
    "#     # Normalize income to reasonable scale\n",
    "#     data['Income'] = data['Income'] / 100000  # Scale to 0.3-1.5 range\n",
    "    \n",
    "#     # Generate clicks with realistic structure\n",
    "#     clicks_base = (\n",
    "#         0.5 +  # baseline\n",
    "#         0.3 * (data['Ad_Type'] == 'Video').astype(float) +\n",
    "#         0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float) +\n",
    "#         0.01 * data['Age'] +\n",
    "#         0.2 * data['Income'] +\n",
    "#         np.random.randn(n) * 0.5\n",
    "#     )\n",
    "#     data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "#     # Generate CTR (correlated with clicks but not in instrument)\n",
    "#     data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "#     # Generate conversion rate with causal effect from clicks\n",
    "#     # Plus confounding through unobserved factors\n",
    "#     unobserved_confounder = np.random.randn(n) * 0.05\n",
    "    \n",
    "#     conversion_base = (\n",
    "#         0.05 +  # baseline\n",
    "#         0.08 * data['Clicks'] +  # TRUE CAUSAL EFFECT\n",
    "#         0.02 * data['Income'] +\n",
    "#         0.005 * data['Age'] +\n",
    "#         0.3 * data['CTR'] +\n",
    "#         unobserved_confounder +\n",
    "#         np.random.randn(n) * 0.03\n",
    "#     )\n",
    "#     data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "#     # Add endogeneity: unobserved confounder affects clicks too\n",
    "#     data['Clicks'] = data['Clicks'] + unobserved_confounder * 2\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d540b7",
   "metadata": {},
   "source": [
    "##### Start of Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be069528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORIGINAL DATASET\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  \n",
      "0 2024-01-01 00:00:00  1.266525  0.120705         0.439875  \n",
      "1 2024-01-01 01:00:00  1.240839  0.090825         0.289356  \n",
      "2 2024-01-01 02:00:00  1.255371  0.178216         0.344477  \n",
      "3 2024-01-01 03:00:00  0.829583  0.073841         0.421605  \n",
      "4 2024-01-01 04:00:00  3.401662  0.354961         0.701755  \n",
      "               Age Gender       Income  Location Ad_Type Ad_Topic  \\\n",
      "count   5000.00000   5000  5000.000000      5000    5000     5000   \n",
      "unique         NaN      2          NaN         3       3        4   \n",
      "top            NaN      F          NaN  Suburban  Banner  Fashion   \n",
      "freq           NaN   2526          NaN      1701    1686     1309   \n",
      "mean      41.16820    NaN     0.903564       NaN     NaN      NaN   \n",
      "min       18.00000    NaN     0.300020       NaN     NaN      NaN   \n",
      "25%       29.00000    NaN     0.594980       NaN     NaN      NaN   \n",
      "50%       41.00000    NaN     0.912505       NaN     NaN      NaN   \n",
      "75%       53.00000    NaN     1.205272       NaN     NaN      NaN   \n",
      "max       64.00000    NaN     1.499980       NaN     NaN      NaN   \n",
      "std       13.53105    NaN     0.349124       NaN     NaN      NaN   \n",
      "\n",
      "       Ad_Placement           Click_Time       Clicks          CTR  \\\n",
      "count          5000                 5000  5000.000000  5000.000000   \n",
      "unique            3                  NaN          NaN          NaN   \n",
      "top         Display                  NaN          NaN          NaN   \n",
      "freq           1720                  NaN          NaN          NaN   \n",
      "mean            NaN  2024-04-14 03:30:00     1.321436     0.131129   \n",
      "min             NaN  2024-01-01 00:00:00     0.100000     0.005005   \n",
      "25%             NaN  2024-02-22 01:45:00     0.647578     0.058459   \n",
      "50%             NaN  2024-04-14 03:30:00     1.270048     0.115955   \n",
      "75%             NaN  2024-06-05 05:15:00     1.925475     0.188241   \n",
      "max             NaN  2024-07-27 07:00:00     4.536990     0.594741   \n",
      "std             NaN                  NaN     0.859435     0.095531   \n",
      "\n",
      "        Conversion_Rate  \n",
      "count       5000.000000  \n",
      "unique              NaN  \n",
      "top                 NaN  \n",
      "freq                NaN  \n",
      "mean           0.419249  \n",
      "min            0.010990  \n",
      "25%            0.298808  \n",
      "50%            0.412724  \n",
      "75%            0.532171  \n",
      "max            0.950000  \n",
      "std            0.160666  \n",
      "\n",
      "============================================================\n",
      "DATA CLEANING AND PREPROCESSING\n",
      "============================================================\n",
      "‚úì Winsorized 100 income values at 1st/99th percentiles\n",
      "  Income range: [0, 1]\n",
      "\n",
      "üìä Creating logarithmic transformations:\n",
      "  ‚úì Income_log created (log1p transformation)\n",
      "  ‚úì Age_log created (log1p transformation)\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY:\n",
      "  Initial rows:        5,000\n",
      "  Final rows:          5,000\n",
      "  Rows removed:        0 (0.0%)\n",
      "  Log variables added: 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLEANED AND LOGGED DATASET\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  Income_log  \\\n",
      "0 2024-01-01 00:00:00  1.266525  0.120705         0.439875    0.629547   \n",
      "1 2024-01-01 01:00:00  1.240839  0.090825         0.289356    0.347553   \n",
      "2 2024-01-01 02:00:00  1.255371  0.178216         0.344477    0.611943   \n",
      "3 2024-01-01 03:00:00  0.829583  0.073841         0.421605    0.590383   \n",
      "4 2024-01-01 04:00:00  3.401662  0.354961         0.701755    0.899690   \n",
      "\n",
      "    Age_log  \n",
      "0  4.043051  \n",
      "1  3.850148  \n",
      "2  3.496508  \n",
      "3  4.110874  \n",
      "4  3.258097  \n",
      "\n",
      "============================================================\n",
      "TIME ENGINEERED COLUMN\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  Income_log  \\\n",
      "0 2024-01-01 00:00:00  1.266525  0.120705         0.439875    0.629547   \n",
      "1 2024-01-01 01:00:00  1.240839  0.090825         0.289356    0.347553   \n",
      "2 2024-01-01 02:00:00  1.255371  0.178216         0.344477    0.611943   \n",
      "3 2024-01-01 03:00:00  0.829583  0.073841         0.421605    0.590383   \n",
      "4 2024-01-01 04:00:00  3.401662  0.354961         0.701755    0.899690   \n",
      "\n",
      "    Age_log  Day_of_Week  Hour  \n",
      "0  4.043051            0     0  \n",
      "1  3.850148            0     1  \n",
      "2  3.496508            0     2  \n",
      "3  4.110874            0     3  \n",
      "4  3.258097            0     4  \n",
      "\n",
      "============================================================\n",
      "ENCODED CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "   Age Gender   Income  Location Ad_Type Ad_Topic  Ad_Placement  \\\n",
      "0   56      M  0.87676  Suburban  Native   Travel        Search   \n",
      "1   46      F  0.41560     Rural  Native   Travel       Display   \n",
      "2   32      M  0.84401     Rural  Banner     Tech       Display   \n",
      "3   60      M  0.80468  Suburban  Native     Food  Social_Media   \n",
      "4   25      M  1.45884     Rural  Native  Fashion       Display   \n",
      "\n",
      "           Click_Time    Clicks       CTR  Conversion_Rate  Income_log  \\\n",
      "0 2024-01-01 00:00:00  1.266525  0.120705         0.439875    0.629547   \n",
      "1 2024-01-01 01:00:00  1.240839  0.090825         0.289356    0.347553   \n",
      "2 2024-01-01 02:00:00  1.255371  0.178216         0.344477    0.611943   \n",
      "3 2024-01-01 03:00:00  0.829583  0.073841         0.421605    0.590383   \n",
      "4 2024-01-01 04:00:00  3.401662  0.354961         0.701755    0.899690   \n",
      "\n",
      "    Age_log  Day_of_Week  Hour  Gender_encoded  Location_encoded  \\\n",
      "0  4.043051            0     0               1                 1   \n",
      "1  3.850148            0     1               0                 0   \n",
      "2  3.496508            0     2               1                 0   \n",
      "3  4.110874            0     3               1                 1   \n",
      "4  3.258097            0     4               1                 0   \n",
      "\n",
      "   Ad_Type_encoded  Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "0                1                 3                     1  \n",
      "1                1                 3                     0  \n",
      "2                0                 2                     0  \n",
      "3                1                 1                     2  \n",
      "4                1                 0                     0  \n",
      "\n",
      "============================================================\n",
      "DESCRIPTION OF DF AFTER PREPROCESSING\n",
      "============================================================\n",
      "               Age Gender       Income  Location Ad_Type Ad_Topic  \\\n",
      "count   5000.00000   5000  5000.000000      5000    5000     5000   \n",
      "unique         NaN      2          NaN         3       3        4   \n",
      "top            NaN      F          NaN  Suburban  Banner  Fashion   \n",
      "freq           NaN   2526          NaN      1701    1686     1309   \n",
      "mean      41.16820    NaN     0.903536       NaN     NaN      NaN   \n",
      "min       18.00000    NaN     0.309159       NaN     NaN      NaN   \n",
      "25%       29.00000    NaN     0.594980       NaN     NaN      NaN   \n",
      "50%       41.00000    NaN     0.912505       NaN     NaN      NaN   \n",
      "75%       53.00000    NaN     1.205272       NaN     NaN      NaN   \n",
      "max       64.00000    NaN     1.486421       NaN     NaN      NaN   \n",
      "std       13.53105    NaN     0.348928       NaN     NaN      NaN   \n",
      "\n",
      "       Ad_Placement           Click_Time       Clicks          CTR  \\\n",
      "count          5000                 5000  5000.000000  5000.000000   \n",
      "unique            3                  NaN          NaN          NaN   \n",
      "top         Display                  NaN          NaN          NaN   \n",
      "freq           1720                  NaN          NaN          NaN   \n",
      "mean            NaN  2024-04-14 03:30:00     1.321436     0.131129   \n",
      "min             NaN  2024-01-01 00:00:00     0.100000     0.005005   \n",
      "25%             NaN  2024-02-22 01:45:00     0.647578     0.058459   \n",
      "50%             NaN  2024-04-14 03:30:00     1.270048     0.115955   \n",
      "75%             NaN  2024-06-05 05:15:00     1.925475     0.188241   \n",
      "max             NaN  2024-07-27 07:00:00     4.536990     0.594741   \n",
      "std             NaN                  NaN     0.859435     0.095531   \n",
      "\n",
      "        Conversion_Rate   Income_log      Age_log  Day_of_Week         Hour  \\\n",
      "count       5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
      "unique              NaN          NaN          NaN          NaN          NaN   \n",
      "top                 NaN          NaN          NaN          NaN          NaN   \n",
      "freq                NaN          NaN          NaN          NaN          NaN   \n",
      "mean           0.419249     0.626321     3.684198     2.979200    11.487200   \n",
      "min            0.010990     0.269385     2.944439     0.000000     0.000000   \n",
      "25%            0.298808     0.466861     3.401197     1.000000     5.000000   \n",
      "50%            0.412724     0.648414     3.737670     3.000000    11.000000   \n",
      "75%            0.532171     0.790851     3.988984     5.000000    17.000000   \n",
      "max            0.950000     0.910844     4.174387     6.000000    23.000000   \n",
      "std            0.160666     0.188324     0.349952     1.994082     6.925332   \n",
      "\n",
      "        Gender_encoded  Location_encoded  Ad_Type_encoded  Ad_Topic_encoded  \\\n",
      "count      5000.000000       5000.000000      5000.000000       5000.000000   \n",
      "unique             NaN               NaN              NaN               NaN   \n",
      "top                NaN               NaN              NaN               NaN   \n",
      "freq               NaN               NaN              NaN               NaN   \n",
      "mean          0.494800          1.000600         0.992400          1.489800   \n",
      "min           0.000000          0.000000         0.000000          0.000000   \n",
      "25%           0.000000          0.000000         0.000000          0.000000   \n",
      "50%           0.000000          1.000000         1.000000          1.000000   \n",
      "75%           1.000000          2.000000         2.000000          3.000000   \n",
      "max           1.000000          2.000000         2.000000          3.000000   \n",
      "std           0.500023          0.812362         0.816625          1.132675   \n",
      "\n",
      "        Ad_Placement_encoded  \n",
      "count            5000.000000  \n",
      "unique                   NaN  \n",
      "top                      NaN  \n",
      "freq                     NaN  \n",
      "mean                0.991600  \n",
      "min                 0.000000  \n",
      "25%                 0.000000  \n",
      "50%                 1.000000  \n",
      "75%                 2.000000  \n",
      "max                 2.000000  \n",
      "std                 0.824418  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning and preprocessing\n",
    "# df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "df = generate_example_data(n=5000)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ORIGINAL DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "df = clean_data(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CLEANED AND LOGGED DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = engineer_time_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('TIME ENGINEERED COLUMN')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = encode_categorical_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ENCODED CATEGORICAL VARIABLES')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('DESCRIPTION OF DF AFTER PREPROCESSING')\n",
    "print(\"=\"*60)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "856f402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Advanced cleaning and instrument stuff.\n",
    "# # Cleaning and preprocessing\n",
    "# # df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "# df = generate_example_data(n=5000)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print('ORIGINAL DATASET')\n",
    "# print(\"=\"*60)\n",
    "# print(df.head())\n",
    "\n",
    "# df = clean_data_strict(df)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print('CLEANED AND LOGGED DATASET')\n",
    "# print(\"=\"*60)\n",
    "# print(df.head())\n",
    "\n",
    "# df = engineer_time_features_enhanced(df)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print('TIME ENGINEERED COLUMN')\n",
    "# print(\"=\"*60)\n",
    "# print(df.head())\n",
    "\n",
    "# df = encode_categorical_features(df)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print('ENCODED CATEGORICAL VARIABLES')\n",
    "# print(\"=\"*60)\n",
    "# print(df.head())\n",
    "\n",
    "# # df = engineer_instrument_features_comprehensive(df)\n",
    "# # print(\"\\n\" + \"=\"*60)\n",
    "# # print('ADVANCED INSTRUMENT FEATURE VARIABLES')\n",
    "# # print(\"=\"*60)\n",
    "# # print(df.head())\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print('DESCRIPTION OF DF AFTER PREPROCESSING')\n",
    "# print(\"=\"*60)\n",
    "# print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39c214b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING ML INSTRUMENT\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE INSTRUMENT FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "[1] Demographics √ó Ad Characteristics:\n",
      "  Created 9 interactions\n",
      "\n",
      "[2] Demographics √ó Time:\n",
      "  Created 2 interactions\n",
      "\n",
      "[3] Ad Characteristics √ó Time:\n",
      "  Created 3 interactions\n",
      "\n",
      "[4] Location-specific interactions:\n",
      "  Created 6 interactions\n",
      "\n",
      "[5] Nonlinear transformations:\n",
      "  Created 8 transformations\n",
      "\n",
      "[6] Three-way interactions:\n",
      "  Created 0 three-way interactions\n",
      "\n",
      "[7] Ad characteristic interactions:\n",
      "  Created 3 ad interactions\n",
      "\n",
      "============================================================\n",
      "TOTAL NEW FEATURES CREATED: 31\n",
      "============================================================\n",
      "\n",
      "Age                                               int32\n",
      "Gender                                           object\n",
      "Income                                          float64\n",
      "Location                                         object\n",
      "Ad_Type                                          object\n",
      "Ad_Topic                                         object\n",
      "Ad_Placement                                     object\n",
      "Click_Time                               datetime64[ns]\n",
      "Clicks                                          float64\n",
      "CTR                                             float64\n",
      "Conversion_Rate                                 float64\n",
      "Income_log                                      float64\n",
      "Age_log                                         float64\n",
      "Day_of_Week                                       int32\n",
      "Hour                                              int32\n",
      "Gender_encoded                                    int64\n",
      "Location_encoded                                  int64\n",
      "Ad_Type_encoded                                   int64\n",
      "Ad_Topic_encoded                                  int64\n",
      "Ad_Placement_encoded                              int64\n",
      "Age_x_Ad_Type_encoded                             int64\n",
      "Age_x_Ad_Topic_encoded                            int64\n",
      "Age_x_Ad_Placement_encoded                        int64\n",
      "Income_x_Ad_Type_encoded                        float64\n",
      "Income_x_Ad_Topic_encoded                       float64\n",
      "Income_x_Ad_Placement_encoded                   float64\n",
      "Gender_encoded_x_Ad_Type_encoded                  int64\n",
      "Gender_encoded_x_Ad_Topic_encoded                 int64\n",
      "Gender_encoded_x_Ad_Placement_encoded             int64\n",
      "Age_x_Hour                                        int32\n",
      "Income_x_Hour                                   float64\n",
      "Ad_Type_encoded_x_Hour                            int64\n",
      "Ad_Topic_encoded_x_Hour                           int64\n",
      "Ad_Placement_encoded_x_Hour                       int64\n",
      "Location_x_Age                                    int64\n",
      "Location_x_Income                               float64\n",
      "Location_x_Gender_encoded                         int64\n",
      "Location_x_Ad_Type_encoded                        int64\n",
      "Location_x_Ad_Topic_encoded                       int64\n",
      "Location_x_Ad_Placement_encoded                   int64\n",
      "Age_squared                                       int32\n",
      "Age_cubed                                         int32\n",
      "Age_sqrt                                        float64\n",
      "Income_squared                                  float64\n",
      "Income_sqrt                                     float64\n",
      "Hour_squared                                      int32\n",
      "Hour_sin                                        float64\n",
      "Hour_cos                                        float64\n",
      "AdType_x_Placement                                int64\n",
      "AdType_x_Topic                                    int64\n",
      "AdTopic_x_Placement                               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ML instrument creation took about 1m40secs with real data\n",
    "# ML instrument creation took about 3m8secs with synthetic data\n",
    "# ML instrument creation took about 5m40secs with real data\n",
    "# with advanced instrument variables it took 11m51secs and still has a very bad score.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CREATING ML INSTRUMENT')\n",
    "print(\"=\"*60)\n",
    "df, X, model = create_ml_instrument(df, model_type='stacking', use_enhanced_features=True)\n",
    "# NOTE Remember that the instrument is really weak when created not using the interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b41b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ML INSTRUMENT DIAGNOSTICS\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "SAMPLE INFORMATION:\n",
      "  Sample size (n):              5,000\n",
      "  Number of features (k):       18\n",
      "\n",
      "FIRST-STAGE PERFORMANCE:\n",
      "  R-squared:                    0.0199\n",
      "  Correlation (Z, D):           0.1412\n",
      "  F-statistic:                  101.24\n",
      "  Cragg-Donald statistic:       99.61\n",
      "\n",
      "BENCHMARKS & INTERPRETATION:\n",
      "  Criterion                           Threshold    Status\n",
      "  ----------------------------------- ------------ --------------------\n",
      "  Weak Instrument (F < 10)            10.00        ‚úì STRONG\n",
      "  Stock-Yogo 10% max bias             16.38        ‚úì‚úì EXCELLENT\n",
      "  Stock-Yogo 15% max bias             8.96         ‚úì GOOD\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  ‚úì‚úì VERY STRONG INSTRUMENT\n",
      "     Maximum IV bias < 10% of OLS bias\n",
      "\n",
      "‚Ñπ Stacking ensemble used - feature importances not directly available\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# diagnostics for ml (complex) instrument strength\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ML INSTRUMENT DIAGNOSTICS')\n",
    "print(\"=\"*60)\n",
    "enhanced_instrument_diagnostics(df, X, df['Clicks'], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51dc9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OLS ESTIMATION SUMMARY (statsmodels.api.OLS)\n",
      "======================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        Conversion_Rate   R-squared:                       0.930\n",
      "Model:                            OLS   Adj. R-squared:                  0.929\n",
      "Method:                 Least Squares   F-statistic:                     7726.\n",
      "Date:                Thu, 13 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:20:18   Log-Likelihood:                 8679.7\n",
      "No. Observations:                5000   AIC:                        -1.734e+04\n",
      "Df Residuals:                    4991   BIC:                        -1.728e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                    0.0237      0.003      8.039      0.000       0.018       0.029\n",
      "Clicks                   0.1572      0.001    209.987      0.000       0.156       0.159\n",
      "Age                      0.0045   4.52e-05    100.572      0.000       0.004       0.005\n",
      "Income                   0.0138      0.002      7.925      0.000       0.010       0.017\n",
      "Gender_encoded           0.0018      0.001      1.528      0.126      -0.001       0.004\n",
      "Location_encoded     -2.417e-05      0.001     -0.033      0.974      -0.001       0.001\n",
      "Ad_Type_encoded         -0.0076      0.001    -10.222      0.000      -0.009      -0.006\n",
      "Ad_Topic_encoded        -0.0007      0.001     -1.256      0.209      -0.002       0.000\n",
      "Ad_Placement_encoded    -0.0040      0.001     -5.459      0.000      -0.005      -0.003\n",
      "==============================================================================\n",
      "Omnibus:                        4.095   Durbin-Watson:                   2.032\n",
      "Prob(Omnibus):                  0.129   Jarque-Bera (JB):                4.073\n",
      "Skew:                           0.056   Prob(JB):                        0.130\n",
      "Kurtosis:                       3.083   Cond. No.                         228.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "# ols\n",
    "\n",
    "controls = [\n",
    "    'Age', 'Income',\n",
    "    'Gender_encoded', 'Location_encoded',\n",
    "    'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "]\n",
    "\n",
    "results, data_used = run_ols(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    base_controls=controls,\n",
    "    # include_interactions=True,   # optional\n",
    "    add_constant=True,\n",
    "    cov_type='robust'            # or 'cluster' with cluster_col='CampaignID'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "589c0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added exogenous interaction: Ad_Type_x_Placement\n",
      "\n",
      "======================================================================\n",
      "2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\n",
      "======================================================================\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:        Conversion_Rate   R-squared:                     -0.5283\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                -0.5311\n",
      "No. Observations:                5000   F-statistic:                    819.72\n",
      "Date:                Thu, Nov 13 2025   P-value (F-stat)                0.0000\n",
      "Time:                        16:20:18   Distribution:                  chi2(9)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                                  Parameter Estimates                                   \n",
      "========================================================================================\n",
      "                      Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                0.1677     0.2978     0.5632     0.5733     -0.4160      0.7514\n",
      "Age                      0.0063     0.0037     1.7050     0.0882     -0.0009      0.0136\n",
      "Income                   0.0540     0.0836     0.6467     0.5178     -0.1097      0.2178\n",
      "Gender_encoded           0.0022     0.0057     0.3855     0.6998     -0.0089      0.0133\n",
      "Location_encoded         0.0010     0.0040     0.2472     0.8048     -0.0069      0.0089\n",
      "Ad_Type_encoded          0.0241     0.0647     0.3726     0.7094     -0.1027      0.1509\n",
      "Ad_Topic_encoded         0.0008     0.0038     0.2085     0.8349     -0.0067      0.0082\n",
      "Ad_Placement_encoded     0.0113     0.0314     0.3612     0.7180     -0.0502      0.0729\n",
      "Ad_Type_x_Placement     -0.0002     0.0042    -0.0447     0.9643     -0.0084      0.0080\n",
      "Clicks                  -0.0732     0.4748    -0.1543     0.8774     -1.0039      0.8574\n",
      "========================================================================================\n",
      "\n",
      "Endogenous: Clicks\n",
      "Instruments: Clicks_predicted\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# 2SLS with ml featured instrument\n",
    "# The r-squared value is super tiny with real data for whatever reason.\n",
    "# However, with my synthetic data I actually get a good r-squared score and a small p-value.\n",
    "# Perhpas the move is to end the real data stuff there, but continue with Raj Chetty methodlogies with the\n",
    "# synthetic data for reaserch sake.\n",
    "\n",
    "controls = [\n",
    "    'Age', 'Income',\n",
    "    'Gender_encoded', 'Location_encoded',\n",
    "    'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "]\n",
    "\n",
    "results, data_used = run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=controls,\n",
    "    include_interactions=True,   # optional\n",
    "    add_constant=True,\n",
    "    cov_type='robust'            # or 'cluster' with cluster_col='CampaignID'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8966eda",
   "metadata": {},
   "source": [
    "##### End of Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d64cb6",
   "metadata": {},
   "source": [
    "##### Spitballing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea394ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORIGINAL DATASET\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                   Click_Time  Conversion_Rate     CTR  \n",
      "0  2024-01-18 20:45:56.898459           0.0981  0.0737  \n",
      "1  2023-04-24 20:45:56.898459           0.0937  0.0592  \n",
      "2  2024-02-24 20:45:56.898459           0.1912  0.0563  \n",
      "3  2023-12-13 20:45:56.898459           0.1122  0.0232  \n",
      "4  2023-07-02 20:45:56.898459           0.1426  0.0539  \n",
      "                 Age Gender         Income Location Ad_Type Ad_Topic  \\\n",
      "count   10000.000000  10000   10000.000000    10000   10000    10000   \n",
      "unique           NaN      3            NaN        3       4        6   \n",
      "top              NaN   Male            NaN    Rural  Banner  Finance   \n",
      "freq             NaN   4986            NaN     3408    2560     1734   \n",
      "mean       34.235200    NaN   50080.040922      NaN     NaN      NaN   \n",
      "std        14.790752    NaN   19935.832667      NaN     NaN      NaN   \n",
      "min       -21.000000    NaN  -38932.640000      NaN     NaN      NaN   \n",
      "25%        24.000000    NaN   36892.040000      NaN     NaN      NaN   \n",
      "50%        34.000000    NaN   50122.520000      NaN     NaN      NaN   \n",
      "75%        44.000000    NaN   63271.342500      NaN     NaN      NaN   \n",
      "max        92.000000    NaN  126635.800000      NaN     NaN      NaN   \n",
      "\n",
      "        Ad_Placement        Clicks                  Click_Time  \\\n",
      "count          10000  10000.000000                       10000   \n",
      "unique             3           NaN                        6503   \n",
      "top     Social Media           NaN  2023-11-03 20:45:56.900348   \n",
      "freq            3340           NaN                           7   \n",
      "mean             NaN      5.030600                         NaN   \n",
      "std              NaN      2.258046                         NaN   \n",
      "min              NaN      0.000000                         NaN   \n",
      "25%              NaN      3.000000                         NaN   \n",
      "50%              NaN      5.000000                         NaN   \n",
      "75%              NaN      6.000000                         NaN   \n",
      "max              NaN     17.000000                         NaN   \n",
      "\n",
      "        Conversion_Rate           CTR  \n",
      "count      10000.000000  10000.000000  \n",
      "unique              NaN           NaN  \n",
      "top                 NaN           NaN  \n",
      "freq                NaN           NaN  \n",
      "mean           0.202246      0.050425  \n",
      "std            0.121094      0.019821  \n",
      "min            0.001000      0.000000  \n",
      "25%            0.109500      0.037100  \n",
      "50%            0.180650      0.050300  \n",
      "75%            0.275100      0.063700  \n",
      "max            0.731700      0.127200  \n",
      "\n",
      "============================================================\n",
      "DATA CLEANING AND PREPROCESSING\n",
      "============================================================\n",
      "‚úì Converted 70 negative income values to missing\n",
      "‚úì Imputed 70 missing income values with median\n",
      "‚úì Winsorized 200 income values at 1st/99th percentiles\n",
      "  Income range: [7,384, 96,445]\n",
      "‚úì Filtered 457 rows with implausible ages (keeping 10-90)\n",
      "\n",
      "üìä Creating logarithmic transformations:\n",
      "  ‚úì Income_log created (log1p transformation)\n",
      "  ‚úì Age_log created (log1p transformation)\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY:\n",
      "  Initial rows:        10,000\n",
      "  Final rows:          9,543\n",
      "  Rows removed:        457 (4.6%)\n",
      "  Log variables added: 2\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLEANED AND LOGGED DATASET\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                   Click_Time  Conversion_Rate     CTR  Income_log   Age_log  \n",
      "0  2024-01-18 20:45:56.898459           0.0981  0.0737   10.483422  4.127134  \n",
      "1  2023-04-24 20:45:56.898459           0.0937  0.0592   10.767521  3.737670  \n",
      "2  2024-02-24 20:45:56.898459           0.1912  0.0563   11.129134  3.912023  \n",
      "3  2023-12-13 20:45:56.898459           0.1122  0.0232   11.075764  4.234107  \n",
      "4  2023-07-02 20:45:56.898459           0.1426  0.0539    9.957521  4.158883  \n",
      "\n",
      "============================================================\n",
      "TIME ENGINEERED COLUMN\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                  Click_Time  Conversion_Rate     CTR  Income_log   Age_log  \\\n",
      "0 2024-01-18 20:45:56.898459           0.0981  0.0737   10.483422  4.127134   \n",
      "1 2023-04-24 20:45:56.898459           0.0937  0.0592   10.767521  3.737670   \n",
      "2 2024-02-24 20:45:56.898459           0.1912  0.0563   11.129134  3.912023   \n",
      "3 2023-12-13 20:45:56.898459           0.1122  0.0232   11.075764  4.234107   \n",
      "4 2023-07-02 20:45:56.898459           0.1426  0.0539    9.957521  4.158883   \n",
      "\n",
      "   Day_of_Week  Hour  \n",
      "0            3    20  \n",
      "1            0    20  \n",
      "2            5    20  \n",
      "3            2    20  \n",
      "4            6    20  \n",
      "\n",
      "============================================================\n",
      "ENCODED CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                  Click_Time  Conversion_Rate     CTR  Income_log   Age_log  \\\n",
      "0 2024-01-18 20:45:56.898459           0.0981  0.0737   10.483422  4.127134   \n",
      "1 2023-04-24 20:45:56.898459           0.0937  0.0592   10.767521  3.737670   \n",
      "2 2024-02-24 20:45:56.898459           0.1912  0.0563   11.129134  3.912023   \n",
      "3 2023-12-13 20:45:56.898459           0.1122  0.0232   11.075764  4.234107   \n",
      "4 2023-07-02 20:45:56.898459           0.1426  0.0539    9.957521  4.158883   \n",
      "\n",
      "   Day_of_Week  Hour  Gender_encoded  Location_encoded  Ad_Type_encoded  \\\n",
      "0            3    20               1                 2                0   \n",
      "1            0    20               1                 0                3   \n",
      "2            5    20               0                 0                2   \n",
      "3            2    20               0                 1                2   \n",
      "4            6    20               1                 2                1   \n",
      "\n",
      "   Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "0                 5                     1  \n",
      "1                 5                     0  \n",
      "2                 2                     1  \n",
      "3                 3                     2  \n",
      "4                 0                     0  \n",
      "\n",
      "============================================================\n",
      "DESCRIPTION OF DF AFTER PREPROCESSING\n",
      "============================================================\n",
      "                Age Gender        Income Location Ad_Type Ad_Topic  \\\n",
      "count   9543.000000   9543   9543.000000     9543    9543     9543   \n",
      "unique          NaN      3           NaN        3       4        6   \n",
      "top             NaN   Male           NaN    Rural  Banner  Finance   \n",
      "freq            NaN   4751           NaN     3239    2460     1637   \n",
      "mean      35.690035    NaN  50410.732326      NaN     NaN      NaN   \n",
      "min       10.000000    NaN   7384.365600      NaN     NaN      NaN   \n",
      "25%       26.000000    NaN  37185.925000      NaN     NaN      NaN   \n",
      "50%       35.000000    NaN  50278.030000      NaN     NaN      NaN   \n",
      "75%       45.000000    NaN  63240.720000      NaN     NaN      NaN   \n",
      "max       86.000000    NaN  96444.632200      NaN     NaN      NaN   \n",
      "std       13.451463    NaN  19129.415676      NaN     NaN      NaN   \n",
      "\n",
      "       Ad_Placement       Clicks                     Click_Time  \\\n",
      "count          9543  9543.000000                           9543   \n",
      "unique            3          NaN                            NaN   \n",
      "top         Website          NaN                            NaN   \n",
      "freq           3192          NaN                            NaN   \n",
      "mean            NaN     5.024730  2023-10-21 14:53:36.391400192   \n",
      "min             NaN     0.000000     2023-04-22 20:45:56.899351   \n",
      "25%             NaN     3.000000  2023-07-23 08:45:56.912400640   \n",
      "50%             NaN     5.000000  2023-10-21 20:45:56.920348928   \n",
      "75%             NaN     6.000000  2024-01-18 20:45:56.923352064   \n",
      "max             NaN    17.000000     2024-04-19 20:45:56.927349   \n",
      "std             NaN     2.258317                            NaN   \n",
      "\n",
      "        Conversion_Rate          CTR   Income_log      Age_log  Day_of_Week  \\\n",
      "count       9543.000000  9543.000000  9543.000000  9543.000000  9543.000000   \n",
      "unique              NaN          NaN          NaN          NaN          NaN   \n",
      "top                 NaN          NaN          NaN          NaN          NaN   \n",
      "freq                NaN          NaN          NaN          NaN          NaN   \n",
      "mean           0.202256     0.050354    10.734944     3.527792     3.013203   \n",
      "min            0.001000     0.000000     8.907256     2.397895     0.000000   \n",
      "25%            0.108600     0.036900    10.523712     3.295837     1.000000   \n",
      "50%            0.180000     0.050300    10.825343     3.583519     3.000000   \n",
      "75%            0.275350     0.063700    11.054719     3.828641     5.000000   \n",
      "max            0.731700     0.127200    11.476735     4.465908     6.000000   \n",
      "std            0.121675     0.019890     0.474438     0.402814     1.983593   \n",
      "\n",
      "          Hour  Gender_encoded  Location_encoded  Ad_Type_encoded  \\\n",
      "count   9543.0     9543.000000        9543.00000      9543.000000   \n",
      "unique     NaN             NaN               NaN              NaN   \n",
      "top        NaN             NaN               NaN              NaN   \n",
      "freq       NaN             NaN               NaN              NaN   \n",
      "mean      20.0        0.708268           0.98994         1.492508   \n",
      "min       20.0        0.000000           0.00000         0.000000   \n",
      "25%       20.0        0.000000           0.00000         0.000000   \n",
      "50%       20.0        1.000000           1.00000         1.000000   \n",
      "75%       20.0        1.000000           2.00000         3.000000   \n",
      "max       20.0        2.000000           2.00000         3.000000   \n",
      "std        0.0        0.645821           0.81776         1.126518   \n",
      "\n",
      "        Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "count        9543.000000           9543.000000  \n",
      "unique               NaN                   NaN  \n",
      "top                  NaN                   NaN  \n",
      "freq                 NaN                   NaN  \n",
      "mean            2.491250              1.002305  \n",
      "min             0.000000              0.000000  \n",
      "25%             1.000000              0.000000  \n",
      "50%             2.000000              1.000000  \n",
      "75%             4.000000              2.000000  \n",
      "max             5.000000              2.000000  \n",
      "std             1.710835              0.816536  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning and preprocessing\n",
    "df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "# df = generate_example_data(n=5000)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ORIGINAL DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "df = clean_data(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CLEANED AND LOGGED DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = engineer_time_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('TIME ENGINEERED COLUMN')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = encode_categorical_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ENCODED CATEGORICAL VARIABLES')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('DESCRIPTION OF DF AFTER PREPROCESSING')\n",
    "print(\"=\"*60)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c30c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST-STAGE SUMMARY (Clicks ~ Z + X):\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Clicks   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.8155\n",
      "Date:                Thu, 13 Nov 2025   Prob (F-statistic):              0.589\n",
      "Time:                        16:33:58   Log-Likelihood:                -21311.\n",
      "No. Observations:                9543   AIC:                         4.264e+04\n",
      "Df Residuals:                    9534   BIC:                         4.270e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                    4.2969      0.573      7.494      0.000       3.173       5.421\n",
      "Ad_Type_encoded         -0.0081      0.021     -0.391      0.695      -0.049       0.032\n",
      "Ad_Placement_encoded    -0.0280      0.028     -1.000      0.317      -0.083       0.027\n",
      "Ad_Topic_encoded         0.0190      0.014      1.399      0.162      -0.008       0.046\n",
      "Day_of_Week              0.0006      0.012      0.049      0.961      -0.023       0.024\n",
      "Age_log                  0.0146      0.057      0.255      0.798      -0.097       0.126\n",
      "Income_log               0.0644      0.049      1.305      0.192      -0.032       0.161\n",
      "Gender_encoded           0.0107      0.036      0.301      0.763      -0.059       0.081\n",
      "Location_encoded        -0.0318      0.028     -1.128      0.259      -0.087       0.023\n",
      "==============================================================================\n",
      "Omnibus:                      291.250   Durbin-Watson:                   2.035\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              318.089\n",
      "Skew:                           0.437   Prob(JB):                     8.47e-70\n",
      "Kurtosis:                       3.192   Cond. No.                         302.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "Partial F-statistic for instruments (relevance): 0.77\n",
      "\n",
      "SECOND-STAGE IV SUMMARY (Conversion_Rate ~ Clicks_hat + X):\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:        Conversion_Rate   R-squared:                     -0.2450\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                -0.2457\n",
      "No. Observations:                9543   F-statistic:                    2.1999\n",
      "Date:                Thu, Nov 13 2025   P-value (F-stat)                0.8209\n",
      "Time:                        16:33:59   Distribution:                  chi2(5)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                                Parameter Estimates                                 \n",
      "====================================================================================\n",
      "                  Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------------\n",
      "const                0.2984     0.1508     1.9790     0.0478      0.0029      0.5939\n",
      "Age_log             -0.0020     0.0035    -0.5702     0.5685     -0.0088      0.0048\n",
      "Income_log           0.0040     0.0037     1.0683     0.2854     -0.0033      0.0113\n",
      "Gender_encoded       0.0005     0.0022     0.2242     0.8226     -0.0038      0.0047\n",
      "Location_encoded    -0.0021     0.0020    -1.0182     0.3086     -0.0060      0.0019\n",
      "Clicks              -0.0259     0.0344    -0.7550     0.4502     -0.0933      0.0414\n",
      "====================================================================================\n",
      "\n",
      "Endogenous: Clicks\n",
      "Instruments: Ad_Type_encoded, Ad_Placement_encoded, Ad_Topic_encoded, Day_of_Week\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n",
      "\n",
      "Correlation(Clicks, Clicks_hat): 0.026\n",
      "N: 9543 | Instruments: 4 | Controls: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Core packages\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "# 1) Prepare data columns\n",
    "# Encoded dummies are assumed present; otherwise create pandas get_dummies for raw categorical columns.\n",
    "\n",
    "# Instruments (Z)\n",
    "Z_cols = [\n",
    "    'Ad_Type_encoded',        # categorical encoding (0..3)\n",
    "    'Ad_Placement_encoded',   # categorical encoding (0..2)\n",
    "    'Ad_Topic_encoded',       # categorical encoding (0..5)\n",
    "    'Day_of_Week'             # 0..6\n",
    "    # 'Hour'  # include only if it varies; drop if constant\n",
    "]\n",
    "\n",
    "# Exogenous controls (X)\n",
    "X_cols = [\n",
    "    'Age_log',\n",
    "    'Income_log',\n",
    "    'Gender_encoded',\n",
    "    'Location_encoded'\n",
    "]\n",
    "\n",
    "# Outcome (Y) and endogenous regressor (D)\n",
    "Y_col = 'Conversion_Rate'\n",
    "D_col = 'Clicks'\n",
    "\n",
    "# Drop rows with missing values in required columns\n",
    "cols_needed = Z_cols + X_cols + [Y_col, D_col]\n",
    "df2 = df.dropna(subset=cols_needed).copy()\n",
    "\n",
    "# Optional: add interactions to strengthen first-stage relevance (comment/uncomment)\n",
    "# df2['Type_x_Placement'] = df2['Ad_Type_encoded'] * df2['Ad_Placement_encoded']\n",
    "# df2['Topic_x_DOW'] = df2['Ad_Topic_encoded'] * df2['Day_of_Week']\n",
    "# Z_cols += ['Type_x_Placement', 'Topic_x_DOW']\n",
    "\n",
    "# 2) First-stage OLS: Clicks on instruments + controls\n",
    "Z = df2[Z_cols]\n",
    "X = df2[X_cols]\n",
    "D = df2[D_col].astype(float)\n",
    "\n",
    "# Add constant\n",
    "FS_design = sm.add_constant(pd.concat([Z, X], axis=1))\n",
    "fs_model = sm.OLS(D, FS_design).fit(cov_type='HC1')  # robust SE (White)\n",
    "\n",
    "print(\"FIRST-STAGE SUMMARY (Clicks ~ Z + X):\")\n",
    "print(fs_model.summary())\n",
    "\n",
    "# First-stage F-stat for instruments (partial F): compute using nested models\n",
    "FS_design_ZX = FS_design\n",
    "FS_design_X_only = sm.add_constant(X)\n",
    "\n",
    "restricted = sm.OLS(D, FS_design_X_only).fit(cov_type='HC1')\n",
    "unrestricted = fs_model\n",
    "\n",
    "SSR_r = np.sum(restricted.resid**2)\n",
    "SSR_ur = np.sum(unrestricted.resid**2)\n",
    "q = FS_design_ZX.shape[1] - FS_design_X_only.shape[1]  # number of instrument parameters\n",
    "n = FS_design_ZX.shape[0]\n",
    "k = FS_design_ZX.shape[1]  # total parameters\n",
    "\n",
    "F_partial = ((SSR_r - SSR_ur) / q) / (SSR_ur / (n - k))\n",
    "print(f\"\\nPartial F-statistic for instruments (relevance): {F_partial:.2f}\")\n",
    "\n",
    "# Predicted Clicks\n",
    "df2['Clicks_hat'] = unrestricted.predict(FS_design)\n",
    "\n",
    "# 3) Second-stage IV: Conversion_Rate on Clicks instrumented by (Z), with controls (X)\n",
    "Y = df2[Y_col].astype(float)\n",
    "\n",
    "# Build matrices for IV2SLS\n",
    "iv_exog = sm.add_constant(df2[X_cols])            # controls + constant\n",
    "iv_endog = df2[[D_col]]                           # endogenous regressor\n",
    "iv_instr = df2[Z_cols]                            # instruments\n",
    "\n",
    "iv_model = IV2SLS(Y, iv_exog, iv_endog, iv_instr).fit(cov_type='robust')\n",
    "\n",
    "print(\"\\nSECOND-STAGE IV SUMMARY (Conversion_Rate ~ Clicks_hat + X):\")\n",
    "print(iv_model.summary)\n",
    "\n",
    "# 4) Sanity checks\n",
    "corr_clicks_hat = np.corrcoef(df2['Clicks'], df2['Clicks_hat'])[0, 1]\n",
    "print(f\"\\nCorrelation(Clicks, Clicks_hat): {corr_clicks_hat:.3f}\")\n",
    "print(f\"N: {len(df2)} | Instruments: {len(Z_cols)} | Controls: {len(X_cols)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
