{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d640f",
   "metadata": {},
   "source": [
    "Previous file was getting too chunky. This one has just the finalized complete pipeline for the ml feature engineered thang for 2SLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885348d4",
   "metadata": {},
   "source": [
    "##### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from linearmodels.iv import IV2SLS\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1d516",
   "metadata": {},
   "source": [
    "Thought vomit\n",
    "What are my knowns?\n",
    "\n",
    "I want to show the data initially, and then also show that clicks is endogenous to conversion rate.\n",
    "\n",
    "Once i'm there I want to show my data isn't that clean and make a cleaned dataset.\n",
    "\n",
    "So there needs to be a preprocessing step that handles bad data, and logs stuff appropriately.\n",
    "\n",
    "Now. I want to make the interaction terms and such.\n",
    "\n",
    "Once that is all done I will create a machine learning instrument from enhanced features.\n",
    "\n",
    "Then it seems I'm foggy on how the instrument creation and 2sls will work with each other.\n",
    "\n",
    "Plus the whole point of this is to try to integrate Raj Chetty's 2014 forecast bias with value-added estimates into this somehow.\n",
    "\n",
    "I want to make sure my entire implementation is proper. Even if the results are null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee7a9e",
   "metadata": {},
   "source": [
    "##### Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63393a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data before analysis.\n",
    "    \n",
    "    Performs:\n",
    "    1. Handle negative income values\n",
    "    2. Impute missing income with median\n",
    "    3. Winsorize income at 1st and 99th percentiles\n",
    "    4. Filter age to plausible range (10-90 years)\n",
    "    5. Create logarithmic transformations for skewed variables\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. CLEAN INCOME\n",
    "    # =====================================================================\n",
    "    if 'Income' in df.columns:\n",
    "        # Convert negative income to missing\n",
    "        neg_income_count = (df['Income'] < 0).sum()\n",
    "        df.loc[df['Income'] < 0, 'Income'] = np.nan\n",
    "        \n",
    "        if neg_income_count > 0:\n",
    "            print(f\"âœ“ Converted {neg_income_count} negative income values to missing\")\n",
    "        \n",
    "        # Impute missing income with median\n",
    "        missing_income = df['Income'].isna().sum()\n",
    "        if missing_income > 0:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df['Income'] = imputer.fit_transform(df[['Income']])\n",
    "            print(f\"âœ“ Imputed {missing_income} missing income values with median\")\n",
    "        \n",
    "        # Winsorize: Cap extremes at 1st and 99th percentile\n",
    "        lower, upper = df['Income'].quantile([0.01, 0.99])\n",
    "        income_before = df['Income'].copy()\n",
    "        df['Income'] = df['Income'].clip(lower, upper)\n",
    "        winsorized = (income_before != df['Income']).sum()\n",
    "        print(f\"âœ“ Winsorized {winsorized} income values at 1st/99th percentiles\")\n",
    "        print(f\"  Income range: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. FILTER AGE\n",
    "    # =====================================================================\n",
    "    if 'Age' in df.columns:\n",
    "        age_before = len(df)\n",
    "        df = df[df['Age'].between(10, 90)]\n",
    "        age_filtered = age_before - len(df)\n",
    "        if age_filtered > 0:\n",
    "            print(f\"âœ“ Filtered {age_filtered} rows with implausible ages (keeping 10-90)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. CREATE LOGARITHMIC TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    print(f\"\\nðŸ“Š Creating logarithmic transformations:\")\n",
    "    \n",
    "    # Log of Income (if positive)\n",
    "    if 'Income' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['Income'])\n",
    "        print(f\"  âœ“ Income_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of Clicks (if exists and positive)\n",
    "    if 'Clicks' in df.columns:\n",
    "        df['Clicks_log'] = np.log1p(df['Clicks'])\n",
    "        print(f\"  âœ“ Clicks_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of Age (for nonlinear age effects)\n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_log'] = np.log1p(df['Age'])\n",
    "        print(f\"  âœ“ Age_log created (log1p transformation)\")\n",
    "    \n",
    "    # Log of CTR (if exists and positive)\n",
    "    if 'CTR' in df.columns:\n",
    "        # Ensure CTR is positive before log\n",
    "        if (df['CTR'] > 0).all():\n",
    "            df['CTR_log'] = np.log(df['CTR'])\n",
    "            print(f\"  âœ“ CTR_log created (log transformation)\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SUMMARY\n",
    "    # =====================================================================\n",
    "    final_rows = len(df)\n",
    "    rows_removed = initial_rows - final_rows\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLEANING SUMMARY:\")\n",
    "    print(f\"  Initial rows:        {initial_rows:,}\")\n",
    "    print(f\"  Final rows:          {final_rows:,}\")\n",
    "    print(f\"  Rows removed:        {rows_removed:,} ({rows_removed/initial_rows*100:.1f}%)\")\n",
    "    print(f\"  Log variables added: {len([col for col in df.columns if '_log' in col])}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def engineer_time_features(df):\n",
    "    \"\"\"Extract day of week and hour from Click_Time\"\"\"\n",
    "    if 'Click_Time' in df.columns:\n",
    "        df['Click_Time'] = pd.to_datetime(df['Click_Time'])\n",
    "        df['Day_of_Week'] = df['Click_Time'].dt.dayofweek\n",
    "        df['Hour'] = df['Click_Time'].dt.hour\n",
    "    return df\n",
    "    \n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical variables\"\"\"\n",
    "    categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            # df[col] = le # This line was wierd, idk why it did this, but i may have to revert things back.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# gonna hold off on this interaction term stuff for now.\n",
    "def engineer_instrument_features(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Create rich features that predict clicks but don't directly affect conversions.\n",
    "    \n",
    "    This is crucial for instrument strength. We create:\n",
    "    1. Interaction features between ad characteristics and demographics\n",
    "    2. Time-based features (weekend, business hours)\n",
    "    3. Nonlinear transformations\n",
    "    4. Complex interactions between multiple variables\n",
    "    \n",
    "    Key principle: These features should predict CLICKS well, but only affect\n",
    "    CONVERSIONS through clicks (exclusion restriction).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 1. AD CHARACTERISTICS Ã— DEMOGRAPHICS INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics respond differently to ad types\n",
    "    \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Type_encoded']):\n",
    "        df['Income_x_AdType'] = df['Income'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Income Ã— Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Topic_encoded']):\n",
    "        df['Age_x_AdTopic'] = df['Age'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Age Ã— Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded']):\n",
    "        df['Income_x_Placement'] = df['Income'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Income Ã— Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Placement_encoded']):\n",
    "        df['Age_x_Placement'] = df['Age'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Age Ã— Ad Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 2. TIME-BASED FEATURES AND INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Click patterns vary by time of day/week\n",
    "    \n",
    "    if 'Day_of_Week' in df.columns:\n",
    "        df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "        print(\"âœ“ Created Weekend indicator\")\n",
    "        \n",
    "    if 'Hour' in df.columns:\n",
    "        df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "        df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "        df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "        print(\"âœ“ Created time-of-day indicators\")\n",
    "    \n",
    "    # Time Ã— Ad interactions\n",
    "    if all(col in df.columns for col in ['Weekend', 'Ad_Type_encoded']):\n",
    "        df['Weekend_x_AdType'] = df['Weekend'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Weekend Ã— Ad Type interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['BusinessHours', 'Ad_Placement_encoded']):\n",
    "        df['BusinessHours_x_Placement'] = df['BusinessHours'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Business Hours Ã— Ad Placement interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Evening', 'Ad_Topic_encoded']):\n",
    "        df['Evening_x_AdTopic'] = df['Evening'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Evening Ã— Ad Topic interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 3. DEMOGRAPHICS Ã— TIME INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Different demographics have different browsing patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Hour']):\n",
    "        df['Age_x_Hour'] = df['Age'] * df['Hour']\n",
    "        print(\"âœ“ Created Age Ã— Hour interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Weekend']):\n",
    "        df['Income_x_Weekend'] = df['Income'] * df['Weekend']\n",
    "        print(\"âœ“ Created Income Ã— Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Age', 'BusinessHours']):\n",
    "        df['Age_x_BusinessHours'] = df['Age'] * df['BusinessHours']\n",
    "        print(\"âœ“ Created Age Ã— Business Hours interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 4. NONLINEAR TRANSFORMATIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Relationships may be nonlinear (using log-transformed versions)\n",
    "    \n",
    "    if 'Age_log' in df.columns:\n",
    "        df['Age_squared'] = df['Age'] ** 2\n",
    "        print(\"âœ“ Created Age squared\")\n",
    "        \n",
    "    if 'Income_log' in df.columns:\n",
    "        df['Income_squared'] = df['Income'] ** 2\n",
    "        df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "        print(\"âœ“ Created Income squared and sqrt\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 5. COMPLEX CATEGORICAL INTERACTIONS\n",
    "    # =====================================================================\n",
    "    # Rationale: Certain combinations may be particularly predictive\n",
    "    \n",
    "    # Location Ã— Demographics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Age']):\n",
    "        df['Location_x_Age'] = df['Location_encoded'] * df['Age']\n",
    "        print(\"âœ“ Created Location Ã— Age interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Income']):\n",
    "        df['Location_x_Income'] = df['Location_encoded'] * df['Income']\n",
    "        print(\"âœ“ Created Location Ã— Income interaction\")\n",
    "    \n",
    "    # Location Ã— Ad characteristics\n",
    "    if all(col in df.columns for col in ['Location_encoded', 'Ad_Placement_encoded']):\n",
    "        df['Location_x_Placement'] = df['Location_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Location Ã— Placement interaction\")\n",
    "    \n",
    "    # Gender Ã— Ad characteristics\n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Topic_encoded']):\n",
    "        df['Gender_x_AdTopic'] = df['Gender_encoded'] * df['Ad_Topic_encoded']\n",
    "        print(\"âœ“ Created Gender Ã— Ad Topic interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Gender_encoded', 'Ad_Type_encoded']):\n",
    "        df['Gender_x_AdType'] = df['Gender_encoded'] * df['Ad_Type_encoded']\n",
    "        print(\"âœ“ Created Gender Ã— Ad Type interaction\")\n",
    "    \n",
    "    # Ad Type Ã— Placement (different placements work for different types)\n",
    "    if all(col in df.columns for col in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "        df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "        print(\"âœ“ Created Ad Type Ã— Placement interaction\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # 6. THREE-WAY INTERACTIONS (most powerful)\n",
    "    # =====================================================================\n",
    "    # Rationale: Capture complex patterns\n",
    "    \n",
    "    if all(col in df.columns for col in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "        df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "        print(\"âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\")\n",
    "        \n",
    "    if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "        df['Income_x_Placement_x_BizHours'] = df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "        print(\"âœ“ Created Income Ã— Placement Ã— Business Hours interaction\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ccd05",
   "metadata": {},
   "source": [
    "##### Preprocessing End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415e8b1",
   "metadata": {},
   "source": [
    "##### Create ML Instrument Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8602ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_instrument(df, model_type='stacking', cv_folds=5, use_enhanced_features=False):\n",
    "    \"\"\"\n",
    "    Generate ML-based instrument for Clicks using ensemble methods.\n",
    "    Returns a new DataFrame with 'Clicks_predicted' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Define instrument features (strictly policy-side, not outcomes!) ---\n",
    "    base_features = [\n",
    "        'Age', 'Income',\n",
    "        'Gender_encoded', 'Location_encoded',\n",
    "        'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "        'Day_of_Week', 'Hour'\n",
    "    ]\n",
    "\n",
    "    enhanced_features = [\n",
    "        # Interactions\n",
    "        'Income_x_AdType', 'Age_x_AdTopic', 'Income_x_Placement', 'Age_x_Placement',\n",
    "        'Weekend_x_AdType', 'BusinessHours_x_Placement', 'Evening_x_AdTopic',\n",
    "        'Age_x_Hour', 'Income_x_Weekend', 'Age_x_BusinessHours',\n",
    "        'Location_x_Age', 'Location_x_Income', 'Location_x_Placement',\n",
    "        'Gender_x_AdTopic', 'Gender_x_AdType', 'AdType_x_Placement',\n",
    "        'Age_x_AdType_x_Weekend', 'Income_x_Placement_x_BizHours',\n",
    "        # Time features\n",
    "        'Weekend', 'BusinessHours', 'Evening', 'Morning',\n",
    "        # Nonlinear (now using cleaned log versions) NOTE only created if i log age, and income...\n",
    "        'Age_squared', 'Age_log', 'Income_log', 'Income_squared', 'Income_sqrt',\n",
    "        'Clicks_log', 'CTR_log'\n",
    "    ]\n",
    "\n",
    "    if use_enhanced_features:\n",
    "        # Assume you have a separate function to engineer features\n",
    "        df = engineer_instrument_features(df)\n",
    "        instrument_features = base_features + enhanced_features\n",
    "    else:\n",
    "        instrument_features = base_features\n",
    "\n",
    "    # Filter available features\n",
    "    available_features = [f for f in instrument_features if f in df.columns]\n",
    "    X = df[available_features]\n",
    "    y = df['Clicks']\n",
    "\n",
    "    # --- Step 2: Build model ---\n",
    "    if model_type == 'stacking':\n",
    "        base_models = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=200, random_state=42))\n",
    "        ]\n",
    "        try:\n",
    "            from xgboost import XGBRegressor\n",
    "            base_models.append(('xgb', XGBRegressor(n_estimators=200, random_state=42, n_jobs=-1)))\n",
    "        except ImportError:\n",
    "            pass\n",
    "        model = StackingRegressor(estimators=base_models, final_estimator=Ridge(alpha=1.0), cv=cv_folds, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'rf':\n",
    "        model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "    elif model_type == 'gb':\n",
    "        model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose 'stacking', 'rf', or 'gb'.\")\n",
    "\n",
    "    # --- Step 3: Generate out-of-fold predictions ---\n",
    "    clicks_pred = cross_val_predict(model, X, y, cv=cv_folds, n_jobs=-1)\n",
    "    df = df.copy()\n",
    "    df['Clicks_predicted'] = clicks_pred\n",
    "\n",
    "    # --- Step 4: Fit final model (optional, for diagnostics) ---\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return df, X, model\n",
    "\n",
    "def enhanced_instrument_diagnostics(df, X, y, model):\n",
    "    \"\"\"\n",
    "    Functional: Comprehensive instrument strength testing with Stock-Yogo critical values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'Clicks' and 'Clicks_predicted' columns.\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix used in first-stage model.\n",
    "    y : pd.Series or np.array\n",
    "        True clicks (endogenous regressor).\n",
    "    model : fitted sklearn model\n",
    "        First-stage ML model used to generate instruments.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract instrument (Z) and endogenous regressor (D) ---\n",
    "    if 'Clicks_predicted' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Clicks_predicted' column\")\n",
    "    z = df['Clicks_predicted'].values\n",
    "    d = df['Clicks'].values\n",
    "\n",
    "    n = len(d)\n",
    "    k = X.shape[1]\n",
    "\n",
    "    # --- First-stage RÂ² and F-statistic ---\n",
    "    d_resid = d - d.mean()\n",
    "    ss_tot = np.sum(d_resid**2)\n",
    "    ss_res = np.sum((d - z)**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    f_stat = (r_squared / 1) / ((1 - r_squared) / (n - k - 1))\n",
    "\n",
    "    # --- Correlation ---\n",
    "    corr = np.corrcoef(z, d)[0, 1]\n",
    "\n",
    "    # --- Cragg-Donald statistic ---\n",
    "    cragg_donald = n * r_squared\n",
    "\n",
    "    # --- Display results ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"\\nSAMPLE INFORMATION:\")\n",
    "    print(f\"  Sample size (n):              {n:,}\")\n",
    "    print(f\"  Number of features (k):       {k}\")\n",
    "    print(\"\\nFIRST-STAGE PERFORMANCE:\")\n",
    "    print(f\"  R-squared:                    {r_squared:.4f}\")\n",
    "    print(f\"  Correlation (Z, D):           {corr:.4f}\")\n",
    "    print(f\"  F-statistic:                  {f_stat:.2f}\")\n",
    "    print(f\"  Cragg-Donald statistic:       {cragg_donald:.2f}\")\n",
    "\n",
    "    print(\"\\nBENCHMARKS & INTERPRETATION:\")\n",
    "    print(f\"  {'Criterion':<35} {'Threshold':<12} {'Status'}\")\n",
    "    print(f\"  {'-'*35} {'-'*12} {'-'*20}\")\n",
    "    weak_status = \"âœ“ STRONG\" if f_stat > 10 else \"âœ— WEAK\"\n",
    "    print(f\"  {'Weak Instrument (F < 10)':<35} {'10.00':<12} {weak_status}\")\n",
    "    sy_10_status = \"âœ“âœ“ EXCELLENT\" if f_stat > 16.38 else \"âœ— Below threshold\"\n",
    "    sy_15_status = \"âœ“ GOOD\" if f_stat > 8.96 else \"âœ— Below threshold\"\n",
    "    print(f\"  {'Stock-Yogo 10% max bias':<35} {'16.38':<12} {sy_10_status}\")\n",
    "    print(f\"  {'Stock-Yogo 15% max bias':<35} {'8.96':<12} {sy_15_status}\")\n",
    "\n",
    "    print(\"\\nOVERALL ASSESSMENT:\")\n",
    "    if f_stat > 16.38:\n",
    "        print(\"  âœ“âœ“ VERY STRONG INSTRUMENT\")\n",
    "        print(\"     Maximum IV bias < 10% of OLS bias\")\n",
    "    elif f_stat > 10:\n",
    "        print(\"  âœ“ STRONG INSTRUMENT\")\n",
    "        print(\"     Acceptable for causal inference\")\n",
    "    elif f_stat > 5:\n",
    "        print(\"  âš  MODERATELY WEAK INSTRUMENT\")\n",
    "        print(\"     Proceed with caution\")\n",
    "    else:\n",
    "        print(\"  âœ— WEAK INSTRUMENT\")\n",
    "        print(\"     Results may be unreliable\")\n",
    "\n",
    "    # --- Feature importance (if available) ---\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING CLICKS:\")\n",
    "        importances = model.feature_importances_\n",
    "        top_features = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for i, (feat, imp) in enumerate(top_features, 1):\n",
    "            print(f\"  {i:2d}. {feat:35s} {imp:.4f}\")\n",
    "    elif hasattr(model, 'final_estimator_'):\n",
    "        print(\"\\nâ„¹ Stacking ensemble used - feature importances not directly available\")\n",
    "\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5b313",
   "metadata": {},
   "source": [
    "##### Create ML Instrument End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5d99",
   "metadata": {},
   "source": [
    "##### 2SLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486a9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=None,\n",
    "    include_interactions=False,\n",
    "    add_constant=True,\n",
    "    cluster_col=None,\n",
    "    cov_type='robust'  # 'robust' for HC, or 'cluster' if cluster_col is set\n",
    "):\n",
    "    \"\"\"\n",
    "    Functional 2SLS using linearmodels.iv.IV2SLS.\n",
    "\n",
    "    Model:\n",
    "        First stage: D = Ï€0 + Ï€1 Z + Î“ X + Î½\n",
    "        Second stage: Y = Î± + Î² D + Î˜ X + Îµ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain y_col, d_col, z_col, and any control columns.\n",
    "    y_col : str\n",
    "        Outcome column (e.g., 'Conversion_Rate').\n",
    "    d_col : str\n",
    "        Endogenous regressor (e.g., 'Clicks').\n",
    "    z_col : str\n",
    "        Instrument column (e.g., 'Clicks_predicted' from ML first stage).\n",
    "    base_controls : list[str] or None\n",
    "        Exogenous controls. Do NOT include outcome-adjacent variables like CTR.\n",
    "        Recommended: ['Age', 'Income', 'Gender_encoded', 'Location_encoded',\n",
    "                      'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'].\n",
    "    include_interactions : bool\n",
    "        If True, include Ad_Type Ã— Ad_Placement interaction (exogenous).\n",
    "    add_constant : bool\n",
    "        If True, add a constant term automatically.\n",
    "    cluster_col : str or None\n",
    "        Column name for cluster-robust SEs (e.g., 'UserID', 'CampaignID').\n",
    "    cov_type : str\n",
    "        'robust' (HC), 'cluster' (requires cluster_col), or 'unadjusted'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : IV2SLSResults\n",
    "        Fitted IV results object from linearmodels.\n",
    "    data_used : pd.DataFrame\n",
    "        DataFrame with columns actually used in estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Validate required columns ---\n",
    "    required = [y_col, d_col, z_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # --- Controls: enforce exogeneity discipline ---\n",
    "    if base_controls is None:\n",
    "        base_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "        ]\n",
    "\n",
    "    # Strict: exclude CTR or any post-click/outcome-adjacent metrics from controls\n",
    "    invalid_controls = [c for c in base_controls if c not in df.columns]\n",
    "    if invalid_controls:\n",
    "        # Warn strictly but proceed with available subset\n",
    "        print(f\"âš  Skipping missing controls: {invalid_controls}\")\n",
    "    controls = [c for c in base_controls if c in df.columns]\n",
    "\n",
    "    # --- Optional exogenous interaction ---\n",
    "    interaction_col = None\n",
    "    if include_interactions:\n",
    "        if ('Ad_Type_encoded' in df.columns) and ('Ad_Placement_encoded' in df.columns):\n",
    "            interaction_col = 'Ad_Type_x_Placement'\n",
    "            if interaction_col not in df.columns:\n",
    "                df = df.copy()\n",
    "                df[interaction_col] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "                print(\"âœ“ Added exogenous interaction: Ad_Type_x_Placement\")\n",
    "            controls.append(interaction_col)\n",
    "        else:\n",
    "            print(\"âš  Interaction requested but required columns not present; skipping.\")\n",
    "\n",
    "    # --- Build data frame used in estimation ---\n",
    "    cols_needed = [y_col, d_col, z_col] + controls\n",
    "    data = df[cols_needed].dropna().copy()\n",
    "    if data.empty:\n",
    "        raise ValueError(\"After dropping NA, no rows remain for estimation.\")\n",
    "\n",
    "    # --- Build formula for IV2SLS ---\n",
    "    # dependent ~ exog + [endog ~ instruments]\n",
    "    exog_formula = ' + '.join(controls) if controls else '1'\n",
    "    if add_constant and exog_formula != '1':\n",
    "        exog_formula = '1 + ' + exog_formula  # linearmodels adds constant via '1 +'\n",
    "    elif add_constant and exog_formula == '1':\n",
    "        # '1' already denotes constant in linearmodels formula\n",
    "        pass\n",
    "    else:\n",
    "        # No constant: use '-1' to suppress intercept if you have exog terms\n",
    "        if controls:\n",
    "            exog_formula = '-1 + ' + ' + '.join(controls)\n",
    "\n",
    "    formula = f\"{y_col} ~ {exog_formula} + [{d_col} ~ {z_col}]\"\n",
    "\n",
    "    # --- Fit IV2SLS ---\n",
    "    if cov_type == 'cluster' and (cluster_col is not None) and (cluster_col in df.columns):\n",
    "        clusters = df.loc[data.index, cluster_col]\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='clustered', clusters=clusters)\n",
    "    else:\n",
    "        if cov_type == 'cluster' and cluster_col is None:\n",
    "            print(\"âš  cov_type='cluster' requested but no cluster_col provided; defaulting to robust.\")\n",
    "        results = IV2SLS.from_formula(formula, data=data).fit(cov_type='robust' if cov_type != 'unadjusted' else 'unadjusted')\n",
    "\n",
    "    # --- Strict reporting ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary)\n",
    "\n",
    "    # Optional first-stage diagnostics available via .first_stage (dict of RegressionResults)\n",
    "    # Example:\n",
    "    try:\n",
    "        fs = results.first_stage[d_col]\n",
    "        print(\"\\nFirst-stage summary (endogenous regressor: {}):\".format(d_col))\n",
    "        print(f\"  R-squared: {fs.rsquared:.4f}\")\n",
    "        print(f\"  F-statistic (excluded instrument): {getattr(fs, 'f_statistic', None)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return results, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f630374",
   "metadata": {},
   "source": [
    "##### 2SLS End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983708a",
   "metadata": {},
   "source": [
    "##### Stratified 2SLS Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edefc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "def analyze_subgroup_effects_iv(\n",
    "    df,\n",
    "    subgroup_vars=None,\n",
    "    min_subgroup_size=100,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=None,\n",
    "    add_constant=True,\n",
    "    cov_type='robust',         # 'robust', 'unadjusted', or 'cluster'\n",
    "    cluster_col=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Stratified IV2SLS with robust input validation and rank-deficiency pruning.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Required columns ---\n",
    "    required = [y_col, d_col, z_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # --- Controls: strictly exogenous, no post-treatment metrics ---\n",
    "    if base_controls is None:\n",
    "        base_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "        ]\n",
    "    controls_all = [c for c in base_controls if c in df.columns]\n",
    "    if verbose:\n",
    "        missing_controls = [c for c in base_controls if c not in df.columns]\n",
    "        if missing_controls:\n",
    "            print(f\"âš  Skipping missing controls: {missing_controls}\")\n",
    "\n",
    "    # --- Default subgroup spec ---\n",
    "    if subgroup_vars is None:\n",
    "        subgroup_vars = {\n",
    "            'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "            'Age': [0, 35, 50, 65, np.inf],\n",
    "            'Location': None,\n",
    "            'Ad_Type': None\n",
    "        }\n",
    "\n",
    "    # --- Helper: build formula ---\n",
    "    def build_formula(controls_list):\n",
    "        if controls_list:\n",
    "            exog = ' + '.join(controls_list)\n",
    "            exog = ('1 + ' + exog) if add_constant else ('-1 + ' + exog)\n",
    "        else:\n",
    "            exog = '1' if add_constant else '-1'\n",
    "        return f\"{y_col} ~ {exog} + [{d_col} ~ {z_col}]\"\n",
    "\n",
    "    # --- Helper: prune zero-variance controls in subgroup ---\n",
    "    def prune_controls(sub_df, ctrl_cols):\n",
    "        pruned = []\n",
    "        for c in ctrl_cols:\n",
    "            if c not in sub_df.columns:\n",
    "                continue\n",
    "            # Drop if constant (zero variance) in subgroup\n",
    "            if sub_df[c].nunique() <= 1:\n",
    "                continue\n",
    "            pruned.append(c)\n",
    "        return pruned\n",
    "\n",
    "    rows = []\n",
    "    subgroup_keys = subgroup_vars if isinstance(subgroup_vars, list) else list(subgroup_vars.keys())\n",
    "\n",
    "    for var in subgroup_keys:\n",
    "        df_local = df.copy()\n",
    "\n",
    "        # Build subgroup column\n",
    "        if isinstance(subgroup_vars, dict) and subgroup_vars.get(var) is not None:\n",
    "            bins = subgroup_vars[var]\n",
    "            if not isinstance(bins, (list, tuple)) or len(bins) < 2:\n",
    "                if verbose: print(f\"âš  Invalid bins for {var}; skipping.\")\n",
    "                continue\n",
    "            labels = [f\"{var}_{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "            try:\n",
    "                df_local[f'{var}_subgroup'] = pd.cut(\n",
    "                    df_local[var], bins=bins, labels=labels, include_lowest=True\n",
    "                )\n",
    "                subgroup_col = f'{var}_subgroup'\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"âš  Failed to bin {var}: {e}; skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            subgroup_col = var\n",
    "            if subgroup_col not in df_local.columns:\n",
    "                if verbose: print(f\"âš  Subgroup column {subgroup_col} missing; skipping.\")\n",
    "                continue\n",
    "\n",
    "        subgroups = df_local[subgroup_col].dropna().unique()\n",
    "        if len(subgroups) == 0:\n",
    "            if verbose: print(f\"âš  No valid subgroups for {var}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        for sg in subgroups:\n",
    "            dsg = df_local[df_local[subgroup_col] == sg]\n",
    "            n_obs = len(dsg)\n",
    "            if n_obs < min_subgroup_size:\n",
    "                continue\n",
    "\n",
    "            # Ensure variation in instrument and endogenous regressor\n",
    "            if dsg[z_col].nunique() <= 1 or dsg[d_col].nunique() <= 1:\n",
    "                # No first-stage or second-stage variation â†’ skip\n",
    "                continue\n",
    "\n",
    "            # Build and prune controls for this subgroup\n",
    "            controls = prune_controls(dsg, controls_all)\n",
    "\n",
    "            # Build estimation data\n",
    "            cols_needed = [y_col, d_col, z_col] + controls\n",
    "            data = dsg[cols_needed].dropna()\n",
    "            n_used = len(data)\n",
    "            if n_used < min_subgroup_size:\n",
    "                continue\n",
    "\n",
    "            # Final sanity after NA drop: instrument/endog still vary?\n",
    "            if data[z_col].nunique() <= 1 or data[d_col].nunique() <= 1:\n",
    "                continue\n",
    "\n",
    "            formula = build_formula(controls)\n",
    "\n",
    "            try:\n",
    "                if cov_type == 'cluster' and cluster_col and (cluster_col in dsg.columns):\n",
    "                    clusters = dsg.loc[data.index, cluster_col]\n",
    "                    res = IV2SLS.from_formula(formula, data=data).fit(\n",
    "                        cov_type='clustered', clusters=clusters\n",
    "                    )\n",
    "                else:\n",
    "                    if cov_type == 'cluster' and not cluster_col and verbose:\n",
    "                        print(\"âš  cov_type='cluster' requested without cluster_col; defaulting to robust.\")\n",
    "                    res = IV2SLS.from_formula(formula, data=data).fit(\n",
    "                        cov_type='robust' if cov_type != 'unadjusted' else 'unadjusted'\n",
    "                    )\n",
    "\n",
    "                # Extract second-stage estimates\n",
    "                beta = res.params.get(d_col, np.nan)\n",
    "                se = res.std_errors.get(d_col, np.nan)\n",
    "                pval = res.pvalues.get(d_col, np.nan)\n",
    "                ci_lower = beta - 1.96 * se if pd.notnull(se) else np.nan\n",
    "                ci_upper = beta + 1.96 * se if pd.notnull(se) else np.nan\n",
    "                significant = bool(pd.notnull(pval) and pval < 0.05)\n",
    "\n",
    "                # First-stage diagnostics\n",
    "                fs = res.first_stage\n",
    "                # If multiple endogenous regressors, res.first_stage is a dict\n",
    "                if isinstance(fs, dict):\n",
    "                    fs = fs[d_col]  # select Clicks' first-stage results\n",
    "                fs_r2 = getattr(fs, 'rsquared', np.nan)\n",
    "                fs_fstat_obj = getattr(fs, 'f_statistic', None)\n",
    "                if fs_fstat_obj is not None and hasattr(fs_fstat_obj, 'stat'):\n",
    "                    fs_f = float(fs_fstat_obj.stat)\n",
    "                else:\n",
    "                    fs_f = np.nan\n",
    "                instrument_weak = bool(pd.notnull(fs_f) and fs_f < 10)\n",
    "\n",
    "                rows.append({\n",
    "                    'Variable': var,\n",
    "                    'Subgroup': str(sg),\n",
    "                    'N': n_obs,\n",
    "                    'N_Used': n_used,\n",
    "                    'Controls_Used': ','.join(controls) if controls else '(none)',\n",
    "                    'First_Stage_R2': fs_r2,\n",
    "                    'First_Stage_F': fs_f,\n",
    "                    'Instrument_Weak': instrument_weak,\n",
    "                    'Beta': beta,\n",
    "                    'Std_Error': se,\n",
    "                    'P_Value': pval,\n",
    "                    'CI_Lower': ci_lower,\n",
    "                    'CI_Upper': ci_upper,\n",
    "                    'Significant': significant\n",
    "                })\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"âœ— Error in subgroup '{var}={sg}': {e}\")\n",
    "                continue\n",
    "\n",
    "    if not rows:\n",
    "        if verbose: print(\"âš  No subgroups estimated successfully.\")\n",
    "        return None\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out['Abs_Effect'] = out['Beta'].abs()\n",
    "    out = out.sort_values('Abs_Effect', ascending=False)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97952f6c",
   "metadata": {},
   "source": [
    "##### Stratified 2SLS End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_data(n=2000):\n",
    "    \"\"\"Generate synthetic data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Age': np.random.randint(18, 65, n),\n",
    "        'Gender': np.random.choice(['M', 'F'], n),\n",
    "        'Income': np.random.randint(30000, 150000, n),\n",
    "        'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "        'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "        'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "        'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "        'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "    })\n",
    "    \n",
    "    # Normalize income to reasonable scale\n",
    "    data['Income'] = data['Income'] / 100000  # Scale to 0.3-1.5 range\n",
    "    \n",
    "    # Generate clicks with realistic structure\n",
    "    clicks_base = (\n",
    "        0.5 +  # baseline\n",
    "        0.3 * (data['Ad_Type'] == 'Video').astype(float) +\n",
    "        0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float) +\n",
    "        0.01 * data['Age'] +\n",
    "        0.2 * data['Income'] +\n",
    "        np.random.randn(n) * 0.5\n",
    "    )\n",
    "    data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "    # Generate CTR (correlated with clicks but not in instrument)\n",
    "    data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "    # Generate conversion rate with causal effect from clicks\n",
    "    # Plus confounding through unobserved factors\n",
    "    unobserved_confounder = np.random.randn(n) * 0.05\n",
    "    \n",
    "    conversion_base = (\n",
    "        0.05 +  # baseline\n",
    "        0.08 * data['Clicks'] +  # TRUE CAUSAL EFFECT\n",
    "        0.02 * data['Income'] +\n",
    "        0.005 * data['Age'] +\n",
    "        0.3 * data['CTR'] +\n",
    "        unobserved_confounder +\n",
    "        np.random.randn(n) * 0.03\n",
    "    )\n",
    "    data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "    # Add endogeneity: unobserved confounder affects clicks too\n",
    "    data['Clicks'] = data['Clicks'] + unobserved_confounder * 2\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d540b7",
   "metadata": {},
   "source": [
    "##### Start of Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be069528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORIGINAL DATASET\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                   Click_Time  Conversion_Rate     CTR  \n",
      "0  2024-01-18 20:45:56.898459           0.0981  0.0737  \n",
      "1  2023-04-24 20:45:56.898459           0.0937  0.0592  \n",
      "2  2024-02-24 20:45:56.898459           0.1912  0.0563  \n",
      "3  2023-12-13 20:45:56.898459           0.1122  0.0232  \n",
      "4  2023-07-02 20:45:56.898459           0.1426  0.0539  \n",
      "\n",
      "============================================================\n",
      "DATA CLEANING AND PREPROCESSING\n",
      "============================================================\n",
      "âœ“ Converted 70 negative income values to missing\n",
      "âœ“ Imputed 70 missing income values with median\n",
      "âœ“ Winsorized 200 income values at 1st/99th percentiles\n",
      "  Income range: [7,384, 96,445]\n",
      "âœ“ Filtered 457 rows with implausible ages (keeping 10-90)\n",
      "\n",
      "ðŸ“Š Creating logarithmic transformations:\n",
      "  âœ“ Income_log created (log1p transformation)\n",
      "  âœ“ Clicks_log created (log1p transformation)\n",
      "  âœ“ Age_log created (log1p transformation)\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY:\n",
      "  Initial rows:        10,000\n",
      "  Final rows:          9,543\n",
      "  Rows removed:        457 (4.6%)\n",
      "  Log variables added: 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLEANED AND LOGGED DATASET\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                   Click_Time  Conversion_Rate     CTR  Income_log  \\\n",
      "0  2024-01-18 20:45:56.898459           0.0981  0.0737   10.483422   \n",
      "1  2023-04-24 20:45:56.898459           0.0937  0.0592   10.767521   \n",
      "2  2024-02-24 20:45:56.898459           0.1912  0.0563   11.129134   \n",
      "3  2023-12-13 20:45:56.898459           0.1122  0.0232   11.075764   \n",
      "4  2023-07-02 20:45:56.898459           0.1426  0.0539    9.957521   \n",
      "\n",
      "   Clicks_log   Age_log  \n",
      "0    1.386294  4.127134  \n",
      "1    1.791759  3.737670  \n",
      "2    1.609438  3.912023  \n",
      "3    1.945910  4.234107  \n",
      "4    1.791759  4.158883  \n",
      "\n",
      "============================================================\n",
      "TIME ENGINEERED COLUMN\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                  Click_Time  Conversion_Rate     CTR  Income_log  Clicks_log  \\\n",
      "0 2024-01-18 20:45:56.898459           0.0981  0.0737   10.483422    1.386294   \n",
      "1 2023-04-24 20:45:56.898459           0.0937  0.0592   10.767521    1.791759   \n",
      "2 2024-02-24 20:45:56.898459           0.1912  0.0563   11.129134    1.609438   \n",
      "3 2023-12-13 20:45:56.898459           0.1122  0.0232   11.075764    1.945910   \n",
      "4 2023-07-02 20:45:56.898459           0.1426  0.0539    9.957521    1.791759   \n",
      "\n",
      "    Age_log  Day_of_Week  Hour  \n",
      "0  4.127134            3    20  \n",
      "1  3.737670            0    20  \n",
      "2  3.912023            5    20  \n",
      "3  4.234107            2    20  \n",
      "4  4.158883            6    20  \n",
      "\n",
      "============================================================\n",
      "ENCODED CATEGORICAL VARIABLES\n",
      "============================================================\n",
      "   Age  Gender    Income  Location Ad_Type Ad_Topic   Ad_Placement  Clicks  \\\n",
      "0   61    Male  35717.43     Urban  Banner   Travel   Social Media       3   \n",
      "1   41    Male  47453.25     Rural   Video   Travel  Search Engine       5   \n",
      "2   49  Female  68126.35     Rural    Text     Food   Social Media       4   \n",
      "3   68  Female  64585.73  Suburban    Text   Health        Website       6   \n",
      "4   63    Male  21109.40     Urban  Native  Fashion  Search Engine       5   \n",
      "\n",
      "                  Click_Time  Conversion_Rate  ...  Income_log  Clicks_log  \\\n",
      "0 2024-01-18 20:45:56.898459           0.0981  ...   10.483422    1.386294   \n",
      "1 2023-04-24 20:45:56.898459           0.0937  ...   10.767521    1.791759   \n",
      "2 2024-02-24 20:45:56.898459           0.1912  ...   11.129134    1.609438   \n",
      "3 2023-12-13 20:45:56.898459           0.1122  ...   11.075764    1.945910   \n",
      "4 2023-07-02 20:45:56.898459           0.1426  ...    9.957521    1.791759   \n",
      "\n",
      "    Age_log  Day_of_Week  Hour  Gender_encoded  Location_encoded  \\\n",
      "0  4.127134            3    20               1                 2   \n",
      "1  3.737670            0    20               1                 0   \n",
      "2  3.912023            5    20               0                 0   \n",
      "3  4.234107            2    20               0                 1   \n",
      "4  4.158883            6    20               1                 2   \n",
      "\n",
      "   Ad_Type_encoded  Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "0                0                 5                     1  \n",
      "1                3                 5                     0  \n",
      "2                2                 2                     1  \n",
      "3                2                 3                     2  \n",
      "4                1                 0                     0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "============================================================\n",
      "DESCRIPTION OF DF AFTER PREPROCESSING\n",
      "============================================================\n",
      "                Age Gender        Income Location Ad_Type Ad_Topic  \\\n",
      "count   9543.000000   9543   9543.000000     9543    9543     9543   \n",
      "unique          NaN      3           NaN        3       4        6   \n",
      "top             NaN   Male           NaN    Rural  Banner  Finance   \n",
      "freq            NaN   4751           NaN     3239    2460     1637   \n",
      "mean      35.690035    NaN  50410.732326      NaN     NaN      NaN   \n",
      "min       10.000000    NaN   7384.365600      NaN     NaN      NaN   \n",
      "25%       26.000000    NaN  37185.925000      NaN     NaN      NaN   \n",
      "50%       35.000000    NaN  50278.030000      NaN     NaN      NaN   \n",
      "75%       45.000000    NaN  63240.720000      NaN     NaN      NaN   \n",
      "max       86.000000    NaN  96444.632200      NaN     NaN      NaN   \n",
      "std       13.451463    NaN  19129.415676      NaN     NaN      NaN   \n",
      "\n",
      "       Ad_Placement       Clicks                     Click_Time  \\\n",
      "count          9543  9543.000000                           9543   \n",
      "unique            3          NaN                            NaN   \n",
      "top         Website          NaN                            NaN   \n",
      "freq           3192          NaN                            NaN   \n",
      "mean            NaN     5.024730  2023-10-21 14:53:36.391400192   \n",
      "min             NaN     0.000000     2023-04-22 20:45:56.899351   \n",
      "25%             NaN     3.000000  2023-07-23 08:45:56.912400640   \n",
      "50%             NaN     5.000000  2023-10-21 20:45:56.920348928   \n",
      "75%             NaN     6.000000  2024-01-18 20:45:56.923352064   \n",
      "max             NaN    17.000000     2024-04-19 20:45:56.927349   \n",
      "std             NaN     2.258317                            NaN   \n",
      "\n",
      "        Conversion_Rate  ...   Income_log   Clicks_log      Age_log  \\\n",
      "count       9543.000000  ...  9543.000000  9543.000000  9543.000000   \n",
      "unique              NaN  ...          NaN          NaN          NaN   \n",
      "top                 NaN  ...          NaN          NaN          NaN   \n",
      "freq                NaN  ...          NaN          NaN          NaN   \n",
      "mean           0.202256  ...    10.734944     1.716707     3.527792   \n",
      "min            0.001000  ...     8.907256     0.000000     2.397895   \n",
      "25%            0.108600  ...    10.523712     1.386294     3.295837   \n",
      "50%            0.180000  ...    10.825343     1.791759     3.583519   \n",
      "75%            0.275350  ...    11.054719     1.945910     3.828641   \n",
      "max            0.731700  ...    11.476735     2.890372     4.465908   \n",
      "std            0.121675  ...     0.474438     0.420485     0.402814   \n",
      "\n",
      "        Day_of_Week    Hour  Gender_encoded  Location_encoded  \\\n",
      "count   9543.000000  9543.0     9543.000000        9543.00000   \n",
      "unique          NaN     NaN             NaN               NaN   \n",
      "top             NaN     NaN             NaN               NaN   \n",
      "freq            NaN     NaN             NaN               NaN   \n",
      "mean       3.013203    20.0        0.708268           0.98994   \n",
      "min        0.000000    20.0        0.000000           0.00000   \n",
      "25%        1.000000    20.0        0.000000           0.00000   \n",
      "50%        3.000000    20.0        1.000000           1.00000   \n",
      "75%        5.000000    20.0        1.000000           2.00000   \n",
      "max        6.000000    20.0        2.000000           2.00000   \n",
      "std        1.983593     0.0        0.645821           0.81776   \n",
      "\n",
      "        Ad_Type_encoded  Ad_Topic_encoded  Ad_Placement_encoded  \n",
      "count       9543.000000       9543.000000           9543.000000  \n",
      "unique              NaN               NaN                   NaN  \n",
      "top                 NaN               NaN                   NaN  \n",
      "freq                NaN               NaN                   NaN  \n",
      "mean           1.492508          2.491250              1.002305  \n",
      "min            0.000000          0.000000              0.000000  \n",
      "25%            0.000000          1.000000              0.000000  \n",
      "50%            1.000000          2.000000              1.000000  \n",
      "75%            3.000000          4.000000              2.000000  \n",
      "max            3.000000          5.000000              2.000000  \n",
      "std            1.126518          1.710835              0.816536  \n",
      "\n",
      "[11 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and preprocessing\n",
    "df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "# df = generate_example_data(n=5000)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ORIGINAL DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = clean_data(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CLEANED AND LOGGED DATASET')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = engineer_time_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('TIME ENGINEERED COLUMN')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "df = encode_categorical_features(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ENCODED CATEGORICAL VARIABLES')\n",
    "print(\"=\"*60)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('DESCRIPTION OF DF AFTER PREPROCESSING')\n",
    "print(\"=\"*60)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c214b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING ML INSTRUMENT\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\n",
      "============================================================\n",
      "âœ“ Created Income Ã— Ad Type interaction\n",
      "âœ“ Created Age Ã— Ad Topic interaction\n",
      "âœ“ Created Income Ã— Ad Placement interaction\n",
      "âœ“ Created Age Ã— Ad Placement interaction\n",
      "âœ“ Created Weekend indicator\n",
      "âœ“ Created time-of-day indicators\n",
      "âœ“ Created Weekend Ã— Ad Type interaction\n",
      "âœ“ Created Business Hours Ã— Ad Placement interaction\n",
      "âœ“ Created Evening Ã— Ad Topic interaction\n",
      "âœ“ Created Age Ã— Hour interaction\n",
      "âœ“ Created Income Ã— Weekend interaction\n",
      "âœ“ Created Age Ã— Business Hours interaction\n",
      "âœ“ Created Age squared\n",
      "âœ“ Created Income squared and sqrt\n",
      "âœ“ Created Location Ã— Age interaction\n",
      "âœ“ Created Location Ã— Income interaction\n",
      "âœ“ Created Location Ã— Placement interaction\n",
      "âœ“ Created Gender Ã— Ad Topic interaction\n",
      "âœ“ Created Gender Ã— Ad Type interaction\n",
      "âœ“ Created Ad Type Ã— Placement interaction\n",
      "âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\n",
      "âœ“ Created Income Ã— Placement Ã— Business Hours interaction\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML instrument creation took about 1m40secs with real data\n",
    "# ML instrument creation took about 3m8secs with synthetic data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('CREATING ML INSTRUMENT')\n",
    "print(\"=\"*60)\n",
    "df, X, model = create_ml_instrument(df, use_enhanced_features=True)\n",
    "# NOTE Remember that the instrument is really weak when created not using the interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ML INSTRUMENT DIAGNOSTICS\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "SAMPLE INFORMATION:\n",
      "  Sample size (n):              9,543\n",
      "  Number of features (k):       37\n",
      "\n",
      "FIRST-STAGE PERFORMANCE:\n",
      "  R-squared:                    0.9998\n",
      "  Correlation (Z, D):           0.9999\n",
      "  F-statistic:                  63116170.43\n",
      "  Cragg-Donald statistic:       9541.56\n",
      "\n",
      "BENCHMARKS & INTERPRETATION:\n",
      "  Criterion                           Threshold    Status\n",
      "  ----------------------------------- ------------ --------------------\n",
      "  Weak Instrument (F < 10)            10.00        âœ“ STRONG\n",
      "  Stock-Yogo 10% max bias             16.38        âœ“âœ“ EXCELLENT\n",
      "  Stock-Yogo 15% max bias             8.96         âœ“ GOOD\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  âœ“âœ“ VERY STRONG INSTRUMENT\n",
      "     Maximum IV bias < 10% of OLS bias\n",
      "\n",
      "â„¹ Stacking ensemble used - feature importances not directly available\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# diagnostics for ml instrument strength\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('ML INSTRUMENT DIAGNOSTICS')\n",
    "print(\"=\"*60)\n",
    "enhanced_instrument_diagnostics(df, X, df['Clicks'], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589c0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added exogenous interaction: Ad_Type_x_Placement\n",
      "\n",
      "======================================================================\n",
      "2SLS ESTIMATION SUMMARY (linearmodels.iv.IV2SLS)\n",
      "======================================================================\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:        Conversion_Rate   R-squared:                      0.0006\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                -0.0004\n",
      "No. Observations:                9543   F-statistic:                    5.2175\n",
      "Date:                Wed, Nov 12 2025   P-value (F-stat)                0.8149\n",
      "Time:                        12:17:54   Distribution:                  chi2(9)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                                  Parameter Estimates                                   \n",
      "========================================================================================\n",
      "                      Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                0.1995     0.0069     28.970     0.0000      0.1860      0.2130\n",
      "Age                  -7.117e-05  9.322e-05    -0.7635     0.4452     -0.0003      0.0001\n",
      "Income                6.295e-08  6.549e-08     0.9611     0.3365  -6.542e-08   1.913e-07\n",
      "Gender_encoded           0.0002     0.0019     0.1089     0.9133     -0.0036      0.0040\n",
      "Location_encoded        -0.0012     0.0015    -0.7809     0.4349     -0.0042      0.0018\n",
      "Ad_Type_encoded         -0.0005     0.0017    -0.3169     0.7513     -0.0039      0.0028\n",
      "Ad_Topic_encoded        -0.0004     0.0007    -0.4844     0.6281     -0.0018      0.0011\n",
      "Ad_Placement_encoded    -0.0003     0.0025    -0.1381     0.8901     -0.0053      0.0046\n",
      "Ad_Type_x_Placement      0.0009     0.0013     0.6662     0.5053     -0.0017      0.0035\n",
      "Clicks                   0.0008     0.0006     1.3752     0.1691     -0.0003      0.0019\n",
      "========================================================================================\n",
      "\n",
      "Endogenous: Clicks\n",
      "Instruments: Clicks_predicted\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# 2SLS with ml featured instrument\n",
    "# The r-squared value is super tiny with real data for whatever reason.\n",
    "# However, with my synthetic data I actually get a good r-squared score and a small p-value.\n",
    "# Perhpas the move is to end the real data stuff there, but continue with Raj Chetty methodlogies with the\n",
    "# synthetic data for reaserch sake.\n",
    "\n",
    "controls = [\n",
    "    'Age', 'Income',\n",
    "    'Gender_encoded', 'Location_encoded',\n",
    "    'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "]\n",
    "\n",
    "results, data_used = run_2sls(\n",
    "    df,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=controls,\n",
    "    include_interactions=True,   # optional\n",
    "    add_constant=True,\n",
    "    cov_type='robust'            # or 'cluster' with cluster_col='CampaignID'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a002b7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Subgroup  N_Used     Beta  P_Value  First_Stage_F  Instrument_Weak\n",
      "  Income Income_30000-50000    3248 0.001916 0.054200            NaN            False\n",
      " Ad_Type               Text    2356 0.001605 0.170903            NaN            False\n",
      " Ad_Type             Banner    2460 0.001221 0.261627            NaN            False\n",
      "     Age          Age_35-50    3261 0.001156 0.224826            NaN            False\n",
      "Location           Suburban    3161 0.001114 0.256089            NaN            False\n",
      "     Age          Age_50-65    1185 0.000766 0.625189            NaN            False\n",
      "Location              Urban    3143 0.000761 0.427995            NaN            False\n",
      "     Age           Age_0-35    4918 0.000730 0.350965            NaN            False\n",
      " Ad_Type             Native    2325 0.000670 0.560855            NaN            False\n",
      "Location              Rural    3239 0.000470 0.625379            NaN            False\n"
     ]
    }
   ],
   "source": [
    "# stratified 2sls time\n",
    "controls = [\n",
    "    'Age', 'Income',\n",
    "    'Gender_encoded', 'Location_encoded',\n",
    "    'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded'\n",
    "]\n",
    "\n",
    "results_df = analyze_subgroup_effects_iv(\n",
    "    df,\n",
    "    subgroup_vars={'Income': [0, 30000, 50000, 70000, np.inf], 'Age': [0, 35, 50, 65, np.inf], 'Location': None, 'Ad_Type': None},\n",
    "    min_subgroup_size=200,\n",
    "    y_col='Conversion_Rate',\n",
    "    d_col='Clicks',\n",
    "    z_col='Clicks_predicted',\n",
    "    base_controls=controls,\n",
    "    add_constant=True,\n",
    "    cov_type='robust'\n",
    ")\n",
    "\n",
    "# Inspect strongest effects\n",
    "if results_df is not None:\n",
    "    print(results_df[['Variable','Subgroup','N_Used','Beta','P_Value','First_Stage_F','Instrument_Weak']].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8966eda",
   "metadata": {},
   "source": [
    "##### End of Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
