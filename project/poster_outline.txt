Part I: Empirical Limits with Real Data
Title idea: â€œThe Endogeneity of Clicks: Limits of Instrumental Variable Approaches in Digital Advertisingâ€

Motivation:
Clicks are widely used as a proxy for ad effectiveness, but they are endogenous (correlated with unobserved user intent).
OLS estimates are biased; IV/2SLS requires strong instruments.

Empirical setup:
Dataset: demographics, ad features, clicks, conversions.

Instruments tested: Ad_Type, Ad_Placement, Ad_Topic, Day_of_Week.
ML feature engineering attempted to strengthen instruments.

Findings:
First-stage F-statistics are weak.
Correlation between predicted and actual clicks is near zero.
Even with ML, instruments remain weak.

Conclusion: In practice, observational ad data struggles to support IV inference.

Contribution:
Demonstrates the empirical difficulty of finding strong instruments in digital ads.
Provides diagnostics (Stockâ€“Yogo, Craggâ€“Donald) as a cautionary tale for applied researchers.

##############################################################################################################

Part II: Conceptual Demonstration with Synthetic Data
Title idea: â€œForecast Bias in Click-Based Measures: A Simulation Inspired by Chetty et al. (2014)â€

Motivation:
Even if instruments were strong, are clicks unbiased forecasts of conversions?
Chettyâ€™s forecast bias framework in education provides a template.

Synthetic design:
Generate data where clicks are endogenous to latent user interest.
Define ad â€œeventsâ€ (e.g., placement/type changes) analogous to teacher turnover.
Compare forecasted conversions (based on clicks) to actual conversions.

Forecast bias test:
Calibration slope: regress actual conversions on predicted conversions.

Event-study validation: forecasted vs. realized changes when ad features shift.
Show systematic overprediction (e.g., flashy ads with high clicks but low conversions).

Findings:
OLS forecasts are biased (systematic misprediction).
2SLS forecasts correct bias when instruments are strong (in simulation).
Demonstrates how forecast bias manifests in ad settings.

Contribution:
Extends Chettyâ€™s framework to digital advertising.
Provides a conceptual lens: clicks may mislead as forecasts, even beyond endogeneity.
Highlights the need for experimental variation in ad studies.

##############################################################################################################

Conclusion / Integration
Part I: Real-world ad data â†’ instruments are weak, IV inference unreliable.

Part II: Synthetic data â†’ forecast bias framework shows why clicks can mislead as forecasts.

Overall message:
Empirically: caution in using clicks as causal regressors.
Conceptually: clicks may suffer forecast bias, requiring better measures or experimental designs.
Managerial implication: donâ€™t equate clicks with conversions; design studies to validate predictive measures.

ğŸ¯ Why this works
Part I grounds the paper in real data and shows the empirical challenge.
Part II elevates the paper by connecting to Chettyâ€™s influential framework, even if synthetic.
Together, they tell a coherent story: â€œClicks are endogenous and weakly instrumented in practice; conceptually,
they may also suffer forecast bias.â€

#################POSTER TIME#############################################################################################

ğŸ§© Poster Layout Strategy for Your Two-Part Study
Hereâ€™s how to map your study onto the sections of the Weber State template:

ğŸ”¹ Title of Research Project
â€œClicks, Instruments, and Forecast Bias: Evaluating Causal Inference in Digital Advertisingâ€ Keep it jargon-free but signal both empirical and conceptual depth.

ğŸ”¹ Authors Line
Include your name, department, and any collaborators or advisors.

ğŸ”¹ Introduction
Frame the problem and motivation:

Clicks are widely used to measure ad effectiveness.
But theyâ€™re endogenous and may mislead causal inference.

You explore this in two parts: real-world instrument diagnostics and a synthetic forecast bias simulation.

ğŸ”¹ Research Goal
Split into two bullets:

Part I: Test whether ML-enhanced instruments can reliably identify causal effects of clicks on conversions.

Part II: Demonstrate how forecast bias can distort click-based predictions using synthetic data.

ğŸ”¹ Methods
Divide into two sub-panels:

Part I: Real Data Instrument Diagnostics
Dataset: 9,543 ad impressions with demographics, ad features, clicks, conversions.

ML feature engineering to predict Clicks.
2SLS vs. OLS comparison using Ad_Type, Ad_Placement, Ad_Topic, Day_of_Week as instruments.
Instrument strength diagnostics (F-stat, Craggâ€“Donald, correlation).

Part II: Synthetic Forecast Bias Simulation
Simulated ad cohorts with endogenous click behavior.

Forecast conversion rates using OLS and 2SLS.
Apply Chetty-style calibration slope and event-based validation.
Measure systematic misprediction (forecast bias).

ğŸ”¹ Figures and Results
Use 3â€“4 visual panels:

First-stage diagnostics (real data):
Bar chart of F-statistics across models.
Scatter plot of Clicks vs. predicted Clicks (low correlation).

2SLS vs. OLS estimates (real data):
Coefficient comparison with confidence intervals.
Highlight instability due to weak instruments.

Forecast bias calibration (synthetic):
Plot: actual vs. predicted conversions across cohorts.
Show slope < 1 â†’ overprediction.

Event-study panel (synthetic):
Forecasted vs. realized conversion changes when ad features switch.

ğŸ”¹ Conclusion
Summarize each part:

Part I: ML-enhanced instruments are still weak in practice; causal inference from clicks is fragile.

Part II: Forecast bias can systematically distort click-based predictions, even with strong instruments.

Implication: Clicks are not just endogenous â€” they may be unreliable forecasters. Better instruments or 
experimental designs are needed.

ğŸ”¹ Acknowledgments
Credit advisors, data providers, or funding sources.

ğŸ”¹ References
Include Chetty et al. (2014), key econometrics texts, and any ML or ad analytics papers you cite.


NICE .

Following this Jupyter Notebook, please write up exerpts on these sections listed below.

Introduction, Research Goal, Methods, Figures and Results, Conclusion, Acknowledgements, and References?

##############################################POSTER OUTLINE BEGINNING#################################################
ğŸ§¾ Poster Abstract
Clicks, Instruments, and Forecast Bias: Evaluating Causal Inference in Digital Advertising

This study investigates the reliability of click-based measures in estimating and forecasting ad effectiveness. 
In Part I, we simulate ad impression data to compare Ordinary Least Squares (OLS) and Two-Stage Least Squares (2SLS)
estimates of the causal effect of clicks on conversion rates. Using machine learningâ€“engineered instruments derived
from ad features, we demonstrate that strong instruments can recover the true causal effect, while OLS suffers from 
endogeneity bias.

In Part II, we adapt Raj Chettyâ€™s forecast bias framework to the advertising domain. We test whether 
click-based forecasts systematically mispredict conversion outcomes across ad configurations. Using synthetic data with 
known causal structure, we conduct two validation exercises: (1) a calibration slope test showing that OLS forecasts are 
biased (slope â‰ˆ 0.32) while 2SLS forecasts are unbiased (slope â‰ˆ 1.03); and (2) an event-style analysis comparing 
forecasted vs. actual conversion changes following simulated ad feature â€œshocks.â€ Results show that 2SLS corrects 
forecast bias, while OLS consistently overpredicts.

Together, these findings highlight the dual role of endogeneity: it distorts both causal inference and predictive 
validity. We conclude that clicks may be unreliable as forecasting measures unless instrumented properly, and we propose 
the forecast bias framework as a diagnostic tool for validating predictive metrics in digital advertising.




ğŸŸ£ Introduction
Clicks are widely used to measure ad effectiveness, yet they are often endogenous â€” influenced by latent user intent and 
ad targeting. This endogeneity undermines causal inference and may distort predictions of conversion outcomes. Inspired 
by Raj Chettyâ€™s forecast bias framework in education, this study investigates whether clicks are reliable both as causal 
regressors and as forecasting tools in digital advertising.

ğŸŸ£ Research Goal
Part I: Evaluate whether machine learningâ€“enhanced instruments can recover the true causal effect of clicks on 
conversions using 2SLS.

Part II: Test whether click-based forecasts systematically mispredict conversion outcomes using Chetty-style validation 
techniques.

ğŸŸ£ Methods
We simulate synthetic ad impression data with known causal structure:
Endogeneity: Clicks are influenced by latent user intent.
Instruments: Ad_Type Ã— Ad_Placement combinations are used to predict clicks.
Outcome: Conversion_Rate is generated with a true causal effect of clicks.

Two models are fit:
OLS: Treats clicks as exogenous.
2SLS: Uses instruments to correct for endogeneity.

We apply two forecast bias diagnostics:
Calibration slope test: Regress actual conversion rates on predicted rates across ad cohorts.
Event-style validation: Simulate ad configuration â€œshocksâ€ and compare forecasted vs. actual changes in conversion rates.

ğŸŸ£ Figures and Results
Instrument strength: ML-enhanced instruments yield strong first-stage F-statistics.
Calibration slope:
OLS slope â‰ˆ 0.32 â†’ systematic overprediction.
2SLS slope â‰ˆ 1.03 â†’ unbiased forecasts.

Event-style validation:
OLS forecasts overshoot actual conversion changes.
2SLS forecasts align closely with observed outcomes.

These results demonstrate that 2SLS corrects both causal bias and forecast bias when instruments are strong.

ğŸŸ£ Conclusion
This study shows that endogeneity in click data distorts both causal inference and predictive accuracy. While OLS overestimates conversion effects and mispredicts outcomes, 2SLS â€” when supported by strong instruments â€” recovers the true causal effect and produces calibrated forecasts. We adapt Raj Chettyâ€™s forecast bias framework to digital advertising, offering a diagnostic lens for validating predictive metrics in observational settings.

ğŸŸ£ Acknowledgements
We thank the developers of the linearmodels and statsmodels Python libraries, and acknowledge Raj Chettyâ€™s foundational work on forecast bias in education, which inspired this framework. Gratitude also goes to mentors and peers who provided feedback on early drafts and simulation design.

ğŸŸ£ References
Chetty, R., Friedman, J. N., & Rockoff, J. E. (2014). Measuring the impacts of teachers II: Teacher value-added and student outcomes in adulthood. American Economic Review, 104(9), 2633â€“2679.
Stock, J. H., & Yogo, M. (2005). Testing for weak instruments in linear IV regression. Identification and Inference for Econometric Models.
Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data. MIT Press.
Python packages: linearmodels, statsmodels, pandas, numpy, matplotlib, seaborn.
##############################################POSTER OUTLINE END#################################################
