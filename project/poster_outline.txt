Part I: Empirical Limits with Real Data
Title idea: â€œThe Endogeneity of Clicks: Limits of Instrumental Variable Approaches in Digital Advertisingâ€

Motivation:
Clicks are widely used as a proxy for ad effectiveness, but they are endogenous (correlated with unobserved user intent).
OLS estimates are biased; IV/2SLS requires strong instruments.

Empirical setup:
Dataset: demographics, ad features, clicks, conversions.

Instruments tested: Ad_Type, Ad_Placement, Ad_Topic, Day_of_Week.
ML feature engineering attempted to strengthen instruments.

Findings:
First-stage F-statistics are weak.
Correlation between predicted and actual clicks is near zero.
Even with ML, instruments remain weak.

Conclusion: In practice, observational ad data struggles to support IV inference.

Contribution:
Demonstrates the empirical difficulty of finding strong instruments in digital ads.
Provides diagnostics (Stockâ€“Yogo, Craggâ€“Donald) as a cautionary tale for applied researchers.

##############################################################################################################

Part II: Conceptual Demonstration with Synthetic Data
Title idea: â€œForecast Bias in Click-Based Measures: A Simulation Inspired by Chetty et al. (2014)â€

Motivation:
Even if instruments were strong, are clicks unbiased forecasts of conversions?
Chettyâ€™s forecast bias framework in education provides a template.

Synthetic design:
Generate data where clicks are endogenous to latent user interest.
Define ad â€œeventsâ€ (e.g., placement/type changes) analogous to teacher turnover.
Compare forecasted conversions (based on clicks) to actual conversions.

Forecast bias test:
Calibration slope: regress actual conversions on predicted conversions.

Event-study validation: forecasted vs. realized changes when ad features shift.
Show systematic overprediction (e.g., flashy ads with high clicks but low conversions).

Findings:
OLS forecasts are biased (systematic misprediction).
2SLS forecasts correct bias when instruments are strong (in simulation).
Demonstrates how forecast bias manifests in ad settings.

Contribution:
Extends Chettyâ€™s framework to digital advertising.
Provides a conceptual lens: clicks may mislead as forecasts, even beyond endogeneity.
Highlights the need for experimental variation in ad studies.

##############################################################################################################

Conclusion / Integration
Part I: Real-world ad data â†’ instruments are weak, IV inference unreliable.

Part II: Synthetic data â†’ forecast bias framework shows why clicks can mislead as forecasts.

Overall message:
Empirically: caution in using clicks as causal regressors.
Conceptually: clicks may suffer forecast bias, requiring better measures or experimental designs.
Managerial implication: donâ€™t equate clicks with conversions; design studies to validate predictive measures.

ğŸ¯ Why this works
Part I grounds the paper in real data and shows the empirical challenge.
Part II elevates the paper by connecting to Chettyâ€™s influential framework, even if synthetic.
Together, they tell a coherent story: â€œClicks are endogenous and weakly instrumented in practice; conceptually,
they may also suffer forecast bias.â€

#################POSTER TIME#############################################################################################

ğŸ§© Poster Layout Strategy for Your Two-Part Study
Hereâ€™s how to map your study onto the sections of the Weber State template:

ğŸ”¹ Title of Research Project
â€œClicks, Instruments, and Forecast Bias: Evaluating Causal Inference in Digital Advertisingâ€ Keep it jargon-free but signal both empirical and conceptual depth.

ğŸ”¹ Authors Line
Include your name, department, and any collaborators or advisors.

ğŸ”¹ Introduction
Frame the problem and motivation:

Clicks are widely used to measure ad effectiveness.
But theyâ€™re endogenous and may mislead causal inference.

You explore this in two parts: real-world instrument diagnostics and a synthetic forecast bias simulation.

ğŸ”¹ Research Goal
Split into two bullets:

Part I: Test whether ML-enhanced instruments can reliably identify causal effects of clicks on conversions.

Part II: Demonstrate how forecast bias can distort click-based predictions using synthetic data.

ğŸ”¹ Methods
Divide into two sub-panels:

Part I: Real Data Instrument Diagnostics
Dataset: 9,543 ad impressions with demographics, ad features, clicks, conversions.

ML feature engineering to predict Clicks.
2SLS vs. OLS comparison using Ad_Type, Ad_Placement, Ad_Topic, Day_of_Week as instruments.
Instrument strength diagnostics (F-stat, Craggâ€“Donald, correlation).

Part II: Synthetic Forecast Bias Simulation
Simulated ad cohorts with endogenous click behavior.

Forecast conversion rates using OLS and 2SLS.
Apply Chetty-style calibration slope and event-based validation.
Measure systematic misprediction (forecast bias).

ğŸ”¹ Figures and Results
Use 3â€“4 visual panels:

First-stage diagnostics (real data):
Bar chart of F-statistics across models.
Scatter plot of Clicks vs. predicted Clicks (low correlation).

2SLS vs. OLS estimates (real data):
Coefficient comparison with confidence intervals.
Highlight instability due to weak instruments.

Forecast bias calibration (synthetic):
Plot: actual vs. predicted conversions across cohorts.
Show slope < 1 â†’ overprediction.

Event-study panel (synthetic):
Forecasted vs. realized conversion changes when ad features switch.

ğŸ”¹ Conclusion
Summarize each part:

Part I: ML-enhanced instruments are still weak in practice; causal inference from clicks is fragile.

Part II: Forecast bias can systematically distort click-based predictions, even with strong instruments.

Implication: Clicks are not just endogenous â€” they may be unreliable forecasters. Better instruments or 
experimental designs are needed.

ğŸ”¹ Acknowledgments
Credit advisors, data providers, or funding sources.

ğŸ”¹ References
Include Chetty et al. (2014), key econometrics texts, and any ML or ad analytics papers you cite.


NICE .

Following this Jupyter Notebook, please write up exerpts on these sections listed below.

Introduction, Research Goal, Methods, Figures and Results, Conclusion, Acknowledgements, and References?

##############################################POSTER OUTLINE BEGINNING#################################################
ğŸ§¾ Poster Abstract
Clicks, Instruments, and Forecast Bias: Evaluating Causal Inference in Digital Advertising

This study investigates the reliability of click-based measures in estimating and forecasting ad effectiveness. 
In Part I, we simulate ad impression data to compare Ordinary Least Squares (OLS) and Two-Stage Least Squares (2SLS)
estimates of the causal effect of clicks on conversion rates. Using machine learningâ€“engineered instruments derived
from ad features, we demonstrate that strong instruments can recover the true causal effect, while OLS suffers from 
endogeneity bias.

Part II: Forecast Bias in Click-Based Predictions Building on Raj Chettyâ€™s (2014) forecast bias framework, we adapt the 
methodology to the advertising domain. Using synthetic data with a known causal structure, we examine whether 
click-based forecasts systematically mispredict conversion outcomes across ad configurations. Two validation exercises 
are conducted: (1) a calibration slope test, where OLS-based forecasts exhibit attenuation bias while 2SLS-based 
forecasts align more closely with the true causal effect; and (2) an event-style analysis, where simulated shocks to ad 
features reveal that OLS forecasts tend to overpredict conversion changes, while 2SLS forecasts reduce bias. Although 
the implementation is preliminary and subject to limitations, results suggest that instrumented approaches can correct 
systematic forecast bias in advertising contexts.

Together, these findings highlight the dual role of endogeneity: it distorts both causal inference and predictive 
validity. We conclude that clicks may be unreliable as forecasting measures unless instrumented properly, and we propose 
the forecast bias framework as a diagnostic tool for validating predictive metrics in digital advertising.



ğŸŸ£ Introduction
Clicks are widely used to measure ad effectiveness, yet they are often endogenous â€” influenced by latent user intent and 
ad targeting. This endogeneity undermines causal inference and may distort predictions of conversion outcomes. Inspired 
by Raj Chettyâ€™s forecast bias framework in education, this study investigates whether clicks are reliable both as causal 
regressors and as forecasting tools in digital advertising.

ğŸŸ£ Research Goal
Part I: Evaluate whether machine learningâ€“enhanced instruments can recover the true causal effect of clicks on 
conversions using 2SLS.

Part II: Test whether click-based forecasts systematically mispredict conversion outcomes using Chetty-style validation 
techniques.

ğŸŸ£ Methods
We simulate synthetic ad impression data with known causal structure:
Endogeneity: Clicks are influenced by latent user intent.
Instruments: Ad_Type Ã— Ad_Placement combinations are used to predict clicks.
Outcome: Conversion_Rate is generated with a true causal effect of clicks.

Two models are fit:
OLS: Treats clicks as exogenous.
2SLS: Uses instruments to correct for endogeneity.

We apply two forecast bias diagnostics:
Calibration slope test: Regress actual conversion rates on predicted rates across ad cohorts.
Event-style validation: Simulate ad configuration â€œshocksâ€ and compare forecasted vs. actual changes in conversion rates.

ğŸŸ£ Figures and Results
Instrument strength: ML-enhanced instruments yield strong first-stage F-statistics.
Calibration slope:
OLS slope â‰ˆ 0.32 â†’ systematic overprediction.
2SLS slope â‰ˆ 1.03 â†’ unbiased forecasts.

Event-style validation:
OLS forecasts overshoot actual conversion changes.
2SLS forecasts align closely with observed outcomes.

These results demonstrate that 2SLS corrects both causal bias and forecast bias when instruments are strong.

ğŸŸ£ Conclusion
This study shows that endogeneity in click data distorts both causal inference and predictive accuracy. While OLS overestimates conversion effects and mispredicts outcomes, 2SLS â€” when supported by strong instruments â€” recovers the true causal effect and produces calibrated forecasts. We adapt Raj Chettyâ€™s forecast bias framework to digital advertising, offering a diagnostic lens for validating predictive metrics in observational settings.

ğŸŸ£ Acknowledgements
We thank the developers of the linearmodels and statsmodels Python libraries, and acknowledge Raj Chettyâ€™s foundational work on forecast bias in education, which inspired this framework. Gratitude also goes to mentors and peers who provided feedback on early drafts and simulation design.

ğŸŸ£ References
Chetty, R., Friedman, J. N., & Rockoff, J. E. (2014). Measuring the impacts of teachers II: Teacher value-added and student outcomes in adulthood. American Economic Review, 104(9), 2633â€“2679.
Stock, J. H., & Yogo, M. (2005). Testing for weak instruments in linear IV regression. Identification and Inference for Econometric Models.
Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data. MIT Press.
Python packages: linearmodels, statsmodels, pandas, numpy, matplotlib, seaborn.

##############################################POSTER OUTLINE END#################################################

ğŸŸ£ Title of Research Project (simple, no jargon)
Clicks, Causality, and Forecast Bias in Digital Ads

Ian Last NameÂ¹, First Name Last NameÂ², First Name Last NameÂ³ Â¹Department of Business Economics, Â²Department of Integrative Biology, Â³Department of Computer Science Weber State University

ğŸŸ£ Introduction
Clicks are widely used to measure ad effectiveness, but theyâ€™re often endogenous â€” shaped by user intent and targeting. This undermines causal inference and may distort predictions. 
Inspired by Raj Chettyâ€™s forecast bias framework in education, we test whether clicks are reliable for both causal estimation and forecasting in digital advertising.

ğŸŸ£ Research Goal
Part I: Estimate the causal effect of clicks on conversions using machine learningâ€“enhanced instruments and 2SLS.

Part II: Diagnose forecast bias in click-based predictions using Chetty-style calibration and event-style validation.

ğŸŸ£ Methods
We simulate synthetic ad impression data with known causal structure inspired by the original dataset:

Endogeneity: Clicks are influenced by latent user intent.
Instruments: Ad_Type Ã— Ad_Placement combinations predict clicks.
Outcome: Conversion_Rate is generated with a true causal effect of clicks.

Two models are fit:

OLS: Treats clicks as exogenous.
2SLS: Uses instruments to correct for endogeneity.

Forecast bias diagnostics:

Calibration slope test: Regress actual conversion rates on predicted rates across ad cohorts.
Event-style validation: Simulate ad configuration â€œshocksâ€ and compare forecasted vs. actual changes.

ğŸŸ£ Figures and Results
Instrument Strength

ML-enhanced instruments yield strong first-stage F-statistics.
Calibration Slope
OLS slope â‰ˆ 0.32 â†’ biased forecasts
2SLS slope â‰ˆ 1.03 â†’ calibrated forecasts

Event-Style Validation
OLS forecasts overshoot actual conversion changes
2SLS forecasts align closely with observed outcomes

Figures to include:
Scatter plot with calibration slopes
Bar chart comparing forecast bias across cohorts

ğŸŸ£ Conclusion
Endogeneity in click data distorts both causal inference and predictive accuracy. OLS overestimates conversion effects and mispredicts outcomes. 2SLS â€” when supported by strong instruments â€” recovers the true causal 
effect and produces calibrated forecasts. We adapt Raj Chettyâ€™s forecast bias framework to digital advertising, offering a diagnostic lens for validating predictive metrics in observational settings.

ğŸŸ£ Acknowledgments
We thank the developers of the linearmodels and statsmodels Python libraries. We also acknowledge Raj Chettyâ€™s foundational work on forecast bias in education, which inspired this framework. Gratitude to mentors and 
peers who provided feedback on early drafts and simulation design.

ğŸŸ£ References
Chetty, R., Friedman, J. N., & Rockoff, J. E. (2014). Measuring the impacts of teachers II: Teacher value-added and student outcomes in adulthood. American Economic Review, 104(9), 2633â€“2679. 
Stock, J. H., & Yogo, M. (2005). Testing for weak instruments in linear IV regression. Python packages: linearmodels, statsmodels, pandas, numpy, matplotlib, seaborn.



##11/16/2025##
Figures and Results Part 2:
â€œThe calibration plot shows whether OLS and 2SLS forecasts are systematically biased across cohorts
 in the crossâ€‘section. The eventâ€‘style validation goes further, simulating a shock to test whether 
 forecasts remain reliable under change. Both measure forecast bias, but calibration is static 
 while event validation is dynamic â€” together they mirror Chettyâ€™s 2014 reliability framework of 
 subgroup calibration and event study validation.â€


 Some help with the final conclusion image...

 ğŸ§© Difference Between â€œFiguresâ€ and â€œResultsâ€
Section	Purpose	Typical Content
Figures	Visual evidence	Charts, diagrams, plots, tables, flowcharts â€” anything that visually 
supports your methods or findings
Results	Interpretation	Textual summary of what the figures show: key findings, patterns, 
statistical significance, effect sizes, etc.
Figures are the raw visuals. Results are the takeaways â€” what those visuals mean in context.

Example: A bar chart showing forecast bias across cohorts goes in Figures. The statement â€œ2SLS 
forecast bias is consistently lower than OLS across all cohortsâ€ goes in Results.

ğŸ§  What to Put in the â€œConclusionâ€ Section
The Conclusion is your final synthesis. It should answer:

What did we learn?

Why does it matter?

What are the limitations?

Whatâ€™s next?

ğŸ”‘ Good structure for your posterâ€™s conclusion:
Summary of findings:

â€œWe find that OLS overstates the effect of clicks due to endogeneity, while 2SLS recovers the
 true causal effect.â€

Forecast bias insight:

â€œChetty-style diagnostics show that 2SLS predictions are better calibrated and more robust to
 simulated shocks.â€

Limitations:

â€œOur validation is based on simulated data and lacks real-time dynamics or external shocks.â€

Future work:

â€œNext steps include applying this framework to real ad data and incorporating longitudinal 
variation.â€

ğŸ–¼ï¸ What Image to Show in the Conclusion Section
The conclusion image should reinforce your main insight. Here are strong options:

Option 1: Final Comparison Plot
A clean bar chart or forest plot comparing:

OLS estimate

2SLS estimate

True causal effect (dotted line at 0.08)

Label: â€œ2SLS recovers the true effect; OLS is biased.â€

Option 2: Forecast Bias Summary
Side-by-side bar chart of average forecast bias (OLS vs 2SLS)

Label: â€œ2SLS predictions are better calibrated across cohorts.â€

Option 3: Schematic Summary
A DAG or flowchart showing:

Instrument â†’ Clicks â†’ Conversion

Confounder affecting both Clicks and Conversion

Label: â€œInstrument corrects for endogeneity in click-based predictions.â€

Would you like help generating one of those conclusion images now? I can create a clean schematic
 or comparison plot that visually wraps up your posterâ€™s story. Just say the word.