{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d640f",
   "metadata": {},
   "source": [
    "Previous file was getting too chunky. This one has just the finalized complete pipeline for the ml feature engineered thang for 2SLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_data(n=2000):\n",
    "    \"\"\"Generate synthetic data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Age': np.random.randint(18, 65, n),\n",
    "        'Gender': np.random.choice(['M', 'F'], n),\n",
    "        'Income': np.random.randint(30000, 150000, n),\n",
    "        'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "        'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "        'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "        'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "        'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "    })\n",
    "    \n",
    "    # Normalize income to reasonable scale\n",
    "    data['Income'] = data['Income'] / 100000  # Scale to 0.3-1.5 range\n",
    "    \n",
    "    # Generate clicks with realistic structure\n",
    "    clicks_base = (\n",
    "        0.5 +  # baseline\n",
    "        0.3 * (data['Ad_Type'] == 'Video').astype(float) +\n",
    "        0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float) +\n",
    "        0.01 * data['Age'] +\n",
    "        0.2 * data['Income'] +\n",
    "        np.random.randn(n) * 0.5\n",
    "    )\n",
    "    data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "    # Generate CTR (correlated with clicks but not in instrument)\n",
    "    data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "    # Generate conversion rate with causal effect from clicks\n",
    "    # Plus confounding through unobserved factors\n",
    "    unobserved_confounder = np.random.randn(n) * 0.05\n",
    "    \n",
    "    conversion_base = (\n",
    "        0.05 +  # baseline\n",
    "        0.08 * data['Clicks'] +  # TRUE CAUSAL EFFECT\n",
    "        0.02 * data['Income'] +\n",
    "        0.005 * data['Age'] +\n",
    "        0.3 * data['CTR'] +\n",
    "        unobserved_confounder +\n",
    "        np.random.randn(n) * 0.03\n",
    "    )\n",
    "    data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "    # Add endogeneity: unobserved confounder affects clicks too\n",
    "    data['Clicks'] = data['Clicks'] + unobserved_confounder * 2\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82af4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA CLEANING AND PREPROCESSING\n",
      "============================================================\n",
      "âœ“ Converted 70 negative income values to missing\n",
      "âœ“ Imputed 70 missing income values with median\n",
      "âœ“ Winsorized 200 income values at 1st/99th percentiles\n",
      "  Income range: [7,384, 96,445]\n",
      "âœ“ Filtered 457 rows with implausible ages (keeping 10-90)\n",
      "\n",
      "ðŸ“Š Creating logarithmic transformations:\n",
      "  âœ“ Income_log created (log1p transformation)\n",
      "  âœ“ Clicks_log created (log1p transformation)\n",
      "  âœ“ Age_log created (log1p transformation)\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY:\n",
      "  Initial rows:        10,000\n",
      "  Final rows:          9,543\n",
      "  Rows removed:        457 (4.6%)\n",
      "  Log variables added: 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\n",
      "============================================================\n",
      "âœ“ Created Income Ã— Ad Type interaction\n",
      "âœ“ Created Age Ã— Ad Topic interaction\n",
      "âœ“ Created Income Ã— Ad Placement interaction\n",
      "âœ“ Created Age Ã— Ad Placement interaction\n",
      "âœ“ Created Weekend indicator\n",
      "âœ“ Created time-of-day indicators\n",
      "âœ“ Created Weekend Ã— Ad Type interaction\n",
      "âœ“ Created Business Hours Ã— Ad Placement interaction\n",
      "âœ“ Created Evening Ã— Ad Topic interaction\n",
      "âœ“ Created Age Ã— Hour interaction\n",
      "âœ“ Created Income Ã— Weekend interaction\n",
      "âœ“ Created Age Ã— Business Hours interaction\n",
      "âœ“ Created Age squared\n",
      "âœ“ Created Income squared and sqrt\n",
      "âœ“ Created Location Ã— Age interaction\n",
      "âœ“ Created Location Ã— Income interaction\n",
      "âœ“ Created Location Ã— Placement interaction\n",
      "âœ“ Created Gender Ã— Ad Topic interaction\n",
      "âœ“ Created Gender Ã— Ad Type interaction\n",
      "âœ“ Created Ad Type Ã— Placement interaction\n",
      "âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\n",
      "âœ“ Created Income Ã— Placement Ã— Business Hours interaction\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ML INSTRUMENT CONSTRUCTION\n",
      "============================================================\n",
      "Total features available: 37\n",
      "Model type: STACKING\n",
      "Cross-validation folds: 5\n",
      "\n",
      "Selecting top 10 features based on model importance...\n",
      "Top features selected:\n",
      "1. Clicks_log\n",
      "2. Age_log\n",
      "3. Age_squared\n",
      "4. Age\n",
      "5. Age_x_Hour\n",
      "6. AdType_x_Placement\n",
      "7. Gender_encoded\n",
      "8. Income\n",
      "9. Age_x_AdTopic\n",
      "10. Ad_Type_encoded\n",
      "\n",
      "Building Stacking Ensemble (strongest option)...\n",
      "  âœ“ Using XGBoost as additional base learner\n",
      "\n",
      "Generating out-of-fold predictions (CV=5)...\n",
      "Fitting final model...\n",
      "\n",
      "======================================================================\n",
      "ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "SAMPLE INFORMATION:\n",
      "  Sample size (n):              9,543\n",
      "  Number of features (k):       10\n",
      "\n",
      "FIRST-STAGE PERFORMANCE:\n",
      "  R-squared:                    0.9995\n",
      "  Correlation (Z, D):           0.9998\n",
      "  F-statistic:                  20308373.22\n",
      "  Cragg-Donald statistic:       9538.52\n",
      "\n",
      "BENCHMARKS & INTERPRETATION:\n",
      "  Criterion                           Threshold    Status\n",
      "  ----------------------------------- ------------ --------------------\n",
      "  Weak Instrument (F < 10)            10.00        âœ“ STRONG\n",
      "  Stock-Yogo 10% max bias             16.38        âœ“âœ“ EXCELLENT\n",
      "  Stock-Yogo 15% max bias             8.96         âœ“ GOOD\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  âœ“âœ“ VERY STRONG INSTRUMENT\n",
      "     Maximum IV bias < 10% of OLS bias\n",
      "     Highly reliable causal inference\n",
      "\n",
      "â„¹ Stacking ensemble used - individual feature importances\n",
      "  not directly available, but all base models contribute\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "INTEGRATED CHETTY VALUE-ADDED ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Split method: time\n",
      "  Split ratio: 50.0% train / 50.0% test\n",
      "  Min group size: 100\n",
      "======================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: DATA SPLITTING\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Time-based split:\n",
      "  Training: 2023-04-22 20:45:56.899351 to 2023-10-21 20:45:56.920349\n",
      "  Testing:  2023-10-21 20:45:56.920349 to 2024-04-19 20:45:56.927349\n",
      "  Training N: 4,771\n",
      "  Testing N:  4,772\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: ESTIMATE VALUE-ADDED (Training Sample)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "STRATIFIED 2SLS: HETEROGENEOUS TREATMENT EFFECTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ANALYZING SUBGROUPS BY: INCOME\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Found 4 subgroups: ['Income_0-30000', 'Income_30000-50000', 'Income_50000-70000', 'Income_70000-inf']\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Income_30000-50000' (N=1,627)\n",
      "     First Stage:\n",
      "       F-stat: 498755.95 âœ“ STRONG\n",
      "       RÂ²: 0.9996\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: 0.000222\n",
      "       SE: 0.001417\n",
      "       95% CI: [-0.002555, 0.002999]\n",
      "       p-value: 0.8754 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Income_50000-70000' (N=1,664)\n",
      "     First Stage:\n",
      "       F-stat: 134562.52 âœ“ STRONG\n",
      "       RÂ²: 0.9986\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000446\n",
      "       SE: 0.001350\n",
      "       95% CI: [-0.003092, 0.002201]\n",
      "       p-value: 0.7414 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Income_70000-inf' (N=772)\n",
      "     First Stage:\n",
      "       F-stat: 134797.40 âœ“ STRONG\n",
      "       RÂ²: 0.9994\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: 0.000841\n",
      "       SE: 0.001999\n",
      "       95% CI: [-0.003076, 0.004759]\n",
      "       p-value: 0.6739 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Income_0-30000' (N=708)\n",
      "     First Stage:\n",
      "       F-stat: 302251.38 âœ“ STRONG\n",
      "       RÂ²: 0.9997\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.002041\n",
      "       SE: 0.002045\n",
      "       95% CI: [-0.006050, 0.001968]\n",
      "       p-value: 0.3186 âœ— Not significant\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ANALYZING SUBGROUPS BY: AGE\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Found 4 subgroups: ['Age_0-35', 'Age_35-50', 'Age_50-65', 'Age_65-inf']\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Age_50-65' (N=602)\n",
      "     First Stage:\n",
      "       F-stat: 246170.81 âœ“ STRONG\n",
      "       RÂ²: 0.9997\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: 0.000698\n",
      "       SE: 0.002316\n",
      "       95% CI: [-0.003842, 0.005237]\n",
      "       p-value: 0.7633 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Age_0-35' (N=2,446)\n",
      "     First Stage:\n",
      "       F-stat: 246647.81 âœ“ STRONG\n",
      "       RÂ²: 0.9989\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000440\n",
      "       SE: 0.001117\n",
      "       95% CI: [-0.002629, 0.001749]\n",
      "       p-value: 0.6935 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Age_35-50' (N=1,628)\n",
      "     First Stage:\n",
      "       F-stat: 502012.42 âœ“ STRONG\n",
      "       RÂ²: 0.9996\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000023\n",
      "       SE: 0.001368\n",
      "       95% CI: [-0.002705, 0.002658]\n",
      "       p-value: 0.9863 âœ— Not significant\n",
      "\n",
      "  âš  Skipping 'Age_65-inf': Only 95 observations (min=100)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ANALYZING SUBGROUPS BY: LOCATION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Found 3 subgroups: ['Rural', 'Suburban', 'Urban']\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Urban' (N=1,593)\n",
      "     First Stage:\n",
      "       F-stat: 1082822.84 âœ“ STRONG\n",
      "       RÂ²: 0.9998\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.001102\n",
      "       SE: 0.001415\n",
      "       95% CI: [-0.003875, 0.001671]\n",
      "       p-value: 0.4361 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Rural' (N=1,624)\n",
      "     First Stage:\n",
      "       F-stat: 518118.71 âœ“ STRONG\n",
      "       RÂ²: 0.9996\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000349\n",
      "       SE: 0.001350\n",
      "       95% CI: [-0.002995, 0.002297]\n",
      "       p-value: 0.7959 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Suburban' (N=1,554)\n",
      "     First Stage:\n",
      "       F-stat: 117676.52 âœ“ STRONG\n",
      "       RÂ²: 0.9984\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: 0.000440\n",
      "       SE: 0.001414\n",
      "       95% CI: [-0.002331, 0.003212]\n",
      "       p-value: 0.7556 âœ— Not significant\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ANALYZING SUBGROUPS BY: AD_TYPE\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Found 4 subgroups: ['Banner', 'Native', 'Text', 'Video']\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Banner' (N=1,242)\n",
      "     First Stage:\n",
      "       F-stat: 290827.80 âœ“ STRONG\n",
      "       RÂ²: 0.9995\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.001119\n",
      "       SE: 0.001524\n",
      "       95% CI: [-0.004105, 0.001868]\n",
      "       p-value: 0.4629 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Video' (N=1,188)\n",
      "     First Stage:\n",
      "       F-stat: 533538.45 âœ“ STRONG\n",
      "       RÂ²: 0.9997\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000141\n",
      "       SE: 0.001599\n",
      "       95% CI: [-0.003275, 0.002993]\n",
      "       p-value: 0.9297 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Text' (N=1,178)\n",
      "     First Stage:\n",
      "       F-stat: 881404.80 âœ“ STRONG\n",
      "       RÂ²: 0.9998\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: 0.000867\n",
      "       SE: 0.001662\n",
      "       95% CI: [-0.002390, 0.004125]\n",
      "       p-value: 0.6019 âœ— Not significant\n",
      "\n",
      "  ðŸ“Š Subgroup: 'Native' (N=1,163)\n",
      "     First Stage:\n",
      "       F-stat: 69400.95 âœ“ STRONG\n",
      "       RÂ²: 0.9979\n",
      "     Second Stage (Causal Effect):\n",
      "       Î²: -0.000884\n",
      "       SE: 0.001667\n",
      "       95% CI: [-0.004151, 0.002382]\n",
      "       p-value: 0.5958 âœ— Not significant\n",
      "\n",
      "======================================================================\n",
      "SUBGROUP EFFECTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Variable           Subgroup    N  Causal_Effect_Beta  P_Value  Significant  First_Stage_F  Instrument_Weak\n",
      "  Income     Income_0-30000  708           -0.002041 0.318643        False   3.022514e+05            False\n",
      " Ad_Type             Banner 1242           -0.001119 0.462927        False   2.908278e+05            False\n",
      "Location              Urban 1593           -0.001102 0.436144        False   1.082823e+06            False\n",
      " Ad_Type             Native 1163           -0.000884 0.595812        False   6.940095e+04            False\n",
      " Ad_Type               Text 1178            0.000867 0.601927        False   8.814048e+05            False\n",
      "  Income   Income_70000-inf  772            0.000841 0.673912        False   1.347974e+05            False\n",
      "     Age          Age_50-65  602            0.000698 0.763337        False   2.461708e+05            False\n",
      "  Income Income_50000-70000 1664           -0.000446 0.741431        False   1.345625e+05            False\n",
      "Location           Suburban 1554            0.000440 0.755632        False   1.176765e+05            False\n",
      "     Age           Age_0-35 2446           -0.000440 0.693512        False   2.466478e+05            False\n",
      "Location              Rural 1624           -0.000349 0.795901        False   5.181187e+05            False\n",
      "  Income Income_30000-50000 1627            0.000222 0.875402        False   4.987560e+05            False\n",
      " Ad_Type              Video 1188           -0.000141 0.929650        False   5.335384e+05            False\n",
      "     Age          Age_35-50 1628           -0.000023 0.986335        False   5.020124e+05            False\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS\n",
      "======================================================================\n",
      "\n",
      "âœ— No subgroups with statistically significant effects found\n",
      "\n",
      "ðŸ“Š Effect Heterogeneity:\n",
      "  Range: 0.002908\n",
      "  Max effect: 0.000867 (Text)\n",
      "  Min effect: -0.002041 (Income_0-30000)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Estimated value-added for 14 subgroups\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: FORECAST VALIDATION (Testing Sample)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ Validated 7 groups in testing sample\n",
      "\n",
      "Test Sample Statistics:\n",
      "Variable Subgroup  Value_Added_Train  Mean_Outcome_Test  N_Test\n",
      " Ad_Type   Banner          -0.001119           0.195163    1218\n",
      "Location    Urban          -0.001102           0.200530    1550\n",
      " Ad_Type   Native          -0.000884           0.206343    1162\n",
      " Ad_Type     Text           0.000867           0.200875    1178\n",
      "Location Suburban           0.000440           0.200440    1607\n",
      "Location    Rural          -0.000349           0.201304    1615\n",
      " Ad_Type    Video          -0.000141           0.200927    1214\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 4: FORECAST UNBIASEDNESS TEST\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Regression: Mean_Outcome_test = Î± + Î² * ValueAdded_train + Îµ\n",
      "Hâ‚€: Î² = 1 (unbiased forecasts)\n",
      "\n",
      "Results:\n",
      "  Î± (intercept):     0.200907\n",
      "  Î² (slope):         0.393623 (SE: 1.821654)\n",
      "  Prediction RÂ²:     0.0093\n",
      "\n",
      "  Test Hâ‚€: Î² = 1\n",
      "  t-statistic:       -0.333\n",
      "  p-value:           0.7527\n",
      "\n",
      "  âœ“ FAIL TO REJECT Hâ‚€: No significant bias\n",
      "    â†’ Training estimates predict test outcomes well\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 5: EMPIRICAL BAYES SHRINKAGE\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Variance Decomposition:\n",
      "  Between-group variance (signal): 0.00000000\n",
      "  Within-group variance (noise):   0.00000262\n",
      "  Total variance:                  0.00000066\n",
      "  Reliability (Î»Ì„):                 0.0000\n",
      "\n",
      "Group-Specific Shrinkage:\n",
      "Variable        Subgroup             Raw VA       Î»        Shrunk VA   \n",
      "---------------------------------------------------------------------------\n",
      "Income          Income_0-30000         -0.002041  0.0000   -0.000248\n",
      "Ad_Type         Banner                 -0.001119  0.0000   -0.000248\n",
      "Location        Urban                  -0.001102  0.0000   -0.000248\n",
      "Ad_Type         Native                 -0.000884  0.0000   -0.000248\n",
      "Ad_Type         Text                    0.000867  0.0000   -0.000248\n",
      "Income          Income_70000-inf        0.000841  0.0000   -0.000248\n",
      "Age             Age_50-65               0.000698  0.0000   -0.000248\n",
      "Income          Income_50000-70000     -0.000446  0.0000   -0.000248\n",
      "Location        Suburban                0.000440  0.0000   -0.000248\n",
      "Age             Age_0-35               -0.000440  0.0000   -0.000248\n",
      "Location        Rural                  -0.000349  0.0000   -0.000248\n",
      "Income          Income_30000-50000      0.000222  0.0000   -0.000248\n",
      "Ad_Type         Video                  -0.000141  0.0000   -0.000248\n",
      "Age             Age_35-50              -0.000023  0.0000   -0.000248\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 6: FORECAST BIAS CORRECTION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Applying correction: VA_final = (VA_shrunk - 0.200907) / 0.393623\n",
      "\n",
      "Final Value-Added Estimates:\n",
      "\n",
      "Variable           Subgroup    Raw_VA  Shrunk_VA  Bias_Corrected_VA  N_Train\n",
      "  Income     Income_0-30000 -0.002041  -0.000248          -0.511035      708\n",
      " Ad_Type             Banner -0.001119  -0.000248          -0.511035     1242\n",
      "Location              Urban -0.001102  -0.000248          -0.511035     1593\n",
      " Ad_Type             Native -0.000884  -0.000248          -0.511035     1163\n",
      " Ad_Type               Text  0.000867  -0.000248          -0.511035     1178\n",
      "  Income   Income_70000-inf  0.000841  -0.000248          -0.511035      772\n",
      "     Age          Age_50-65  0.000698  -0.000248          -0.511035      602\n",
      "  Income Income_50000-70000 -0.000446  -0.000248          -0.511035     1664\n",
      "Location           Suburban  0.000440  -0.000248          -0.511035     1554\n",
      "     Age           Age_0-35 -0.000440  -0.000248          -0.511035     2446\n",
      "Location              Rural -0.000349  -0.000248          -0.511035     1624\n",
      "  Income Income_30000-50000  0.000222  -0.000248          -0.511035     1627\n",
      " Ad_Type              Video -0.000141  -0.000248          -0.511035     1188\n",
      "     Age          Age_35-50 -0.000023  -0.000248          -0.511035     1628\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: CORRECTIONS APPLIED\n",
      "======================================================================\n",
      "\n",
      "Average absolute corrections:\n",
      "  Shrinkage effect:      0.000663\n",
      "  Bias correction:       0.510787\n",
      "  Total correction:      0.510787\n",
      "\n",
      "Diagnostics:\n",
      "  Reliability:           0.0000\n",
      "  Forecast bias (Î²):     0.3936\n",
      "  Prediction RÂ²:         0.0093\n",
      "  Bias type:             MINIMAL\n",
      "\n",
      "Recommendations:\n",
      "  âš  Low reliability - value-added estimates are very noisy\n",
      "    â†’ Collect more data or use coarser groupings\n",
      "  âœ“ Minimal forecast bias - raw estimates predict well\n",
      "\n",
      "======================================================================\n",
      "INTEGRATED ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: ESTIMATION APPROACHES\n",
      "======================================================================\n",
      "\n",
      "Variable           Subgroup    Raw_VA  Shrunk_VA  Bias_Corrected_VA  Total_Change    Pct_Change\n",
      "  Income     Income_0-30000 -0.002041  -0.000248          -0.511035     -0.508994 -2.493740e+04\n",
      " Ad_Type             Banner -0.001119  -0.000248          -0.511035     -0.509916 -4.557746e+04\n",
      "Location              Urban -0.001102  -0.000248          -0.511035     -0.509933 -4.626732e+04\n",
      " Ad_Type             Native -0.000884  -0.000248          -0.511035     -0.510151 -5.769047e+04\n",
      " Ad_Type               Text  0.000867  -0.000248          -0.511035     -0.511902 -5.902790e+04\n",
      "  Income   Income_70000-inf  0.000841  -0.000248          -0.511035     -0.511877 -6.083380e+04\n",
      "     Age          Age_50-65  0.000698  -0.000248          -0.511035     -0.511733 -7.334900e+04\n",
      "  Income Income_50000-70000 -0.000446  -0.000248          -0.511035     -0.510590 -1.145992e+05\n",
      "Location           Suburban  0.000440  -0.000248          -0.511035     -0.511475 -1.161988e+05\n",
      "     Age           Age_0-35 -0.000440  -0.000248          -0.511035     -0.510595 -1.160021e+05\n",
      "Location              Rural -0.000349  -0.000248          -0.511035     -0.510686 -1.462303e+05\n",
      "  Income Income_30000-50000  0.000222  -0.000248          -0.511035     -0.511257 -2.300885e+05\n",
      " Ad_Type              Video -0.000141  -0.000248          -0.511035     -0.510894 -3.618054e+05\n",
      "     Age          Age_35-50 -0.000023  -0.000248          -0.511035     -0.511012 -2.180219e+06\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SUMMARY STATISTICS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Mean absolute change:\n",
      "  From shrinkage:        0.000663\n",
      "  From bias correction:  0.510787\n",
      "  Total:                 0.510787\n",
      "\n",
      "Mean % change:           259487.6%\n",
      "\n",
      "Groups with largest corrections:\n",
      "      Subgroup    Raw_VA  Bias_Corrected_VA  Total_Change\n",
      "Income_0-30000 -0.002041          -0.511035     -0.508994\n",
      "        Banner -0.001119          -0.511035     -0.509916\n",
      "         Urban -0.001102          -0.511035     -0.509933\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instrumental Variables (IV) Causal Inference for Ad Conversion Analysis\n",
    "Using ML-Generated Instruments and Two-Stage Least Squares (2SLS)\n",
    "ENHANCED VERSION: Data cleaning, stronger instruments, and log transformations\n",
    "\"\"\"\n",
    "class CausalAdAnalyzer:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline for causal inference in ad conversion analysis\n",
    "    using ML-generated instrumental variables and 2SLS estimation.\n",
    "    \n",
    "    ENHANCED with:\n",
    "    - Robust data cleaning and preprocessing\n",
    "    - Logarithmic transformations for skewed variables\n",
    "    - Rich feature engineering for stronger instruments\n",
    "    - Stacking ensemble for maximum predictive power\n",
    "    - Comprehensive diagnostics including Stock-Yogo tests\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with your dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Must contain columns:\n",
    "            - Conversion_Rate (Y): dependent variable\n",
    "            - Clicks (D): endogenous regressor\n",
    "            - Age, Gender, Income, Location: demographics\n",
    "            - Ad_Type, Ad_Topic, Ad_Placement: ad features\n",
    "            - CTR: click-through rate\n",
    "            - Click_Time: timestamp for feature engineering\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.first_stage_model = None\n",
    "        self.first_stage_results = None\n",
    "        self.second_stage_results = None\n",
    "        \n",
    "        # Clean data on initialization\n",
    "        self._clean_data()\n",
    "        \n",
    "    def _clean_data(self):\n",
    "        \"\"\"\n",
    "        Clean and preprocess data before analysis.\n",
    "        \n",
    "        Performs:\n",
    "        1. Handle negative income values\n",
    "        2. Impute missing income with median\n",
    "        3. Winsorize income at 1st and 99th percentiles\n",
    "        4. Filter age to plausible range (10-90 years)\n",
    "        5. Create logarithmic transformations for skewed variables\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        initial_rows = len(self.data)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. CLEAN INCOME\n",
    "        # =====================================================================\n",
    "        if 'Income' in self.data.columns:\n",
    "            # Convert negative income to missing\n",
    "            neg_income_count = (self.data['Income'] < 0).sum()\n",
    "            self.data.loc[self.data['Income'] < 0, 'Income'] = np.nan\n",
    "            \n",
    "            if neg_income_count > 0:\n",
    "                print(f\"âœ“ Converted {neg_income_count} negative income values to missing\")\n",
    "            \n",
    "            # Impute missing income with median\n",
    "            missing_income = self.data['Income'].isna().sum()\n",
    "            if missing_income > 0:\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                self.data['Income'] = imputer.fit_transform(self.data[['Income']])\n",
    "                print(f\"âœ“ Imputed {missing_income} missing income values with median\")\n",
    "            \n",
    "            # Winsorize: Cap extremes at 1st and 99th percentile\n",
    "            lower, upper = self.data['Income'].quantile([0.01, 0.99])\n",
    "            income_before = self.data['Income'].copy()\n",
    "            self.data['Income'] = self.data['Income'].clip(lower, upper)\n",
    "            winsorized = (income_before != self.data['Income']).sum()\n",
    "            print(f\"âœ“ Winsorized {winsorized} income values at 1st/99th percentiles\")\n",
    "            print(f\"  Income range: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. FILTER AGE\n",
    "        # =====================================================================\n",
    "        if 'Age' in self.data.columns:\n",
    "            age_before = len(self.data)\n",
    "            self.data = self.data[self.data['Age'].between(10, 90)]\n",
    "            age_filtered = age_before - len(self.data)\n",
    "            if age_filtered > 0:\n",
    "                print(f\"âœ“ Filtered {age_filtered} rows with implausible ages (keeping 10-90)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. CREATE LOGARITHMIC TRANSFORMATIONS\n",
    "        # =====================================================================\n",
    "        print(f\"\\nðŸ“Š Creating logarithmic transformations:\")\n",
    "        \n",
    "        # Log of Income (if positive)\n",
    "        if 'Income' in self.data.columns:\n",
    "            self.data['Income_log'] = np.log1p(self.data['Income'])\n",
    "            print(f\"  âœ“ Income_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of Clicks (if exists and positive)\n",
    "        if 'Clicks' in self.data.columns:\n",
    "            self.data['Clicks_log'] = np.log1p(self.data['Clicks'])\n",
    "            print(f\"  âœ“ Clicks_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of Age (for nonlinear age effects)\n",
    "        if 'Age' in self.data.columns:\n",
    "            self.data['Age_log'] = np.log1p(self.data['Age'])\n",
    "            print(f\"  âœ“ Age_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of CTR (if exists and positive)\n",
    "        if 'CTR' in self.data.columns:\n",
    "            # Ensure CTR is positive before log\n",
    "            if (self.data['CTR'] > 0).all():\n",
    "                self.data['CTR_log'] = np.log(self.data['CTR'])\n",
    "                print(f\"  âœ“ CTR_log created (log transformation)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # SUMMARY\n",
    "        # =====================================================================\n",
    "        final_rows = len(self.data)\n",
    "        rows_removed = initial_rows - final_rows\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLEANING SUMMARY:\")\n",
    "        print(f\"  Initial rows:        {initial_rows:,}\")\n",
    "        print(f\"  Final rows:          {final_rows:,}\")\n",
    "        print(f\"  Rows removed:        {rows_removed:,} ({rows_removed/initial_rows*100:.1f}%)\")\n",
    "        print(f\"  Log variables added: {len([col for col in self.data.columns if '_log' in col])}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def engineer_time_features(self):\n",
    "        \"\"\"Extract day of week and hour from Click_Time\"\"\"\n",
    "        if 'Click_Time' in self.data.columns:\n",
    "            self.data['Click_Time'] = pd.to_datetime(self.data['Click_Time'])\n",
    "            self.data['Day_of_Week'] = self.data['Click_Time'].dt.dayofweek\n",
    "            self.data['Hour'] = self.data['Click_Time'].dt.hour\n",
    "        return self\n",
    "    \n",
    "    def encode_categorical_features(self):\n",
    "        \"\"\"Encode categorical variables\"\"\"\n",
    "        categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in self.data.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.data[f'{col}_encoded'] = le.fit_transform(self.data[col].astype(str))\n",
    "                self.encoders[col] = le\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def engineer_instrument_features(self):\n",
    "        \"\"\"\n",
    "        ENHANCED: Create rich features that predict clicks but don't directly affect conversions.\n",
    "        \n",
    "        This is crucial for instrument strength. We create:\n",
    "        1. Interaction features between ad characteristics and demographics\n",
    "        2. Time-based features (weekend, business hours)\n",
    "        3. Nonlinear transformations\n",
    "        4. Complex interactions between multiple variables\n",
    "        \n",
    "        Key principle: These features should predict CLICKS well, but only affect\n",
    "        CONVERSIONS through clicks (exclusion restriction).\n",
    "        \"\"\"\n",
    "        df = self.data\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. AD CHARACTERISTICS Ã— DEMOGRAPHICS INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Different demographics respond differently to ad types\n",
    "        \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Type_encoded']):\n",
    "            df['Income_x_AdType'] = df['Income'] * df['Ad_Type_encoded']\n",
    "            print(\"âœ“ Created Income Ã— Ad Type interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Topic_encoded']):\n",
    "            df['Age_x_AdTopic'] = df['Age'] * df['Ad_Topic_encoded']\n",
    "            print(\"âœ“ Created Age Ã— Ad Topic interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded']):\n",
    "            df['Income_x_Placement'] = df['Income'] * df['Ad_Placement_encoded']\n",
    "            print(\"âœ“ Created Income Ã— Ad Placement interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Placement_encoded']):\n",
    "            df['Age_x_Placement'] = df['Age'] * df['Ad_Placement_encoded']\n",
    "            print(\"âœ“ Created Age Ã— Ad Placement interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. TIME-BASED FEATURES AND INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Click patterns vary by time of day/week\n",
    "        \n",
    "        if 'Day_of_Week' in df.columns:\n",
    "            df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "            print(\"âœ“ Created Weekend indicator\")\n",
    "            \n",
    "        if 'Hour' in df.columns:\n",
    "            df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "            df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "            df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "            print(\"âœ“ Created time-of-day indicators\")\n",
    "        \n",
    "        # Time Ã— Ad interactions\n",
    "        if all(col in df.columns for col in ['Weekend', 'Ad_Type_encoded']):\n",
    "            df['Weekend_x_AdType'] = df['Weekend'] * df['Ad_Type_encoded']\n",
    "            print(\"âœ“ Created Weekend Ã— Ad Type interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['BusinessHours', 'Ad_Placement_encoded']):\n",
    "            df['BusinessHours_x_Placement'] = df['BusinessHours'] * df['Ad_Placement_encoded']\n",
    "            print(\"âœ“ Created Business Hours Ã— Ad Placement interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Evening', 'Ad_Topic_encoded']):\n",
    "            df['Evening_x_AdTopic'] = df['Evening'] * df['Ad_Topic_encoded']\n",
    "            print(\"âœ“ Created Evening Ã— Ad Topic interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. DEMOGRAPHICS Ã— TIME INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Different demographics have different browsing patterns\n",
    "        \n",
    "        if all(col in df.columns for col in ['Age', 'Hour']):\n",
    "            df['Age_x_Hour'] = df['Age'] * df['Hour']\n",
    "            print(\"âœ“ Created Age Ã— Hour interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Weekend']):\n",
    "            df['Income_x_Weekend'] = df['Income'] * df['Weekend']\n",
    "            print(\"âœ“ Created Income Ã— Weekend interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'BusinessHours']):\n",
    "            df['Age_x_BusinessHours'] = df['Age'] * df['BusinessHours']\n",
    "            print(\"âœ“ Created Age Ã— Business Hours interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 4. NONLINEAR TRANSFORMATIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Relationships may be nonlinear (using log-transformed versions)\n",
    "        \n",
    "        if 'Age_log' in df.columns:\n",
    "            df['Age_squared'] = df['Age'] ** 2\n",
    "            print(\"âœ“ Created Age squared\")\n",
    "            \n",
    "        if 'Income_log' in df.columns:\n",
    "            df['Income_squared'] = df['Income'] ** 2\n",
    "            df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "            print(\"âœ“ Created Income squared and sqrt\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 5. COMPLEX CATEGORICAL INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Certain combinations may be particularly predictive\n",
    "        \n",
    "        # Location Ã— Demographics\n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Age']):\n",
    "            df['Location_x_Age'] = df['Location_encoded'] * df['Age']\n",
    "            print(\"âœ“ Created Location Ã— Age interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Income']):\n",
    "            df['Location_x_Income'] = df['Location_encoded'] * df['Income']\n",
    "            print(\"âœ“ Created Location Ã— Income interaction\")\n",
    "        \n",
    "        # Location Ã— Ad characteristics\n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Ad_Placement_encoded']):\n",
    "            df['Location_x_Placement'] = df['Location_encoded'] * df['Ad_Placement_encoded']\n",
    "            print(\"âœ“ Created Location Ã— Placement interaction\")\n",
    "        \n",
    "        # Gender Ã— Ad characteristics\n",
    "        if all(col in df.columns for col in ['Gender_encoded', 'Ad_Topic_encoded']):\n",
    "            df['Gender_x_AdTopic'] = df['Gender_encoded'] * df['Ad_Topic_encoded']\n",
    "            print(\"âœ“ Created Gender Ã— Ad Topic interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Gender_encoded', 'Ad_Type_encoded']):\n",
    "            df['Gender_x_AdType'] = df['Gender_encoded'] * df['Ad_Type_encoded']\n",
    "            print(\"âœ“ Created Gender Ã— Ad Type interaction\")\n",
    "        \n",
    "        # Ad Type Ã— Placement (different placements work for different types)\n",
    "        if all(col in df.columns for col in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "            df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "            print(\"âœ“ Created Ad Type Ã— Placement interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 6. THREE-WAY INTERACTIONS (most powerful)\n",
    "        # =====================================================================\n",
    "        # Rationale: Capture complex patterns\n",
    "        \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "            df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "            print(\"âœ“ Created Age Ã— Ad Type Ã— Weekend interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "            df['Income_x_Placement_x_BizHours'] = df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "            print(\"âœ“ Created Income Ã— Placement Ã— Business Hours interaction\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def create_ml_instrument(self, model_type='stacking', cv_folds=5, use_enhanced_features=True):\n",
    "        \"\"\"\n",
    "        ENHANCED: Generate ML-based instrument for Clicks (D) using ensemble methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            'rf' for Random Forest\n",
    "            'gb' for Gradient Boosting\n",
    "            'stacking' for Stacking Ensemble (RECOMMENDED for strongest instruments)\n",
    "        cv_folds : int\n",
    "            Number of cross-validation folds\n",
    "        use_enhanced_features : bool\n",
    "            Whether to use enhanced feature engineering (recommended)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply enhanced feature engineering if requested\n",
    "        if use_enhanced_features:\n",
    "            self.engineer_instrument_features()\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DEFINE INSTRUMENT FEATURES\n",
    "        # =====================================================================\n",
    "        # Base features (always included)\n",
    "        base_features = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'Day_of_Week', 'Hour'\n",
    "        ]\n",
    "        \n",
    "        # Enhanced features (only if engineered)\n",
    "        enhanced_features = [\n",
    "            # Interactions\n",
    "            'Income_x_AdType', 'Age_x_AdTopic', 'Income_x_Placement', 'Age_x_Placement',\n",
    "            'Weekend_x_AdType', 'BusinessHours_x_Placement', 'Evening_x_AdTopic',\n",
    "            'Age_x_Hour', 'Income_x_Weekend', 'Age_x_BusinessHours',\n",
    "            'Location_x_Age', 'Location_x_Income', 'Location_x_Placement',\n",
    "            'Gender_x_AdTopic', 'Gender_x_AdType', 'AdType_x_Placement',\n",
    "            'Age_x_AdType_x_Weekend', 'Income_x_Placement_x_BizHours',\n",
    "            # Time features\n",
    "            'Weekend', 'BusinessHours', 'Evening', 'Morning',\n",
    "            # Nonlinear (now using cleaned log versions)\n",
    "            'Age_squared', 'Age_log', 'Income_log', 'Income_squared', 'Income_sqrt',\n",
    "            'Clicks_log', 'CTR_log'\n",
    "        ]\n",
    "        \n",
    "        # Combine and filter available features\n",
    "        if use_enhanced_features:\n",
    "            instrument_features = base_features + enhanced_features\n",
    "        else:\n",
    "            instrument_features = base_features\n",
    "            \n",
    "        available_features = [f for f in instrument_features if f in self.data.columns]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ML INSTRUMENT CONSTRUCTION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total features available: {len(available_features)}\")\n",
    "        print(f\"Model type: {model_type.upper()}\")\n",
    "        print(f\"Cross-validation folds: {cv_folds}\")\n",
    "\n",
    "        # Prepare data\n",
    "        X_instrument = self.data[available_features]\n",
    "        y_clicks = self.data['Clicks']\n",
    "        \n",
    "        # Standardize features\n",
    "        X_instrument_scaled = self.scaler.fit_transform(X_instrument)\n",
    "        X_instrument_scaled = pd.DataFrame(\n",
    "            X_instrument_scaled, \n",
    "            columns=available_features,\n",
    "            index=X_instrument.index\n",
    "        )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # FEATURE SELECTION\n",
    "        # =====================================================================\n",
    "        # Optional: Reduce to top N features based on importance\n",
    "        top_n = min(10, len(available_features))\n",
    "        print(f\"\\nSelecting top {top_n} features based on model importance...\")\n",
    "        \n",
    "        # Use a simple model to rank features (e.g., Random Forest)\n",
    "        feature_selector = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        feature_selector.fit(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        # Get top N features\n",
    "        importances = pd.Series(feature_selector.feature_importances_, index=X_instrument_scaled.columns)\n",
    "        top_features = importances.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "        \n",
    "        print(\"Top features selected:\")\n",
    "        for i, feat in enumerate(top_features, 1):\n",
    "            print(f\"{i}. {feat}\")\n",
    "        \n",
    "        # Filter scaled data to top features\n",
    "        X_instrument_scaled = X_instrument_scaled[top_features]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # BUILD MODEL\n",
    "        # =====================================================================\n",
    "        \n",
    "        if model_type == 'stacking':\n",
    "            print(\"\\nBuilding Stacking Ensemble (strongest option)...\")\n",
    "            \n",
    "            # Define base learners with more aggressive parameters\n",
    "            base_models = [\n",
    "                ('rf', RandomForestRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=15,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    max_features='sqrt',\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('gb', GradientBoostingRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            # Try to import XGBoost if available\n",
    "            try:\n",
    "                from xgboost import XGBRegressor\n",
    "                base_models.append(\n",
    "                    ('xgb', XGBRegressor(\n",
    "                        n_estimators=200,\n",
    "                        max_depth=8,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    ))\n",
    "                )\n",
    "                print(\"  âœ“ Using XGBoost as additional base learner\")\n",
    "            except ImportError:\n",
    "                print(\"  â„¹ XGBoost not available, using RF + GB only\")\n",
    "            \n",
    "            # Create stacking ensemble\n",
    "            self.first_stage_model = StackingRegressor(\n",
    "                estimators=base_models,\n",
    "                final_estimator=Ridge(alpha=1.0),\n",
    "                cv=cv_folds,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            print(\"\\nBuilding Random Forest...\")\n",
    "            self.first_stage_model = RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                max_features='sqrt',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'gb':\n",
    "            print(\"\\nBuilding Gradient Boosting...\")\n",
    "            self.first_stage_model = GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=7,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # GENERATE OUT-OF-FOLD PREDICTIONS\n",
    "        # =====================================================================\n",
    "        print(f\"\\nGenerating out-of-fold predictions (CV={cv_folds})...\")\n",
    "        \n",
    "        self.data['Clicks_predicted'] = cross_val_predict(\n",
    "            self.first_stage_model,\n",
    "            X_instrument_scaled,\n",
    "            y_clicks,\n",
    "            cv=cv_folds,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit final model for interpretation\n",
    "        print(\"Fitting final model...\")\n",
    "        self.first_stage_model.fit(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DIAGNOSTICS\n",
    "        # =====================================================================\n",
    "        self._enhanced_instrument_diagnostics(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _enhanced_instrument_diagnostics(self, X, y):\n",
    "        \"\"\"\n",
    "        ENHANCED: Comprehensive instrument strength testing with Stock-Yogo critical values.\n",
    "        \"\"\"\n",
    "        z = self.data['Clicks_predicted'].values\n",
    "        d = self.data['Clicks'].values\n",
    "        \n",
    "        n = len(d)\n",
    "        k = X.shape[1]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. FIRST-STAGE R-SQUARED AND F-STATISTIC\n",
    "        # =====================================================================\n",
    "        z_resid = z - z.mean()\n",
    "        d_resid = d - d.mean()\n",
    "        \n",
    "        ss_tot = np.sum(d_resid**2)\n",
    "        ss_res = np.sum((d - z)**2)\n",
    "        r_squared = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Proper F-statistic for first stage\n",
    "        f_stat = (r_squared / 1) / ((1 - r_squared) / (n - k - 1))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. CORRELATION\n",
    "        # =====================================================================\n",
    "        corr = np.corrcoef(z, d)[0, 1]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. CRAGG-DONALD STATISTIC\n",
    "        # =====================================================================\n",
    "        cragg_donald = n * r_squared\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DISPLAY RESULTS\n",
    "        # =====================================================================\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nSAMPLE INFORMATION:\")\n",
    "        print(f\"  Sample size (n):              {n:,}\")\n",
    "        print(f\"  Number of features (k):       {k}\")\n",
    "        print(f\"\\nFIRST-STAGE PERFORMANCE:\")\n",
    "        print(f\"  R-squared:                    {r_squared:.4f}\")\n",
    "        print(f\"  Correlation (Z, D):           {corr:.4f}\")\n",
    "        print(f\"  F-statistic:                  {f_stat:.2f}\")\n",
    "        print(f\"  Cragg-Donald statistic:       {cragg_donald:.2f}\")\n",
    "        \n",
    "        print(f\"\\nBENCHMARKS & INTERPRETATION:\")\n",
    "        print(f\"  {'Criterion':<35} {'Threshold':<12} {'Status'}\")\n",
    "        print(f\"  {'-'*35} {'-'*12} {'-'*20}\")\n",
    "        \n",
    "        # Weak instrument test\n",
    "        weak_status = \"âœ“ STRONG\" if f_stat > 10 else \"âœ— WEAK\"\n",
    "        print(f\"  {'Weak Instrument (F < 10)':<35} {'10.00':<12} {weak_status}\")\n",
    "        \n",
    "        # Stock-Yogo critical values (for single instrument, single endogenous variable)\n",
    "        sy_10_status = \"âœ“âœ“ EXCELLENT\" if f_stat > 16.38 else \"âœ— Below threshold\"\n",
    "        sy_15_status = \"âœ“ GOOD\" if f_stat > 8.96 else \"âœ— Below threshold\"\n",
    "        \n",
    "        print(f\"  {'Stock-Yogo 10% max bias':<35} {'16.38':<12} {sy_10_status}\")\n",
    "        print(f\"  {'Stock-Yogo 15% max bias':<35} {'8.96':<12} {sy_15_status}\")\n",
    "        \n",
    "        print(f\"\\nOVERALL ASSESSMENT:\")\n",
    "        if f_stat > 16.38:\n",
    "            print(f\"  âœ“âœ“ VERY STRONG INSTRUMENT\")\n",
    "            print(f\"     Maximum IV bias < 10% of OLS bias\")\n",
    "            print(f\"     Highly reliable causal inference\")\n",
    "        elif f_stat > 10:\n",
    "            print(f\"  âœ“ STRONG INSTRUMENT\")\n",
    "            print(f\"     Acceptable for causal inference\")\n",
    "            print(f\"     Results should be reliable\")\n",
    "        elif f_stat > 5:\n",
    "            print(f\"  âš  MODERATELY WEAK INSTRUMENT\")\n",
    "            print(f\"     Proceed with caution\")\n",
    "            print(f\"     Consider sensitivity analysis\")\n",
    "        else:\n",
    "            print(f\"  âœ— WEAK INSTRUMENT\")\n",
    "            print(f\"     Results may be unreliable\")\n",
    "            print(f\"     Consider alternative identification strategies\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # FEATURE IMPORTANCE (if available)\n",
    "        # =====================================================================\n",
    "        if hasattr(self.first_stage_model, 'feature_importances_'):\n",
    "            print(f\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING CLICKS:\")\n",
    "            importances = self.first_stage_model.feature_importances_\n",
    "            top_features = sorted(\n",
    "                zip(X.columns, importances), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:10]\n",
    "            \n",
    "            for i, (feat, imp) in enumerate(top_features, 1):\n",
    "                print(f\"  {i:2d}. {feat:35s} {imp:.4f}\")\n",
    "        \n",
    "        elif hasattr(self.first_stage_model, 'final_estimator_'):\n",
    "            print(f\"\\nâ„¹ Stacking ensemble used - individual feature importances\")\n",
    "            print(f\"  not directly available, but all base models contribute\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def run_2sls(self, include_interactions=False):\n",
    "        \"\"\"\n",
    "        Step 3: Two-Stage Least Squares Estimation\n",
    "        \n",
    "        First Stage: D = Ï€â‚€ + Ï€â‚Z + Ï€â‚‚X + Î½\n",
    "        Second Stage: Y = Î± + Î²DÌ‚ + Î³X + Îµ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        include_interactions : bool\n",
    "            Whether to include Ad_Type Ã— Ad_Placement interactions\n",
    "        \"\"\"\n",
    "        # Exogenous controls (X)\n",
    "        exog_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'CTR'\n",
    "        ]\n",
    "        \n",
    "        available_controls = [f for f in exog_controls if f in self.data.columns]\n",
    "        \n",
    "        # Add interaction terms if requested\n",
    "        if include_interactions:\n",
    "            if 'Ad_Type_encoded' in self.data.columns and 'Ad_Placement_encoded' in self.data.columns:\n",
    "                self.data['Ad_Type_x_Placement'] = (\n",
    "                    self.data['Ad_Type_encoded'] * self.data['Ad_Placement_encoded']\n",
    "                )\n",
    "                available_controls.append('Ad_Type_x_Placement')\n",
    "        \n",
    "        print('2sls data summar: ', self.data.describe(include='all'))\n",
    "\n",
    "        # FIRST STAGE: Regress D on Z and X\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FIRST STAGE REGRESSION: D ~ Z + X\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print('available controls: ', available_controls)\n",
    "\n",
    "        X_first_stage = sm.add_constant(pd.concat([\n",
    "            self.data[['Clicks_predicted']],\n",
    "            self.data[available_controls]\n",
    "        ], axis=1))\n",
    "        \n",
    "        y_first_stage = self.data['Clicks']\n",
    "        \n",
    "        self.first_stage_results = sm.OLS(y_first_stage, X_first_stage).fit()\n",
    "        \n",
    "        print(\"\\nFirst Stage Summary:\")\n",
    "        print(f\"R-squared: {self.first_stage_results.rsquared:.4f}\")\n",
    "        print(f\"F-statistic: {self.first_stage_results.fvalue:.2f}\")\n",
    "        print(f\"Instrument coefficient: {self.first_stage_results.params['Clicks_predicted']:.4f}\")\n",
    "        print(f\"Instrument p-value: {self.first_stage_results.pvalues['Clicks_predicted']:.4f}\")\n",
    "        \n",
    "        # Get fitted values from first stage\n",
    "        D_hat = self.first_stage_results.fittedvalues\n",
    "        \n",
    "        # SECOND STAGE: Regress Y on D_hat and X\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SECOND STAGE REGRESSION: Y ~ DÌ‚ + X\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        X_second_stage = sm.add_constant(pd.concat([\n",
    "            pd.Series(D_hat, name='Clicks_fitted'),\n",
    "            self.data[available_controls]\n",
    "        ], axis=1))\n",
    "        \n",
    "        y_second_stage = self.data['Conversion_Rate']\n",
    "        \n",
    "        self.second_stage_results = sm.OLS(y_second_stage, X_second_stage).fit()\n",
    "        \n",
    "        # Manual calculation of correct standard errors for 2SLS\n",
    "        self._calculate_2sls_standard_errors(available_controls)\n",
    "        \n",
    "        self._display_results()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    ### stratified 2sls for subgroup effects\n",
    "    def analyze_subgroup_effects(self, subgroup_vars=None, min_subgroup_size=100):\n",
    "        \"\"\"\n",
    "        Stratified 2SLS: Run separate 2SLS regressions within subgroups to identify\n",
    "        heterogeneous treatment effects.\n",
    "\n",
    "        This helps explain why average effects may be weak - effects may be strong\n",
    "        in specific segments but cancel out in aggregate.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        subgroup_vars : list of str or dict, optional\n",
    "            Variables to stratify by. Can be:\n",
    "            - List of column names (will auto-create bins for continuous vars)\n",
    "            - Dict mapping column names to bin specifications\n",
    "            Example: ['Location', 'Ad_Type'] or \n",
    "                     {'Income': [0, 30000, 60000, np.inf], 'Age': [0, 35, 50, 65, np.inf]}\n",
    "        min_subgroup_size : int\n",
    "            Minimum observations required per subgroup (for statistical power)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        results_df : pd.DataFrame\n",
    "            Subgroup-specific causal effects with diagnostics\n",
    "        \"\"\"\n",
    "\n",
    "        if subgroup_vars is None:\n",
    "            # Default subgroups based on theory\n",
    "            subgroup_vars = {\n",
    "                'Income': [0, 30000, 50000, 70000, np.inf],  # Quartile-like bins\n",
    "                'Age': [0, 35, 50, 65, np.inf],              # Life stage bins\n",
    "                'Location': None,                             # Use as-is (categorical)\n",
    "                'Ad_Type': None                               # Use as-is (categorical)\n",
    "            }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STRATIFIED 2SLS: HETEROGENEOUS TREATMENT EFFECTS ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Store results for each subgroup\n",
    "        all_results = []\n",
    "\n",
    "        # Exogenous controls for 2SLS\n",
    "        exog_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'CTR'\n",
    "        ]\n",
    "        available_controls = [f for f in exog_controls if f in self.data.columns]\n",
    "\n",
    "        # =========================================================================\n",
    "        # PROCESS EACH SUBGROUP VARIABLE\n",
    "        # =========================================================================\n",
    "\n",
    "        for var in (subgroup_vars if isinstance(subgroup_vars, list) else subgroup_vars.keys()):\n",
    "\n",
    "            print(f\"\\n{'â”€'*70}\")\n",
    "            print(f\"ANALYZING SUBGROUPS BY: {var.upper()}\")\n",
    "            print(f\"{'â”€'*70}\")\n",
    "\n",
    "            # Create subgroups\n",
    "            if isinstance(subgroup_vars, dict) and subgroup_vars.get(var) is not None:\n",
    "                # Continuous variable with specified bins\n",
    "                bins = subgroup_vars[var]\n",
    "                labels = [f\"{var}_{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "                self.data[f'{var}_subgroup'] = pd.cut(\n",
    "                    self.data[var], \n",
    "                    bins=bins, \n",
    "                    labels=labels,\n",
    "                    include_lowest=True\n",
    "                )\n",
    "                subgroup_col = f'{var}_subgroup'\n",
    "            else:\n",
    "                # Categorical variable - use as is\n",
    "                subgroup_col = var\n",
    "\n",
    "            # Get unique subgroups\n",
    "            subgroups = self.data[subgroup_col].dropna().unique()\n",
    "\n",
    "            print(f\"\\nFound {len(subgroups)} subgroups: {sorted([str(s) for s in subgroups])}\")\n",
    "\n",
    "            # =====================================================================\n",
    "            # RUN 2SLS FOR EACH SUBGROUP\n",
    "            # =====================================================================\n",
    "\n",
    "            for subgroup in subgroups:\n",
    "\n",
    "                # Filter data to subgroup\n",
    "                subgroup_data = self.data[self.data[subgroup_col] == subgroup].copy()\n",
    "                n_obs = len(subgroup_data)\n",
    "\n",
    "                # Skip if too small\n",
    "                if n_obs < min_subgroup_size:\n",
    "                    print(f\"\\n  âš  Skipping '{subgroup}': Only {n_obs} observations (min={min_subgroup_size})\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n  ðŸ“Š Subgroup: '{subgroup}' (N={n_obs:,})\")\n",
    "\n",
    "                # Check if we have the instrument in this subgroup\n",
    "                if 'Clicks_predicted' not in subgroup_data.columns:\n",
    "                    print(f\"     âœ— No instrument available\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    # FIRST STAGE: D ~ Z + X (within subgroup)\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "                    X_first = sm.add_constant(pd.concat([\n",
    "                        subgroup_data[['Clicks_predicted']],\n",
    "                        subgroup_data[available_controls]\n",
    "                    ], axis=1))\n",
    "\n",
    "                    y_first = subgroup_data['Clicks']\n",
    "\n",
    "                    first_stage = sm.OLS(y_first, X_first).fit()\n",
    "\n",
    "                    # First stage diagnostics\n",
    "                    f_stat_first = first_stage.fvalue\n",
    "                    r2_first = first_stage.rsquared\n",
    "                    instrument_coef = first_stage.params['Clicks_predicted']\n",
    "                    instrument_pval = first_stage.pvalues['Clicks_predicted']\n",
    "\n",
    "                    # Weak instrument check\n",
    "                    is_weak = f_stat_first < 10\n",
    "\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    # SECOND STAGE: Y ~ DÌ‚ + X (within subgroup)\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "                    D_hat = first_stage.fittedvalues\n",
    "\n",
    "                    X_second = sm.add_constant(pd.concat([\n",
    "                        pd.Series(D_hat, name='Clicks_fitted'),\n",
    "                        subgroup_data[available_controls]\n",
    "                    ], axis=1))\n",
    "\n",
    "                    y_second = subgroup_data['Conversion_Rate']\n",
    "\n",
    "                    second_stage = sm.OLS(y_second, X_second).fit()\n",
    "\n",
    "                    # Extract causal effect\n",
    "                    causal_effect = second_stage.params['Clicks_fitted']\n",
    "                    se = second_stage.bse['Clicks_fitted']\n",
    "                    tstat = second_stage.tvalues['Clicks_fitted']\n",
    "                    pval = second_stage.pvalues['Clicks_fitted']\n",
    "                    ci_lower = causal_effect - 1.96 * se\n",
    "                    ci_upper = causal_effect + 1.96 * se\n",
    "\n",
    "                    # Statistical significance\n",
    "                    is_significant = pval < 0.05\n",
    "\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    # DISPLAY SUBGROUP RESULTS\n",
    "                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "                    print(f\"     First Stage:\")\n",
    "                    print(f\"       F-stat: {f_stat_first:.2f} {'âœ— WEAK' if is_weak else 'âœ“ STRONG'}\")\n",
    "                    print(f\"       RÂ²: {r2_first:.4f}\")\n",
    "\n",
    "                    print(f\"     Second Stage (Causal Effect):\")\n",
    "                    print(f\"       Î²: {causal_effect:.6f}\")\n",
    "                    print(f\"       SE: {se:.6f}\")\n",
    "                    print(f\"       95% CI: [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "                    print(f\"       p-value: {pval:.4f} {'âœ“ SIGNIFICANT' if is_significant else 'âœ— Not significant'}\")\n",
    "\n",
    "                    # Store results\n",
    "                    all_results.append({\n",
    "                        'Variable': var,\n",
    "                        'Subgroup': str(subgroup),\n",
    "                        'N': n_obs,\n",
    "                        'First_Stage_F': f_stat_first,\n",
    "                        'First_Stage_R2': r2_first,\n",
    "                        'Instrument_Weak': is_weak,\n",
    "                        'Causal_Effect_Beta': causal_effect,\n",
    "                        'Std_Error': se,\n",
    "                        'T_Statistic': tstat,\n",
    "                        'P_Value': pval,\n",
    "                        'CI_Lower': ci_lower,\n",
    "                        'CI_Upper': ci_upper,\n",
    "                        'Significant': is_significant\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"     âœ— Error: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "        # =========================================================================\n",
    "        # SUMMARY TABLE\n",
    "        # =========================================================================\n",
    "\n",
    "        if len(all_results) == 0:\n",
    "            print(\"\\nâš  No subgroups analyzed successfully\")\n",
    "            return None\n",
    "\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SUBGROUP EFFECTS SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Sort by absolute effect size\n",
    "        results_df['Abs_Effect'] = results_df['Causal_Effect_Beta'].abs()\n",
    "        results_df = results_df.sort_values('Abs_Effect', ascending=False)\n",
    "\n",
    "        # Display table\n",
    "        display_cols = [\n",
    "            'Variable', 'Subgroup', 'N', \n",
    "            'Causal_Effect_Beta', 'P_Value', 'Significant',\n",
    "            'First_Stage_F', 'Instrument_Weak'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n\" + results_df[display_cols].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # KEY INSIGHTS\n",
    "        # =========================================================================\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEY INSIGHTS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Find strongest effects\n",
    "        significant_effects = results_df[results_df['Significant'] == True]\n",
    "\n",
    "        if len(significant_effects) > 0:\n",
    "            print(f\"\\nâœ“ Found {len(significant_effects)} subgroups with SIGNIFICANT causal effects:\")\n",
    "\n",
    "            for _, row in significant_effects.head(5).iterrows():\n",
    "                print(f\"\\n  â€¢ {row['Variable']} = '{row['Subgroup']}':\")\n",
    "                print(f\"    - Causal effect: {row['Causal_Effect_Beta']:.6f}\")\n",
    "                print(f\"    - 95% CI: [{row['CI_Lower']:.6f}, {row['CI_Upper']:.6f}]\")\n",
    "                print(f\"    - p-value: {row['P_Value']:.4f}\")\n",
    "                print(f\"    - Sample size: {row['N']:,}\")\n",
    "        else:\n",
    "            print(\"\\nâœ— No subgroups with statistically significant effects found\")\n",
    "\n",
    "        # Check for weak instruments in subgroups\n",
    "        weak_instruments = results_df[results_df['Instrument_Weak'] == True]\n",
    "        if len(weak_instruments) > 0:\n",
    "            print(f\"\\nâš  Warning: {len(weak_instruments)} subgroups have weak instruments (F < 10)\")\n",
    "            print(\"  Consider these results with caution\")\n",
    "\n",
    "        # Effect heterogeneity\n",
    "        effect_range = results_df['Causal_Effect_Beta'].max() - results_df['Causal_Effect_Beta'].min()\n",
    "        print(f\"\\nðŸ“Š Effect Heterogeneity:\")\n",
    "        print(f\"  Range: {effect_range:.6f}\")\n",
    "        print(f\"  Max effect: {results_df['Causal_Effect_Beta'].max():.6f} ({results_df.loc[results_df['Causal_Effect_Beta'].idxmax(), 'Subgroup']})\")\n",
    "        print(f\"  Min effect: {results_df['Causal_Effect_Beta'].min():.6f} ({results_df.loc[results_df['Causal_Effect_Beta'].idxmin(), 'Subgroup']})\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Store results for later access\n",
    "        self.subgroup_results = results_df\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    # def _calculate_2sls_standard_errors(self, controls):\n",
    "    #     \"\"\"\n",
    "    #     Calculate correct 2SLS standard errors\n",
    "    #     (OLS on second stage gives incorrect SEs)\n",
    "    #     \"\"\"\n",
    "    #     # Get residuals from second stage\n",
    "    #     residuals = self.second_stage_results.resid\n",
    "        \n",
    "    #     # Calculate robust variance-covariance matrix\n",
    "    #     n = len(residuals)\n",
    "    #     k = len(self.second_stage_results.params)\n",
    "        \n",
    "    #     # Simple correction factor\n",
    "    #     correction = n / (n - k)\n",
    "        \n",
    "    #     # Store corrected standard errors\n",
    "    #     self.corrected_se = np.sqrt(np.diag(self.second_stage_results.cov_params()) * correction)\n",
    "    #     self.corrected_tvalues = self.second_stage_results.params / self.corrected_se\n",
    "    #     self.corrected_pvalues = 2 * (1 - stats.t.cdf(np.abs(self.corrected_tvalues), n - k))\n",
    "    \n",
    "    def _display_results(self):\n",
    "        \"\"\"Display 2SLS results\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TWO-STAGE LEAST SQUARES (2SLS) RESULTS\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create results table\n",
    "        results_df = pd.DataFrame({\n",
    "            'Coefficient': self.second_stage_results.params,\n",
    "            'Std Error': self.corrected_se,\n",
    "            't-statistic': self.corrected_tvalues,\n",
    "            'P-value': self.corrected_pvalues\n",
    "        })\n",
    "        \n",
    "        # Add confidence intervals\n",
    "        results_df['95% CI Lower'] = results_df['Coefficient'] - 1.96 * results_df['Std Error']\n",
    "        results_df['95% CI Upper'] = results_df['Coefficient'] + 1.96 * results_df['Std Error']\n",
    "        \n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CAUSAL INTERPRETATION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        clicks_coef = self.second_stage_results.params['Clicks_fitted']\n",
    "        clicks_se = self.corrected_se[1]  # Index 1 for Clicks_fitted (after constant)\n",
    "        clicks_pval = self.corrected_pvalues[1]\n",
    "        \n",
    "        print(f\"\\nCausal Effect of Clicks on Conversion Rate:\")\n",
    "        print(f\"  Coefficient (Î²): {clicks_coef:.6f}\")\n",
    "        print(f\"  Std. Error: {clicks_se:.6f}\")\n",
    "        print(f\"  95% CI: [{clicks_coef - 1.96*clicks_se:.6f}, {clicks_coef + 1.96*clicks_se:.6f}]\")\n",
    "        print(f\"  P-value: {clicks_pval:.4f}\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  A 1-unit increase in Clicks causes a {clicks_coef:.6f} change\")\n",
    "        print(f\"  in Conversion Rate (controlling for confounders)\")\n",
    "        \n",
    "        if clicks_pval < 0.05:\n",
    "            print(f\"  âœ“ Effect is statistically significant at 5% level\")\n",
    "        else:\n",
    "            print(f\"  âœ— Effect is NOT statistically significant at 5% level\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    def estimate_value_added(self):\n",
    "        \"\"\"\n",
    "        Step 4: Value-Added Estimation\n",
    "        \n",
    "        Estimate the incremental contribution of different ad features\n",
    "        after controlling for user characteristics and predicted clicks.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Group by Ad Type\n",
    "        if 'Ad_Type' in self.data.columns:\n",
    "            results['by_ad_type'] = self._group_value_added('Ad_Type')\n",
    "        \n",
    "        # Group by Ad Placement\n",
    "        if 'Ad_Placement' in self.data.columns:\n",
    "            results['by_ad_placement'] = self._group_value_added('Ad_Placement')\n",
    "        \n",
    "        # Group by Ad Topic\n",
    "        if 'Ad_Topic' in self.data.columns:\n",
    "            results['by_ad_topic'] = self._group_value_added('Ad_Topic')\n",
    "        \n",
    "        self._display_value_added(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _group_value_added(self, group_col):\n",
    "        \"\"\"Calculate value-added for a specific grouping variable\"\"\"\n",
    "        group_results = []\n",
    "        \n",
    "        for group in self.data[group_col].unique():\n",
    "            # Create indicator variable\n",
    "            indicator = (self.data[group_col] == group).astype(int)\n",
    "            \n",
    "            # Prepare regression with interaction\n",
    "            y = self.data['Conversion_Rate']\n",
    "            X = sm.add_constant(pd.DataFrame({\n",
    "                'Clicks_predicted': self.data['Clicks_predicted'],\n",
    "                'Age': self.data['Age'],\n",
    "                'Income': self.data['Income'],\n",
    "                'indicator': indicator,\n",
    "                'interaction': indicator * self.data['Clicks_predicted']\n",
    "            }))\n",
    "            \n",
    "            # Run OLS\n",
    "            try:\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                \n",
    "                group_results.append({\n",
    "                    'Group': str(group),\n",
    "                    'Intercept_Effect': f\"{model.params['indicator']:.6f}\",\n",
    "                    'Slope_Effect': f\"{model.params['interaction']:.6f}\",\n",
    "                    'P_value_Intercept': f\"{model.pvalues['indicator']:.4f}\",\n",
    "                    'P_value_Slope': f\"{model.pvalues['interaction']:.4f}\",\n",
    "                    'Significant': 'âœ“' if model.pvalues['indicator'] < 0.05 or model.pvalues['interaction'] < 0.05 else 'âœ—'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not estimate for {group}: {str(e)}\")\n",
    "        \n",
    "        return pd.DataFrame(group_results)\n",
    "    \n",
    "    def _display_value_added(self, results):\n",
    "        \"\"\"Display value-added results\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"VALUE-ADDED ESTIMATION RESULTS\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for key, df in results.items():\n",
    "            if len(df) > 0:\n",
    "                print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                print(df.to_string(index=False))\n",
    "                print(f\"{'-'*60}\\n\")\n",
    "\n",
    "    def run_chetty_value_added_analysis(self, subgroup_vars=None, split_method='time', \n",
    "                                         split_ratio=0.5, min_group_size=100):\n",
    "        \"\"\"\n",
    "        INTEGRATED PIPELINE: Combines subgroup analysis with Chetty's forecast bias framework.\n",
    "\n",
    "        This is the main method you should call. It properly chains:\n",
    "        1. analyze_subgroup_effects() on TRAINING data\n",
    "        2. Forecast validation on TESTING data  \n",
    "        3. Empirical Bayes shrinkage\n",
    "        4. Bias correction\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        subgroup_vars : list of str or dict\n",
    "            Variables to stratify by (same format as analyze_subgroup_effects)\n",
    "            Example: {'Income': [0, 30000, 60000, np.inf], 'Location': None}\n",
    "        split_method : str\n",
    "            'time' - split by Click_Time (chronological)\n",
    "            'random' - random split with seed\n",
    "        split_ratio : float\n",
    "            Proportion for training (default: 0.5)\n",
    "        min_group_size : int\n",
    "            Minimum observations per group in EACH split\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            Complete results including raw estimates, bias tests, and corrections\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTEGRATED CHETTY VALUE-ADDED ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Split method: {split_method}\")\n",
    "        print(f\"  Split ratio: {split_ratio:.1%} train / {1-split_ratio:.1%} test\")\n",
    "        print(f\"  Min group size: {min_group_size}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 1: SPLIT DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 1: DATA SPLITTING\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        original_data = self.data.copy()\n",
    "\n",
    "        if split_method == 'time':\n",
    "            self.data = self.data.sort_values('Click_Time')\n",
    "            split_idx = int(len(self.data) * split_ratio)\n",
    "            train_indices = self.data.index[:split_idx]\n",
    "            test_indices = self.data.index[split_idx:]\n",
    "\n",
    "            train_data = self.data.loc[train_indices].copy()\n",
    "            test_data = self.data.loc[test_indices].copy()\n",
    "\n",
    "            print(f\"âœ“ Time-based split:\")\n",
    "            print(f\"  Training: {train_data['Click_Time'].min()} to {train_data['Click_Time'].max()}\")\n",
    "            print(f\"  Testing:  {test_data['Click_Time'].min()} to {test_data['Click_Time'].max()}\")\n",
    "\n",
    "        elif split_method == 'random':\n",
    "            train_data = self.data.sample(frac=split_ratio, random_state=42)\n",
    "            test_data = self.data.drop(train_data.index)\n",
    "            print(f\"âœ“ Random split (seed=42)\")\n",
    "\n",
    "        print(f\"  Training N: {len(train_data):,}\")\n",
    "        print(f\"  Testing N:  {len(test_data):,}\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 2: RUN SUBGROUP ANALYSIS ON TRAINING DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 2: ESTIMATE VALUE-ADDED (Training Sample)\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        # Temporarily replace self.data with training data\n",
    "        self.data = train_data.copy()\n",
    "\n",
    "        # Run subgroup analysis (this uses the existing method)\n",
    "        train_results = self.analyze_subgroup_effects(\n",
    "            subgroup_vars=subgroup_vars,\n",
    "            min_subgroup_size=min_group_size\n",
    "        )\n",
    "\n",
    "        if train_results is None or len(train_results) == 0:\n",
    "            print(\"\\nâœ— No subgroups successfully estimated in training data\")\n",
    "            self.data = original_data  # Restore\n",
    "            return None\n",
    "\n",
    "        print(f\"\\nâœ“ Estimated value-added for {len(train_results)} subgroups\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 3: VALIDATE FORECASTS IN TESTING DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 3: FORECAST VALIDATION (Testing Sample)\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        # For each group, calculate mean outcome in test sample\n",
    "        test_outcomes = []\n",
    "\n",
    "        for _, row in train_results.iterrows():\n",
    "            var = row['Variable']\n",
    "            subgroup = row['Subgroup']\n",
    "\n",
    "            # Filter test data to this subgroup\n",
    "            if '_subgroup' in var:\n",
    "                # This was a binned continuous variable\n",
    "                # We need to recreate the bins\n",
    "                continue  # Skip for now - handle separately\n",
    "            else:\n",
    "                # Categorical variable\n",
    "                group_test = test_data[test_data[var] == subgroup]\n",
    "\n",
    "            if len(group_test) < min_group_size:\n",
    "                continue\n",
    "            \n",
    "            # Mean conversion rate in test sample\n",
    "            mean_outcome = group_test['Conversion_Rate'].mean()\n",
    "            n_test = len(group_test)\n",
    "\n",
    "            test_outcomes.append({\n",
    "                'Variable': var,\n",
    "                'Subgroup': subgroup,\n",
    "                'Value_Added_Train': row['Causal_Effect_Beta'],\n",
    "                'SE_Train': row['Std_Error'],\n",
    "                'N_Train': row['N'],\n",
    "                'Mean_Outcome_Test': mean_outcome,\n",
    "                'N_Test': n_test,\n",
    "                'First_Stage_F': row['First_Stage_F']\n",
    "            })\n",
    "\n",
    "        test_df = pd.DataFrame(test_outcomes)\n",
    "\n",
    "        if len(test_df) < 3:\n",
    "            print(f\"\\nâœ— Insufficient groups for validation (need â‰¥3, have {len(test_df)})\")\n",
    "            self.data = original_data\n",
    "            return None\n",
    "\n",
    "        print(f\"âœ“ Validated {len(test_df)} groups in testing sample\")\n",
    "        print(f\"\\nTest Sample Statistics:\")\n",
    "        print(test_df[['Variable', 'Subgroup', 'Value_Added_Train', 'Mean_Outcome_Test', 'N_Test']].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 4: FORECAST UNBIASEDNESS TEST\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 4: FORECAST UNBIASEDNESS TEST\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "        print(\"\\nRegression: Mean_Outcome_test = Î± + Î² * ValueAdded_train + Îµ\")\n",
    "        print(\"Hâ‚€: Î² = 1 (unbiased forecasts)\")\n",
    "\n",
    "        # Run regression\n",
    "        X_forecast = sm.add_constant(test_df['Value_Added_Train'])\n",
    "        y_forecast = test_df['Mean_Outcome_Test']\n",
    "\n",
    "        # Weight by test sample size for precision\n",
    "        weights = np.sqrt(test_df['N_Test'])\n",
    "        forecast_model = sm.WLS(y_forecast, X_forecast, weights=weights).fit()\n",
    "\n",
    "        # Extract coefficients\n",
    "        alpha = forecast_model.params['const']\n",
    "        beta = forecast_model.params['Value_Added_Train']\n",
    "        beta_se = forecast_model.bse['Value_Added_Train']\n",
    "        r_squared = forecast_model.rsquared\n",
    "\n",
    "        # Test Î² = 1\n",
    "        t_stat_bias = (beta - 1) / beta_se\n",
    "        p_val_bias = 2 * (1 - stats.t.cdf(abs(t_stat_bias), len(test_df) - 2))\n",
    "\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Î± (intercept):     {alpha:.6f}\")\n",
    "        print(f\"  Î² (slope):         {beta:.6f} (SE: {beta_se:.6f})\")\n",
    "        print(f\"  Prediction RÂ²:     {r_squared:.4f}\")\n",
    "        print(f\"\\n  Test Hâ‚€: Î² = 1\")\n",
    "        print(f\"  t-statistic:       {t_stat_bias:.3f}\")\n",
    "        print(f\"  p-value:           {p_val_bias:.4f}\")\n",
    "\n",
    "        is_biased = p_val_bias < 0.05\n",
    "\n",
    "        if is_biased:\n",
    "            print(f\"\\n  âœ— REJECT Hâ‚€: Significant forecast bias detected\")\n",
    "            if beta < 1:\n",
    "                bias_direction = \"OVERPREDICTION (regression to mean)\"\n",
    "                print(f\"    â†’ Training estimates overpredict test outcomes\")\n",
    "                print(f\"    â†’ Shrinkage is strongly recommended\")\n",
    "            else:\n",
    "                bias_direction = \"UNDERPREDICTION\"\n",
    "                print(f\"    â†’ Training estimates underpredict test outcomes\")\n",
    "        else:\n",
    "            bias_direction = \"MINIMAL\"\n",
    "            print(f\"\\n  âœ“ FAIL TO REJECT Hâ‚€: No significant bias\")\n",
    "            print(f\"    â†’ Training estimates predict test outcomes well\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 5: EMPIRICAL BAYES SHRINKAGE\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 5: EMPIRICAL BAYES SHRINKAGE\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        # Calculate variance components\n",
    "        va_estimates = train_results['Causal_Effect_Beta'].values\n",
    "        va_variances = train_results['Std_Error'].values ** 2\n",
    "\n",
    "        # Grand mean\n",
    "        mu = np.mean(va_estimates)\n",
    "\n",
    "        # Variance decomposition\n",
    "        var_total = np.var(va_estimates)\n",
    "        var_noise = np.mean(va_variances)\n",
    "        var_signal = max(0, var_total - var_noise)\n",
    "\n",
    "        # Reliability\n",
    "        reliability = var_signal / (var_signal + var_noise) if (var_signal + var_noise) > 0 else 0\n",
    "\n",
    "        print(f\"\\nVariance Decomposition:\")\n",
    "        print(f\"  Between-group variance (signal): {var_signal:.8f}\")\n",
    "        print(f\"  Within-group variance (noise):   {var_noise:.8f}\")\n",
    "        print(f\"  Total variance:                  {var_total:.8f}\")\n",
    "        print(f\"  Reliability (Î»Ì„):                 {reliability:.4f}\")\n",
    "\n",
    "        # Group-specific shrinkage\n",
    "        shrinkage_results = []\n",
    "\n",
    "        print(f\"\\nGroup-Specific Shrinkage:\")\n",
    "        print(f\"{'Variable':<15} {'Subgroup':<20} {'Raw VA':<12} {'Î»':<8} {'Shrunk VA':<12}\")\n",
    "        print(f\"{'-'*75}\")\n",
    "\n",
    "        for _, row in train_results.iterrows():\n",
    "            raw_va = row['Causal_Effect_Beta']\n",
    "            se = row['Std_Error']\n",
    "\n",
    "            # Shrinkage factor for this group\n",
    "            lambda_i = var_signal / (var_signal + se**2) if (var_signal + se**2) > 0 else 0\n",
    "\n",
    "            # Shrink toward grand mean\n",
    "            shrunk_va = mu + lambda_i * (raw_va - mu)\n",
    "\n",
    "            print(f\"{row['Variable']:<15} {str(row['Subgroup']):<20} {raw_va:>11.6f} {lambda_i:>7.4f} {shrunk_va:>11.6f}\")\n",
    "\n",
    "            shrinkage_results.append({\n",
    "                'Variable': row['Variable'],\n",
    "                'Subgroup': row['Subgroup'],\n",
    "                'Raw_VA': raw_va,\n",
    "                'Shrinkage_Factor': lambda_i,\n",
    "                'Shrunk_VA': shrunk_va,\n",
    "                'SE': se,\n",
    "                'N_Train': row['N']\n",
    "            })\n",
    "\n",
    "        shrinkage_df = pd.DataFrame(shrinkage_results)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 6: BIAS CORRECTION\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"STEP 6: FORECAST BIAS CORRECTION\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        # Bias-corrected estimates: (Shrunk_VA - Î±) / Î²\n",
    "        shrinkage_df['Bias_Corrected_VA'] = (shrinkage_df['Shrunk_VA'] - alpha) / beta if beta != 0 else shrinkage_df['Shrunk_VA']\n",
    "\n",
    "        print(f\"\\nApplying correction: VA_final = (VA_shrunk - {alpha:.6f}) / {beta:.6f}\")\n",
    "        print(f\"\\nFinal Value-Added Estimates:\")\n",
    "\n",
    "        # Sort by bias-corrected VA\n",
    "        shrinkage_df = shrinkage_df.sort_values('Bias_Corrected_VA', ascending=False)\n",
    "\n",
    "        display_cols = ['Variable', 'Subgroup', 'Raw_VA', 'Shrunk_VA', 'Bias_Corrected_VA', 'N_Train']\n",
    "        print(\"\\n\" + shrinkage_df[display_cols].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 7: SUMMARY AND INTERPRETATION\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SUMMARY: CORRECTIONS APPLIED\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Calculate average corrections\n",
    "        avg_shrinkage = (shrinkage_df['Shrunk_VA'] - shrinkage_df['Raw_VA']).abs().mean()\n",
    "        avg_bias_correction = (shrinkage_df['Bias_Corrected_VA'] - shrinkage_df['Shrunk_VA']).abs().mean()\n",
    "        avg_total_correction = (shrinkage_df['Bias_Corrected_VA'] - shrinkage_df['Raw_VA']).abs().mean()\n",
    "\n",
    "        print(f\"\\nAverage absolute corrections:\")\n",
    "        print(f\"  Shrinkage effect:      {avg_shrinkage:.6f}\")\n",
    "        print(f\"  Bias correction:       {avg_bias_correction:.6f}\")\n",
    "        print(f\"  Total correction:      {avg_total_correction:.6f}\")\n",
    "\n",
    "        print(f\"\\nDiagnostics:\")\n",
    "        print(f\"  Reliability:           {reliability:.4f}\")\n",
    "        print(f\"  Forecast bias (Î²):     {beta:.4f}\")\n",
    "        print(f\"  Prediction RÂ²:         {r_squared:.4f}\")\n",
    "        print(f\"  Bias type:             {bias_direction}\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        if reliability < 0.3:\n",
    "            print(\"  âš  Low reliability - value-added estimates are very noisy\")\n",
    "            print(\"    â†’ Collect more data or use coarser groupings\")\n",
    "        elif reliability < 0.7:\n",
    "            print(\"  âœ“ Moderate reliability - shrinkage is important\")\n",
    "        else:\n",
    "            print(\"  âœ“âœ“ High reliability - raw estimates are fairly trustworthy\")\n",
    "\n",
    "        if is_biased:\n",
    "            print(f\"  âš  Forecast bias detected - bias correction is essential\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Minimal forecast bias - raw estimates predict well\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # PACKAGE RESULTS\n",
    "        # =========================================================================\n",
    "\n",
    "        results = {\n",
    "            'split_info': {\n",
    "                'method': split_method,\n",
    "                'n_train': len(train_data),\n",
    "                'n_test': len(test_data)\n",
    "            },\n",
    "            'train_estimates': train_results,\n",
    "            'test_outcomes': test_df,\n",
    "            'forecast_bias': {\n",
    "                'alpha': alpha,\n",
    "                'beta': beta,\n",
    "                'beta_se': beta_se,\n",
    "                'p_value': p_val_bias,\n",
    "                'r_squared': r_squared,\n",
    "                'is_biased': is_biased,\n",
    "                'direction': bias_direction\n",
    "            },\n",
    "            'shrinkage': {\n",
    "                'reliability': reliability,\n",
    "                'var_signal': var_signal,\n",
    "                'var_noise': var_noise,\n",
    "                'mu': mu\n",
    "            },\n",
    "            'final_estimates': shrinkage_df\n",
    "        }\n",
    "\n",
    "        # Store for later access\n",
    "        self.chetty_results = results\n",
    "\n",
    "        # Restore original data\n",
    "        self.data = original_data\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTEGRATED ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def compare_estimation_approaches(self):\n",
    "        \"\"\"\n",
    "        Create side-by-side comparison of estimation approaches.\n",
    "        Must be called after run_chetty_value_added_analysis().\n",
    "\n",
    "        Compares:\n",
    "        1. Raw 2SLS estimates (training sample)\n",
    "        2. Shrunk estimates (Empirical Bayes)\n",
    "        3. Bias-corrected estimates (Full Chetty method)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        comparison_df : pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'chetty_results'):\n",
    "            print(\"\\nâš  Must run run_chetty_value_added_analysis() first\")\n",
    "            return None\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARISON: ESTIMATION APPROACHES\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        final = self.chetty_results['final_estimates']\n",
    "\n",
    "        # Calculate changes\n",
    "        final['Change_from_Shrinkage'] = final['Shrunk_VA'] - final['Raw_VA']\n",
    "        final['Change_from_Bias_Corr'] = final['Bias_Corrected_VA'] - final['Shrunk_VA']\n",
    "        final['Total_Change'] = final['Bias_Corrected_VA'] - final['Raw_VA']\n",
    "        final['Pct_Change'] = (final['Total_Change'] / final['Raw_VA'].abs()) * 100\n",
    "\n",
    "        display_cols = [\n",
    "            'Variable', 'Subgroup',\n",
    "            'Raw_VA', 'Shrunk_VA', 'Bias_Corrected_VA',\n",
    "            'Total_Change', 'Pct_Change'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n\" + final[display_cols].to_string(index=False))\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "\n",
    "        print(f\"\\nMean absolute change:\")\n",
    "        print(f\"  From shrinkage:        {final['Change_from_Shrinkage'].abs().mean():.6f}\")\n",
    "        print(f\"  From bias correction:  {final['Change_from_Bias_Corr'].abs().mean():.6f}\")\n",
    "        print(f\"  Total:                 {final['Total_Change'].abs().mean():.6f}\")\n",
    "\n",
    "        print(f\"\\nMean % change:           {final['Pct_Change'].abs().mean():.1f}%\")\n",
    "\n",
    "        # Which groups changed most?\n",
    "        print(f\"\\nGroups with largest corrections:\")\n",
    "        top_changes = final.nlargest(3, 'Total_Change', keep='all')[['Subgroup', 'Raw_VA', 'Bias_Corrected_VA', 'Total_Change']]\n",
    "        print(top_changes.to_string(index=False))\n",
    "\n",
    "        return final\n",
    "\n",
    "    ### WITH Subgroup analysis\n",
    "    def run_complete_analysis(self, model_type='stacking', include_interactions=False, \n",
    "                            use_enhanced_features=True, analyze_subgroups=False,\n",
    "                            subgroup_vars=None, min_subgroup_size=100):\n",
    "        \"\"\"\n",
    "        ENHANCED: Run the complete causal inference pipeline with strong instruments.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            'stacking' (RECOMMENDED), 'rf', or 'gb'\n",
    "        include_interactions : bool\n",
    "            Whether to include interactions in 2SLS\n",
    "        use_enhanced_features : bool\n",
    "            Whether to use enhanced feature engineering (strongly recommended)\n",
    "        analyze_subgroups : bool\n",
    "            Whether to run stratified 2SLS to identify heterogeneous treatment effects\n",
    "        subgroup_vars : list of str or dict, optional\n",
    "            Variables to stratify by when analyze_subgroups=True\n",
    "            Example: ['Location', 'Ad_Type'] or \n",
    "                     {'Income': [0, 30000, 60000, np.inf], 'Age': [0, 35, 50, 65, np.inf]}\n",
    "        min_subgroup_size : int\n",
    "            Minimum observations required per subgroup (default: 100)\n",
    "\n",
    "        Pipeline:\n",
    "        1. Engineer time features\n",
    "        2. Encode categorical variables\n",
    "        3. Create ML instrument (with optional enhanced features)\n",
    "        4. Run 2SLS\n",
    "        5. [OPTIONAL] Analyze subgroup effects\n",
    "        6. Estimate value-added\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ENHANCED CAUSAL AD CONVERSION ANALYSIS PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Model type: {model_type.upper()}\")\n",
    "        print(f\"  Enhanced features: {'YES' if use_enhanced_features else 'NO'}\")\n",
    "        print(f\"  Include interactions in 2SLS: {'YES' if include_interactions else 'NO'}\")\n",
    "        print(f\"  Analyze subgroups: {'YES' if analyze_subgroups else 'NO'}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        self.engineer_time_features()\n",
    "        print(\"\\nâœ“ Time features engineered\")\n",
    "\n",
    "        self.encode_categorical_features()\n",
    "        print(\"âœ“ Categorical variables encoded\")\n",
    "\n",
    "        self.create_ml_instrument(\n",
    "            model_type=model_type,\n",
    "            use_enhanced_features=use_enhanced_features\n",
    "        )\n",
    "        print(\"âœ“ ML instrument created\")\n",
    "\n",
    "        self.run_2sls(include_interactions=include_interactions)\n",
    "        print(\"âœ“ 2SLS estimation complete\")\n",
    "\n",
    "        # NEW: Optional subgroup analysis\n",
    "        if analyze_subgroups:\n",
    "            subgroup_results = self.analyze_subgroup_effects(\n",
    "                subgroup_vars=subgroup_vars,\n",
    "                min_subgroup_size=min_subgroup_size\n",
    "            )\n",
    "            print(\"âœ“ Subgroup effects analysis complete\")\n",
    "\n",
    "        value_added_results = self.estimate_value_added()\n",
    "        print(\"âœ“ Value-added estimation complete\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANALYSIS COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return self\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE WITH COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def compare_instrument_approaches(data):\n",
    "    \"\"\"\n",
    "    Compare weak vs strong instruments to demonstrate improvement\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON: WEAK vs STRONG INSTRUMENTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Approach 1: Basic features, single model (WEAK)\n",
    "    print(\"\\n\\n\" + \"ðŸ”´ APPROACH 1: BASIC (LIKELY WEAK)\")\n",
    "    print(\"=\"*70)\n",
    "    analyzer_weak = CausalAdAnalyzer(data.copy())\n",
    "    analyzer_weak.engineer_time_features()\n",
    "    analyzer_weak.encode_categorical_features()\n",
    "    analyzer_weak.create_ml_instrument(\n",
    "        model_type='rf',\n",
    "        use_enhanced_features=False  # No enhanced features\n",
    "    )\n",
    "    \n",
    "    # Approach 2: Enhanced features, stacking ensemble (STRONG)\n",
    "    print(\"\\n\\n\" + \"ðŸŸ¢ APPROACH 2: ENHANCED (STRONG)\")\n",
    "    print(\"=\"*70)\n",
    "    analyzer_strong = CausalAdAnalyzer(data.copy())\n",
    "    analyzer_strong.engineer_time_features()\n",
    "    analyzer_strong.encode_categorical_features()\n",
    "    analyzer_strong.create_ml_instrument(\n",
    "        model_type='stacking',\n",
    "        use_enhanced_features=True  # Enhanced features\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract F-statistics for comparison\n",
    "    z_weak = analyzer_weak.data['Clicks_predicted'].values\n",
    "    d_weak = analyzer_weak.data['Clicks'].values\n",
    "    corr_weak = np.corrcoef(z_weak, d_weak)[0, 1]\n",
    "    f_weak = (corr_weak**2 / (1 - corr_weak**2)) * (len(d_weak) - 2)\n",
    "    \n",
    "    z_strong = analyzer_strong.data['Clicks_predicted'].values\n",
    "    d_strong = analyzer_strong.data['Clicks'].values\n",
    "    ss_tot = np.sum((d_strong - d_strong.mean())**2)\n",
    "    ss_res = np.sum((d_strong - z_strong)**2)\n",
    "    r2_strong = 1 - (ss_res / ss_tot)\n",
    "    n = len(d_strong)\n",
    "    k = len([col for col in analyzer_strong.data.columns if 'x' in col.lower() or 'squared' in col.lower()]) + 9\n",
    "    f_strong = (r2_strong / 1) / ((1 - r2_strong) / (n - k - 1))\n",
    "    \n",
    "    print(f\"\\nApproach 1 (Basic):\")\n",
    "    print(f\"  F-statistic: {f_weak:.2f}\")\n",
    "    print(f\"  Status: {'âœ— WEAK' if f_weak < 10 else 'âœ“ STRONG'}\")\n",
    "    \n",
    "    print(f\"\\nApproach 2 (Enhanced):\")\n",
    "    print(f\"  F-statistic: {f_strong:.2f}\")\n",
    "    print(f\"  Status: {'âœ— WEAK' if f_strong < 10 else 'âœ“ STRONG'}\")\n",
    "    \n",
    "    improvement = ((f_strong - f_weak) / f_weak) * 100\n",
    "    print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDATION: Use Approach 2 (Enhanced) for reliable causal inference\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return analyzer_weak, analyzer_strong\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # reading in the df\n",
    "    df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "\n",
    "    # # default setting with stacking, and enhanced machine learning.\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "\n",
    "    # # # Run complete pipeline\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',          # Use stacking ensemble\n",
    "    #     include_interactions=True,       # Include interactions in 2SLS\n",
    "    #     use_enhanced_features=True      # Use enhanced feature engineering\n",
    "    # )\n",
    "\n",
    "    # compares the methods, weak vs strong.\n",
    "    # analyzer_weak, analyzer_strong = compare_instrument_approaches(df)\n",
    "\n",
    "    # 2sls subgroup effect section\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "\n",
    "    # analyzer.run_2sls(include_interactions=True)\n",
    "\n",
    "    # # NEW: Analyze subgroup effects\n",
    "    # subgroup_results = analyzer.analyze_subgroup_effects(\n",
    "    #     subgroup_vars={\n",
    "    #         'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "    #         'Age': [0, 35, 50, 65, np.inf],\n",
    "    #         'Location': None,  # Categorical\n",
    "    #         'Ad_Type': None    # Categorical\n",
    "    #     },\n",
    "    #     min_subgroup_size=100\n",
    "    # )\n",
    "\n",
    "    # # Then continue with value-added\n",
    "    # analyzer.estimate_value_added()\n",
    "\n",
    "    # # Example 1: Run WITHOUT subgroup analysis (default behavior)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True\n",
    "    # )\n",
    "\n",
    "    # Example 2: Run WITH subgroup analysis (using default subgroups)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True  # Toggle this on\n",
    "    # )\n",
    "\n",
    "    # # Example 3: Run WITH subgroup analysis (custom subgroups)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True,\n",
    "    #     subgroup_vars={\n",
    "    #         'Income': [0, 35000, 55000, 75000, np.inf],  # Custom income bins\n",
    "    #         'Age': [0, 40, 60, np.inf],                   # Custom age bins\n",
    "    #         'Location': None,                              # Use as-is\n",
    "    #         'Ad_Type': None                                # Use as-is\n",
    "    #     },\n",
    "    #     min_subgroup_size=150  # Require at least 150 obs per subgroup\n",
    "    # )\n",
    "\n",
    "    # # Example 4: Only specific categorical subgroups\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True,\n",
    "    #     subgroup_vars=['Location', 'Ad_Type', 'Gender']  # Just these three\n",
    "    # )\n",
    "\n",
    "    # Trying to add Raj Chetty forecast bias's approach.\n",
    "    # After running your analysis This would run independently from the stratified gorup results..\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     use_enhanced_features=True\n",
    "    # )\n",
    "\n",
    "    # # Run Chetty-style forecast bias test\n",
    "    # bias_results = analyzer.test_forecast_bias(\n",
    "    #     split_method='time',      # or 'random'\n",
    "    #     split_ratio=0.5,\n",
    "    #     subgroup_var='Ad_Type',   # or 'Ad_Placement', 'Location'\n",
    "    #     min_group_size=50\n",
    "    # )\n",
    "\n",
    "    # df = generate_example_data(n=2000)    # # Compare methods\n",
    "    # comparison = analyzer.compare_value_added_methods(subgroup_var='Ad_Type')\n",
    "    # Initialize and run basic pipeline\n",
    "    analyzer = CausalAdAnalyzer(df)\n",
    "    analyzer.engineer_time_features()\n",
    "    analyzer.encode_categorical_features()\n",
    "    analyzer.create_ml_instrument(model_type='stacking', use_enhanced_features=True)\n",
    "\n",
    "    # Run integrated Chetty analysis\n",
    "    results = analyzer.run_chetty_value_added_analysis(\n",
    "        subgroup_vars={\n",
    "            'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "            'Age': [0, 35, 50, 65, np.inf],\n",
    "            'Location': None,\n",
    "            'Ad_Type': None\n",
    "        },\n",
    "        split_method='time',      # or 'random'\n",
    "        split_ratio=0.5,\n",
    "        min_group_size=100\n",
    "    )\n",
    "\n",
    "    # Compare approaches\n",
    "    comparison = analyzer.compare_estimation_approaches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3252debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:57:03,147 INFO: Running quick demo of MLInstrumentIV on simulated data\n",
      "2025-11-11 15:57:03,155 INFO: Running out-of-fold predictions for instrument with 5 folds\n",
      "2025-11-11 15:57:09,538 INFO: OOF instrument created and saved to column __instrument_oof__\n",
      "2025-11-11 15:57:09,550 INFO: Partial F for instrument(s): 1058.4318624997827\n",
      "2025-11-11 15:57:09,551 INFO: Partial R^2 for instrument(s): 0.34640990541802924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV summary:\n",
      "                           IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   R-squared:                      0.9270\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.9269\n",
      "No. Observations:                2000   F-statistic:                 1.444e+04\n",
      "Date:                Tue, Nov 11 2025   P-value (F-stat)                0.0000\n",
      "Time:                        15:57:09   Distribution:                  chi2(2)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0010     0.0215     0.0471     0.9624     -0.0411      0.0431\n",
      "X              1.0383     0.0237     43.885     0.0000      0.9919      1.0846\n",
      "endog          1.9767     0.0266     74.295     0.0000      1.9245      2.0288\n",
      "==============================================================================\n",
      "\n",
      "Endogenous: endog\n",
      "Instruments: __instrument_oof__\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n",
      "Partial F: 1058.4318624997827\n",
      "  covariate  coef_on_instrument         pvalue\n",
      "0         X            0.398923  7.076921e-108\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "iv_ml_pipeline.py\n",
    "\n",
    "Rewritten, robust, and well-documented pipeline to construct an ML-based instrument for\n",
    "an endogenous regressor and perform correct 2SLS estimation with diagnostics.\n",
    "\n",
    "Key features:\n",
    "- Out-of-fold (CV) instrument prediction without leakage using sklearn Pipelines\n",
    "- Proper 2SLS estimation using linearmodels.iv.IV2SLS with robust and clustered SEs\n",
    "- Correct computation of partial F-statistic for instrument strength (conditioning on controls)\n",
    "- Placebo / balance tests for instrument validity\n",
    "- Simulation utilities to unit-test the pipeline\n",
    "- Configurable clustering, diagnostics, and model hyperparameters\n",
    "\n",
    "NOTES:\n",
    "- Requires: scikit-learn, pandas, numpy, statsmodels, linearmodels\n",
    "  Install with: pip install scikit-learn pandas numpy statsmodels linearmodels\n",
    "\n",
    "- This file intentionally avoids fitting preprocessors on full data prior to CV.\n",
    "  Use Pipeline objects so that transform/fit happen inside each fold.\n",
    "\n",
    "Author: Generated by ChatGPT (senior-engineer style). Be strict: run tests before trusting results.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, List, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Logging config\n",
    "# ----------------------------\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\"))\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "\n",
    "def _assert_series(name: str, s: pd.Series) -> None:\n",
    "    assert isinstance(s, pd.Series), f\"{name} must be a pandas Series\"\n",
    "    assert not s.empty, f\"{name} must not be empty\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Data classes\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class IVResult:\n",
    "    model: any\n",
    "    params: pd.Series\n",
    "    std_errors: pd.Series\n",
    "    tstats: pd.Series\n",
    "    pvalues: pd.Series\n",
    "    summary: str\n",
    "    first_stage: Optional[sm.regression.linear_model.RegressionResultsWrapper] = None\n",
    "    partial_f: Optional[float] = None\n",
    "    partial_r2: Optional[float] = None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Core pipeline class\n",
    "# ----------------------------\n",
    "\n",
    "class MLInstrumentIV:\n",
    "    \"\"\"\n",
    "    Class to build an ML-based instrument and perform 2SLS with correct inference.\n",
    "\n",
    "    Typical workflow:\n",
    "      - instantiate with desired pipelines and options\n",
    "      - call fit_instrument_oof() to get out-of-fold instrument predictions\n",
    "      - call estimate_iv() to run IV2SLS using linearmodels and get diagnostics\n",
    "      - run balance_tests() to check instrument validity\n",
    "\n",
    "    Notes:\n",
    "      - All preprocessing required for instrument construction should be provided\n",
    "        as part of `instrument_pipeline` so we avoid leakage across CV folds.\n",
    "      - `controls` are treated as exogenous covariates in the structural equation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instrument_pipeline: Optional[Pipeline] = None,\n",
    "        instrument_features: Optional[Sequence[str]] = None,\n",
    "        outcome: str = \"outcome\",\n",
    "        endogenous: str = \"endog\",\n",
    "        controls: Optional[Sequence[str]] = None,\n",
    "        cv_folds: int = 5,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        self.instrument_pipeline = (\n",
    "            instrument_pipeline\n",
    "            if instrument_pipeline is not None\n",
    "            else self._default_instrument_pipeline()\n",
    "        )\n",
    "        self.instrument_features = list(instrument_features) if instrument_features else None\n",
    "        self.outcome = outcome\n",
    "        self.endogenous = endogenous\n",
    "        self.controls = list(controls) if controls else []\n",
    "        self.cv_folds = int(cv_folds)\n",
    "        self.random_state = int(random_state)\n",
    "\n",
    "        # placeholders populated after fit\n",
    "        self.data: Optional[pd.DataFrame] = None\n",
    "        self.oof_instrument_name = \"__instrument_oof__\"\n",
    "        self.fitted_full_pipeline: Optional[Pipeline] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_instrument_pipeline() -> Pipeline:\n",
    "        # Default pipeline: simple imputer + OHE for categoricals + RandomForestRegressor\n",
    "        numeric_transformer = Pipeline(\n",
    "            steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "        )\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numeric_transformer, []),  # actual numeric columns passed later in fit\n",
    "                (\"cat\", categorical_transformer, []),\n",
    "            ],\n",
    "            remainder=\"passthrough\",\n",
    "        )\n",
    "\n",
    "        # Use a tree-based regressor by default. Trees don't need scaling.\n",
    "        pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42))])\n",
    "        # Note: during fit_instrument_oof the ColumnTransformer will be re-constructed with actual columns.\n",
    "        return pipeline\n",
    "\n",
    "    def fit_instrument_oof(self, df: pd.DataFrame, instrument_feature_names: Optional[Sequence[str]] = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Fit the instrument model using cross-validated out-of-fold predictions to avoid leakage.\n",
    "\n",
    "        Returns a pandas Series of OOF predictions aligned with df.index.\n",
    "        \"\"\"\n",
    "        assert isinstance(df, pd.DataFrame), \"df must be a DataFrame\"\n",
    "        self.data = df.copy()\n",
    "\n",
    "        # Determine instrument features\n",
    "        if instrument_feature_names is not None:\n",
    "            self.instrument_features = list(instrument_feature_names)\n",
    "        if not self.instrument_features:\n",
    "            raise ValueError(\"instrument_features must be provided either at init or in fit_instrument_oof\")\n",
    "\n",
    "        X = df[self.instrument_features]\n",
    "        y = df[self.endogenous]\n",
    "        _assert_series(self.endogenous, y)\n",
    "\n",
    "        # Build a ColumnTransformer specific to columns: numeric vs categorical\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        num_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "        cat_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_transformer, numeric_cols),\n",
    "                (\"cat\", cat_transformer, categorical_cols),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "        )\n",
    "\n",
    "        # Clone the user-supplied pipeline but replace or build a pipeline with this preprocessor\n",
    "        pipeline = clone(self.instrument_pipeline)\n",
    "\n",
    "        try:\n",
    "            if hasattr(pipeline, \"steps\") and len(pipeline.steps) >= 1:\n",
    "                # extract last step\n",
    "                *prefix_steps, (last_name, last_estimator) = pipeline.steps\n",
    "                pipeline = Pipeline(prefix_steps + [(\"preprocessor\", preprocessor), (last_name, last_estimator)])\n",
    "            else:\n",
    "                pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=self.random_state))])\n",
    "        except Exception:\n",
    "            pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=self.random_state))])\n",
    "\n",
    "        # Do out-of-fold prediction using cross_val_predict which fits inside folds\n",
    "        cv = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        logger.info(\"Running out-of-fold predictions for instrument with %d folds\", self.cv_folds)\n",
    "\n",
    "        instrument_oof = cross_val_predict(pipeline, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "        # Fit pipeline on full dataset for later use (e.g., feature importances)\n",
    "        pipeline.fit(X, y)\n",
    "        self.fitted_full_pipeline = pipeline\n",
    "\n",
    "        # Save instrument predictions in self.data\n",
    "        self.data[self.oof_instrument_name] = instrument_oof\n",
    "        logger.info(\"OOF instrument created and saved to column %s\", self.oof_instrument_name)\n",
    "\n",
    "        return pd.Series(instrument_oof, index=df.index, name=self.oof_instrument_name)\n",
    "\n",
    "    def _prepare_iv_matrices(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Prepare matrices for IV estimation: y, D (endog), Z (instrument(s)), X (controls + constant)\n",
    "        Returns dict with keys 'y', 'D', 'Z', 'X'\n",
    "        \"\"\"\n",
    "        assert self.data is not None, \"Call fit_instrument_oof first to set self.data\"\n",
    "        df = self.data\n",
    "        y = df[self.outcome]\n",
    "        D = df[[self.endogenous]]\n",
    "\n",
    "        if self.oof_instrument_name not in df.columns:\n",
    "            raise ValueError(\"Instrument OOF column not found in data â€” run fit_instrument_oof first\")\n",
    "\n",
    "        Z = df[[self.oof_instrument_name]]\n",
    "\n",
    "        # Controls\n",
    "        X = df[self.controls] if self.controls else pd.DataFrame(index=df.index)\n",
    "        # Add constant to X (linearmodels expects exog separately)\n",
    "        X_const = sm.add_constant(X, has_constant=\"add\")\n",
    "\n",
    "        return {\"y\": y, \"D\": D, \"Z\": Z, \"X\": X_const}\n",
    "\n",
    "    def _compute_partial_f_and_r2(self, y: pd.Series, D: pd.DataFrame, Z: pd.DataFrame, X: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute partial F statistic and partial R^2 for instrument(s) Z in the first-stage\n",
    "        where first-stage is D ~ Z + X. We compute the SSR-based partial F.\n",
    "        \"\"\"\n",
    "        # Fit full model: D ~ Z + X\n",
    "        X_full = pd.concat([Z, X.drop(columns=[\"const\"])], axis=1) if \"const\" in X.columns else pd.concat([Z, X], axis=1)\n",
    "        X_full = sm.add_constant(X_full, has_constant=\"add\")\n",
    "        first_stage_full = sm.OLS(D, X_full).fit()\n",
    "        ssr_full = first_stage_full.ssr\n",
    "\n",
    "        # Fit reduced model: D ~ X\n",
    "        X_reduced = X\n",
    "        first_stage_reduced = sm.OLS(D, X_reduced).fit()\n",
    "        ssr_reduced = first_stage_reduced.ssr\n",
    "\n",
    "        q = Z.shape[1]  # number of instruments\n",
    "        df_num = q\n",
    "        df_den = int(first_stage_full.df_resid)\n",
    "        if df_den <= 0:\n",
    "            partial_f = float(\"nan\")\n",
    "        else:\n",
    "            partial_f = ((ssr_reduced - ssr_full) / df_num) / (ssr_full / df_den)\n",
    "\n",
    "        # Partial R^2: proportion of variance in D explained by Z conditional on X\n",
    "        if ssr_reduced == 0:\n",
    "            partial_r2 = float(\"nan\")\n",
    "        else:\n",
    "            partial_r2 = max(0.0, float((ssr_reduced - ssr_full) / ssr_reduced))\n",
    "\n",
    "        return {\"partial_f\": float(partial_f), \"partial_r2\": float(partial_r2)}\n",
    "\n",
    "    def estimate_iv(self, cluster_col: Optional[str] = None, cov_type: str = \"robust\") -> IVResult:\n",
    "        \"\"\"\n",
    "        Run IV (2SLS) estimation using the out-of-fold instrument(s) and controls.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_col: optional column name in self.data to use for clustered standard errors\n",
    "        cov_type: covariance type passed to IV2SLS.fit: 'robust', 'unadjusted', 'clustered'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        IVResult dataclass containing summary and diagnostics\n",
    "        \"\"\"\n",
    "        mats = self._prepare_iv_matrices()\n",
    "        y = mats[\"y\"]\n",
    "        D = mats[\"D\"]\n",
    "        Z = mats[\"Z\"]\n",
    "        X = mats[\"X\"]\n",
    "\n",
    "        # Compute partial F and partial R2\n",
    "        pf = self._compute_partial_f_and_r2(y=D.squeeze(), D=D, Z=Z, X=X)\n",
    "        partial_f = pf[\"partial_f\"]\n",
    "        partial_r2 = pf[\"partial_r2\"]\n",
    "        logger.info(\"Partial F for instrument(s): %s\", partial_f)\n",
    "        logger.info(\"Partial R^2 for instrument(s): %s\", partial_r2)\n",
    "\n",
    "        # linearmodels expects arrays/frames. Build IV2SLS\n",
    "        # Note: IV2SLS signature is IV2SLS(dependent, exog, endog, instruments)\n",
    "        exog = X  # includes const\n",
    "        endog = D\n",
    "        instruments = Z\n",
    "\n",
    "        iv_mod = IV2SLS(y, exog, endog, instruments)\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if cov_type == \"clustered\":\n",
    "            if cluster_col is None:\n",
    "                raise ValueError(\"cluster_col must be provided when cov_type='clustered'\")\n",
    "            clusters = self.data[cluster_col]\n",
    "            fit_kwargs[\"cov_type\"] = \"clustered\"\n",
    "            fit_kwargs[\"clusters\"] = clusters\n",
    "        else:\n",
    "            fit_kwargs[\"cov_type\"] = cov_type\n",
    "\n",
    "        iv_res = iv_mod.fit(**fit_kwargs)\n",
    "\n",
    "        # Prepare result\n",
    "        params = iv_res.params\n",
    "        std_errors = iv_res.std_errors\n",
    "        tstats = iv_res.tstats\n",
    "        pvalues = iv_res.pvalues\n",
    "        summary = str(iv_res.summary)\n",
    "\n",
    "        # compute first-stage OLS for reporting\n",
    "        X_fs = pd.concat([instruments, X.drop(columns=[\"const\"])], axis=1) if \"const\" in X.columns else pd.concat([instruments, X], axis=1)\n",
    "        X_fs = sm.add_constant(X_fs, has_constant=\"add\")\n",
    "        first_stage = sm.OLS(D, X_fs).fit(cov_type=\"HC1\")\n",
    "\n",
    "        return IVResult(\n",
    "            model=iv_res,\n",
    "            params=params,\n",
    "            std_errors=std_errors,\n",
    "            tstats=tstats,\n",
    "            pvalues=pvalues,\n",
    "            summary=summary,\n",
    "            first_stage=first_stage,\n",
    "            partial_f=partial_f,\n",
    "            partial_r2=partial_r2,\n",
    "        )\n",
    "\n",
    "    def balance_tests(self, covariates: Sequence[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run balance/placebo tests by regressing pre-treatment covariates on the instrument(s).\n",
    "\n",
    "        Returns a DataFrame with covariate names, coefficients on instrument, and p-values.\n",
    "        \"\"\"\n",
    "        assert self.data is not None, \"Call fit_instrument_oof first\"\n",
    "        results = []\n",
    "        Z = self.data[[self.oof_instrument_name]]\n",
    "        Z_with_const = sm.add_constant(Z, has_constant=\"add\")\n",
    "\n",
    "        for cov in covariates:\n",
    "            if cov not in self.data.columns:\n",
    "                raise ValueError(f\"covariate {cov} not in data\")\n",
    "            y = self.data[cov]\n",
    "            model = sm.OLS(y, Z_with_const).fit(cov_type=\"HC1\")\n",
    "            coef = float(model.params[self.oof_instrument_name])\n",
    "            pval = float(model.pvalues[self.oof_instrument_name])\n",
    "            results.append({\"covariate\": cov, \"coef_on_instrument\": coef, \"pvalue\": pval})\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Simulation utilities (for testing)\n",
    "    # ----------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_data(\n",
    "        n: int = 10000,\n",
    "        instrument_strength: float = 0.8,\n",
    "        endog_beta: float = 2.0,\n",
    "        seed: int = 123,\n",
    "        with_invalid_instrument: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simulate data with a valid (and optionally an invalid) instrument for test purposes.\n",
    "\n",
    "        Structural model:\n",
    "          Z ~ N(0,1)\n",
    "          X ~ N(0,1)\n",
    "          D = gamma*Z + delta*X + u1\n",
    "          Y = beta*D + theta*X + u2\n",
    "\n",
    "        If with_invalid_instrument is True, the instrument Z also directly affects Y (violating exclusion).\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(seed)\n",
    "        Z = rng.normal(size=n)\n",
    "        X = rng.normal(size=n)\n",
    "        u1 = rng.normal(scale=1.0, size=n)\n",
    "        u2 = rng.normal(scale=1.0, size=n)\n",
    "\n",
    "        gamma = instrument_strength\n",
    "        delta = 0.5\n",
    "        beta = endog_beta\n",
    "        theta = 1.0\n",
    "\n",
    "        D = gamma * Z + delta * X + u1\n",
    "        Y = beta * D + theta * X + u2\n",
    "        if with_invalid_instrument:\n",
    "            # direct effect of Z on Y\n",
    "            Y = Y + 0.5 * Z\n",
    "\n",
    "        df = pd.DataFrame({\"Z\": Z, \"X\": X, \"D\": D, \"Y\": Y})\n",
    "        return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example quick-run function\n",
    "# ----------------------------\n",
    "\n",
    "def quick_run_example():\n",
    "    \"\"\"\n",
    "    Run a short demonstration using simulated data to show the pipeline usage.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running quick demo of MLInstrumentIV on simulated data\")\n",
    "    df = MLInstrumentIV.simulate_data(n=2000, instrument_strength=0.9, seed=2025)\n",
    "\n",
    "    # We'll pretend Z is not directly available as an instrument but is used to predict D\n",
    "    # Build naive features: here we'll use X and Z (but in real use-case do not include direct causes of outcome)\n",
    "    features = [\"Z\", \"X\"]\n",
    "    df = df.rename(columns={\"D\": \"endog\", \"Y\": \"outcome\"})\n",
    "\n",
    "    # Create instance\n",
    "    instrument_pipeline = Pipeline([(\"model\", RandomForestRegressor(n_estimators=100, random_state=42))])\n",
    "    mliv = MLInstrumentIV(\n",
    "        instrument_pipeline=instrument_pipeline,\n",
    "        instrument_features=features,\n",
    "        outcome=\"outcome\",\n",
    "        endogenous=\"endog\",\n",
    "        controls=[\"X\"],\n",
    "        cv_folds=5,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    mliv.fit_instrument_oof(df)\n",
    "    ivres = mliv.estimate_iv(cov_type=\"robust\")\n",
    "\n",
    "    print(\"IV summary:\\n\", ivres.summary)\n",
    "    print(\"Partial F:\", ivres.partial_f)\n",
    "\n",
    "    # Balance test on pre-treatment covariate 'X' (should be zero correlation in this sim)\n",
    "    bt = mliv.balance_tests([\"X\"])  # should show insignificant association if instrument valid\n",
    "    print(bt)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# If run as a script\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    quick_run_example()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
