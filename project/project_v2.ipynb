{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670d640f",
   "metadata": {},
   "source": [
    "Previous file was getting too chunky. This one has just the finalized complete pipeline for the ml feature engineered thang for 2SLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_data(n=2000):\n",
    "    \"\"\"Generate synthetic data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Age': np.random.randint(18, 65, n),\n",
    "        'Gender': np.random.choice(['M', 'F'], n),\n",
    "        'Income': np.random.randint(30000, 150000, n),\n",
    "        'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
    "        'Ad_Type': np.random.choice(['Video', 'Banner', 'Native'], n),\n",
    "        'Ad_Topic': np.random.choice(['Tech', 'Fashion', 'Food', 'Travel'], n),\n",
    "        'Ad_Placement': np.random.choice(['Social_Media', 'Search', 'Display'], n),\n",
    "        'Click_Time': pd.date_range('2024-01-01', periods=n, freq='H'),\n",
    "    })\n",
    "    \n",
    "    # Normalize income to reasonable scale\n",
    "    data['Income'] = data['Income'] / 100000  # Scale to 0.3-1.5 range\n",
    "    \n",
    "    # Generate clicks with realistic structure\n",
    "    clicks_base = (\n",
    "        0.5 +  # baseline\n",
    "        0.3 * (data['Ad_Type'] == 'Video').astype(float) +\n",
    "        0.2 * (data['Ad_Placement'] == 'Social_Media').astype(float) +\n",
    "        0.01 * data['Age'] +\n",
    "        0.2 * data['Income'] +\n",
    "        np.random.randn(n) * 0.5\n",
    "    )\n",
    "    data['Clicks'] = np.clip(clicks_base, 0.1, 10)\n",
    "    \n",
    "    # Generate CTR (correlated with clicks but not in instrument)\n",
    "    data['CTR'] = data['Clicks'] * np.random.uniform(0.05, 0.15, n)\n",
    "    \n",
    "    # Generate conversion rate with causal effect from clicks\n",
    "    # Plus confounding through unobserved factors\n",
    "    unobserved_confounder = np.random.randn(n) * 0.05\n",
    "    \n",
    "    conversion_base = (\n",
    "        0.05 +  # baseline\n",
    "        0.08 * data['Clicks'] +  # TRUE CAUSAL EFFECT\n",
    "        0.02 * data['Income'] +\n",
    "        0.005 * data['Age'] +\n",
    "        0.3 * data['CTR'] +\n",
    "        unobserved_confounder +\n",
    "        np.random.randn(n) * 0.03\n",
    "    )\n",
    "    data['Conversion_Rate'] = np.clip(conversion_base, 0.01, 0.95)\n",
    "    \n",
    "    # Add endogeneity: unobserved confounder affects clicks too\n",
    "    data['Clicks'] = data['Clicks'] + unobserved_confounder * 2\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82af4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/project/Dataset_Ads.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1667\u001b[39m\n\u001b[32m   1661\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m analyzer_weak, analyzer_strong\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1665\u001b[39m \n\u001b[32m   1666\u001b[39m     \u001b[38;5;66;03m# reading in the df\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../datasets/project/Dataset_Ads.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;66;03m# # default setting with stacking, and enhanced machine learning.\u001b[39;00m\n\u001b[32m   1670\u001b[39m     \u001b[38;5;66;03m# analyzer = CausalAdAnalyzer(df)\u001b[39;00m\n\u001b[32m   1671\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1760\u001b[39m     \u001b[38;5;66;03m# comparison = analyzer.compare_value_added_methods(subgroup_var='Ad_Type')\u001b[39;00m\n\u001b[32m   1761\u001b[39m     \u001b[38;5;66;03m# Initialize and run basic pipeline\u001b[39;00m\n\u001b[32m   1762\u001b[39m     analyzer = CausalAdAnalyzer(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ianta\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ianta\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ianta\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ianta\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ianta\\anaconda3\\envs\\cuda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../datasets/project/Dataset_Ads.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instrumental Variables (IV) Causal Inference for Ad Conversion Analysis\n",
    "Using ML-Generated Instruments and Two-Stage Least Squares (2SLS)\n",
    "ENHANCED VERSION: Data cleaning, stronger instruments, and log transformations\n",
    "\"\"\"\n",
    "class CausalAdAnalyzer:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline for causal inference in ad conversion analysis\n",
    "    using ML-generated instrumental variables and 2SLS estimation.\n",
    "    \n",
    "    ENHANCED with:\n",
    "    - Robust data cleaning and preprocessing\n",
    "    - Logarithmic transformations for skewed variables\n",
    "    - Rich feature engineering for stronger instruments\n",
    "    - Stacking ensemble for maximum predictive power\n",
    "    - Comprehensive diagnostics including Stock-Yogo tests\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with your dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Must contain columns:\n",
    "            - Conversion_Rate (Y): dependent variable\n",
    "            - Clicks (D): endogenous regressor\n",
    "            - Age, Gender, Income, Location: demographics\n",
    "            - Ad_Type, Ad_Topic, Ad_Placement: ad features\n",
    "            - CTR: click-through rate\n",
    "            - Click_Time: timestamp for feature engineering\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.first_stage_model = None\n",
    "        self.first_stage_results = None\n",
    "        self.second_stage_results = None\n",
    "        \n",
    "        # Clean data on initialization\n",
    "        self._clean_data()\n",
    "        \n",
    "    def _clean_data(self):\n",
    "        \"\"\"\n",
    "        Clean and preprocess data before analysis.\n",
    "        \n",
    "        Performs:\n",
    "        1. Handle negative income values\n",
    "        2. Impute missing income with median\n",
    "        3. Winsorize income at 1st and 99th percentiles\n",
    "        4. Filter age to plausible range (10-90 years)\n",
    "        5. Create logarithmic transformations for skewed variables\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        initial_rows = len(self.data)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. CLEAN INCOME\n",
    "        # =====================================================================\n",
    "        if 'Income' in self.data.columns:\n",
    "            # Convert negative income to missing\n",
    "            neg_income_count = (self.data['Income'] < 0).sum()\n",
    "            self.data.loc[self.data['Income'] < 0, 'Income'] = np.nan\n",
    "            \n",
    "            if neg_income_count > 0:\n",
    "                print(f\"‚úì Converted {neg_income_count} negative income values to missing\")\n",
    "            \n",
    "            # Impute missing income with median\n",
    "            missing_income = self.data['Income'].isna().sum()\n",
    "            if missing_income > 0:\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                self.data['Income'] = imputer.fit_transform(self.data[['Income']])\n",
    "                print(f\"‚úì Imputed {missing_income} missing income values with median\")\n",
    "            \n",
    "            # Winsorize: Cap extremes at 1st and 99th percentile\n",
    "            lower, upper = self.data['Income'].quantile([0.01, 0.99])\n",
    "            income_before = self.data['Income'].copy()\n",
    "            self.data['Income'] = self.data['Income'].clip(lower, upper)\n",
    "            winsorized = (income_before != self.data['Income']).sum()\n",
    "            print(f\"‚úì Winsorized {winsorized} income values at 1st/99th percentiles\")\n",
    "            print(f\"  Income range: [{lower:,.0f}, {upper:,.0f}]\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. FILTER AGE\n",
    "        # =====================================================================\n",
    "        if 'Age' in self.data.columns:\n",
    "            age_before = len(self.data)\n",
    "            self.data = self.data[self.data['Age'].between(10, 90)]\n",
    "            age_filtered = age_before - len(self.data)\n",
    "            if age_filtered > 0:\n",
    "                print(f\"‚úì Filtered {age_filtered} rows with implausible ages (keeping 10-90)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. CREATE LOGARITHMIC TRANSFORMATIONS\n",
    "        # =====================================================================\n",
    "        print(f\"\\nüìä Creating logarithmic transformations:\")\n",
    "        \n",
    "        # Log of Income (if positive)\n",
    "        if 'Income' in self.data.columns:\n",
    "            self.data['Income_log'] = np.log1p(self.data['Income'])\n",
    "            print(f\"  ‚úì Income_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of Clicks (if exists and positive)\n",
    "        if 'Clicks' in self.data.columns:\n",
    "            self.data['Clicks_log'] = np.log1p(self.data['Clicks'])\n",
    "            print(f\"  ‚úì Clicks_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of Age (for nonlinear age effects)\n",
    "        if 'Age' in self.data.columns:\n",
    "            self.data['Age_log'] = np.log1p(self.data['Age'])\n",
    "            print(f\"  ‚úì Age_log created (log1p transformation)\")\n",
    "        \n",
    "        # Log of CTR (if exists and positive)\n",
    "        if 'CTR' in self.data.columns:\n",
    "            # Ensure CTR is positive before log\n",
    "            if (self.data['CTR'] > 0).all():\n",
    "                self.data['CTR_log'] = np.log(self.data['CTR'])\n",
    "                print(f\"  ‚úì CTR_log created (log transformation)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # SUMMARY\n",
    "        # =====================================================================\n",
    "        final_rows = len(self.data)\n",
    "        rows_removed = initial_rows - final_rows\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLEANING SUMMARY:\")\n",
    "        print(f\"  Initial rows:        {initial_rows:,}\")\n",
    "        print(f\"  Final rows:          {final_rows:,}\")\n",
    "        print(f\"  Rows removed:        {rows_removed:,} ({rows_removed/initial_rows*100:.1f}%)\")\n",
    "        print(f\"  Log variables added: {len([col for col in self.data.columns if '_log' in col])}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def engineer_time_features(self):\n",
    "        \"\"\"Extract day of week and hour from Click_Time\"\"\"\n",
    "        if 'Click_Time' in self.data.columns:\n",
    "            self.data['Click_Time'] = pd.to_datetime(self.data['Click_Time'])\n",
    "            self.data['Day_of_Week'] = self.data['Click_Time'].dt.dayofweek\n",
    "            self.data['Hour'] = self.data['Click_Time'].dt.hour\n",
    "        return self\n",
    "    \n",
    "    def encode_categorical_features(self):\n",
    "        \"\"\"Encode categorical variables\"\"\"\n",
    "        categorical_cols = ['Gender', 'Location', 'Ad_Type', 'Ad_Topic', 'Ad_Placement']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in self.data.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.data[f'{col}_encoded'] = le.fit_transform(self.data[col].astype(str))\n",
    "                self.encoders[col] = le\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def engineer_instrument_features(self):\n",
    "        \"\"\"\n",
    "        ENHANCED: Create rich features that predict clicks but don't directly affect conversions.\n",
    "        \n",
    "        This is crucial for instrument strength. We create:\n",
    "        1. Interaction features between ad characteristics and demographics\n",
    "        2. Time-based features (weekend, business hours)\n",
    "        3. Nonlinear transformations\n",
    "        4. Complex interactions between multiple variables\n",
    "        \n",
    "        Key principle: These features should predict CLICKS well, but only affect\n",
    "        CONVERSIONS through clicks (exclusion restriction).\n",
    "        \"\"\"\n",
    "        df = self.data\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE ENGINEERING FOR INSTRUMENT STRENGTH\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. AD CHARACTERISTICS √ó DEMOGRAPHICS INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Different demographics respond differently to ad types\n",
    "        \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Type_encoded']):\n",
    "            df['Income_x_AdType'] = df['Income'] * df['Ad_Type_encoded']\n",
    "            print(\"‚úì Created Income √ó Ad Type interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Topic_encoded']):\n",
    "            df['Age_x_AdTopic'] = df['Age'] * df['Ad_Topic_encoded']\n",
    "            print(\"‚úì Created Age √ó Ad Topic interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded']):\n",
    "            df['Income_x_Placement'] = df['Income'] * df['Ad_Placement_encoded']\n",
    "            print(\"‚úì Created Income √ó Ad Placement interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Placement_encoded']):\n",
    "            df['Age_x_Placement'] = df['Age'] * df['Ad_Placement_encoded']\n",
    "            print(\"‚úì Created Age √ó Ad Placement interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. TIME-BASED FEATURES AND INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Click patterns vary by time of day/week\n",
    "        \n",
    "        if 'Day_of_Week' in df.columns:\n",
    "            df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "            print(\"‚úì Created Weekend indicator\")\n",
    "            \n",
    "        if 'Hour' in df.columns:\n",
    "            df['BusinessHours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "            df['Evening'] = ((df['Hour'] >= 18) & (df['Hour'] <= 23)).astype(int)\n",
    "            df['Morning'] = ((df['Hour'] >= 6) & (df['Hour'] <= 11)).astype(int)\n",
    "            print(\"‚úì Created time-of-day indicators\")\n",
    "        \n",
    "        # Time √ó Ad interactions\n",
    "        if all(col in df.columns for col in ['Weekend', 'Ad_Type_encoded']):\n",
    "            df['Weekend_x_AdType'] = df['Weekend'] * df['Ad_Type_encoded']\n",
    "            print(\"‚úì Created Weekend √ó Ad Type interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['BusinessHours', 'Ad_Placement_encoded']):\n",
    "            df['BusinessHours_x_Placement'] = df['BusinessHours'] * df['Ad_Placement_encoded']\n",
    "            print(\"‚úì Created Business Hours √ó Ad Placement interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Evening', 'Ad_Topic_encoded']):\n",
    "            df['Evening_x_AdTopic'] = df['Evening'] * df['Ad_Topic_encoded']\n",
    "            print(\"‚úì Created Evening √ó Ad Topic interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. DEMOGRAPHICS √ó TIME INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Different demographics have different browsing patterns\n",
    "        \n",
    "        if all(col in df.columns for col in ['Age', 'Hour']):\n",
    "            df['Age_x_Hour'] = df['Age'] * df['Hour']\n",
    "            print(\"‚úì Created Age √ó Hour interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Weekend']):\n",
    "            df['Income_x_Weekend'] = df['Income'] * df['Weekend']\n",
    "            print(\"‚úì Created Income √ó Weekend interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Age', 'BusinessHours']):\n",
    "            df['Age_x_BusinessHours'] = df['Age'] * df['BusinessHours']\n",
    "            print(\"‚úì Created Age √ó Business Hours interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 4. NONLINEAR TRANSFORMATIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Relationships may be nonlinear (using log-transformed versions)\n",
    "        \n",
    "        if 'Age_log' in df.columns:\n",
    "            df['Age_squared'] = df['Age'] ** 2\n",
    "            print(\"‚úì Created Age squared\")\n",
    "            \n",
    "        if 'Income_log' in df.columns:\n",
    "            df['Income_squared'] = df['Income'] ** 2\n",
    "            df['Income_sqrt'] = np.sqrt(df['Income'].clip(lower=0))\n",
    "            print(\"‚úì Created Income squared and sqrt\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 5. COMPLEX CATEGORICAL INTERACTIONS\n",
    "        # =====================================================================\n",
    "        # Rationale: Certain combinations may be particularly predictive\n",
    "        \n",
    "        # Location √ó Demographics\n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Age']):\n",
    "            df['Location_x_Age'] = df['Location_encoded'] * df['Age']\n",
    "            print(\"‚úì Created Location √ó Age interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Income']):\n",
    "            df['Location_x_Income'] = df['Location_encoded'] * df['Income']\n",
    "            print(\"‚úì Created Location √ó Income interaction\")\n",
    "        \n",
    "        # Location √ó Ad characteristics\n",
    "        if all(col in df.columns for col in ['Location_encoded', 'Ad_Placement_encoded']):\n",
    "            df['Location_x_Placement'] = df['Location_encoded'] * df['Ad_Placement_encoded']\n",
    "            print(\"‚úì Created Location √ó Placement interaction\")\n",
    "        \n",
    "        # Gender √ó Ad characteristics\n",
    "        if all(col in df.columns for col in ['Gender_encoded', 'Ad_Topic_encoded']):\n",
    "            df['Gender_x_AdTopic'] = df['Gender_encoded'] * df['Ad_Topic_encoded']\n",
    "            print(\"‚úì Created Gender √ó Ad Topic interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Gender_encoded', 'Ad_Type_encoded']):\n",
    "            df['Gender_x_AdType'] = df['Gender_encoded'] * df['Ad_Type_encoded']\n",
    "            print(\"‚úì Created Gender √ó Ad Type interaction\")\n",
    "        \n",
    "        # Ad Type √ó Placement (different placements work for different types)\n",
    "        if all(col in df.columns for col in ['Ad_Type_encoded', 'Ad_Placement_encoded']):\n",
    "            df['AdType_x_Placement'] = df['Ad_Type_encoded'] * df['Ad_Placement_encoded']\n",
    "            print(\"‚úì Created Ad Type √ó Placement interaction\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 6. THREE-WAY INTERACTIONS (most powerful)\n",
    "        # =====================================================================\n",
    "        # Rationale: Capture complex patterns\n",
    "        \n",
    "        if all(col in df.columns for col in ['Age', 'Ad_Type_encoded', 'Weekend']):\n",
    "            df['Age_x_AdType_x_Weekend'] = df['Age'] * df['Ad_Type_encoded'] * df['Weekend']\n",
    "            print(\"‚úì Created Age √ó Ad Type √ó Weekend interaction\")\n",
    "            \n",
    "        if all(col in df.columns for col in ['Income', 'Ad_Placement_encoded', 'BusinessHours']):\n",
    "            df['Income_x_Placement_x_BizHours'] = df['Income'] * df['Ad_Placement_encoded'] * df['BusinessHours']\n",
    "            print(\"‚úì Created Income √ó Placement √ó Business Hours interaction\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def create_ml_instrument(self, model_type='stacking', cv_folds=5, use_enhanced_features=True):\n",
    "        \"\"\"\n",
    "        ENHANCED: Generate ML-based instrument for Clicks (D) using ensemble methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            'rf' for Random Forest\n",
    "            'gb' for Gradient Boosting\n",
    "            'stacking' for Stacking Ensemble (RECOMMENDED for strongest instruments)\n",
    "        cv_folds : int\n",
    "            Number of cross-validation folds\n",
    "        use_enhanced_features : bool\n",
    "            Whether to use enhanced feature engineering (recommended)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply enhanced feature engineering if requested\n",
    "        if use_enhanced_features:\n",
    "            self.engineer_instrument_features()\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DEFINE INSTRUMENT FEATURES\n",
    "        # =====================================================================\n",
    "        # Base features (always included)\n",
    "        base_features = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'Day_of_Week', 'Hour'\n",
    "        ]\n",
    "        \n",
    "        # Enhanced features (only if engineered)\n",
    "        enhanced_features = [\n",
    "            # Interactions\n",
    "            'Income_x_AdType', 'Age_x_AdTopic', 'Income_x_Placement', 'Age_x_Placement',\n",
    "            'Weekend_x_AdType', 'BusinessHours_x_Placement', 'Evening_x_AdTopic',\n",
    "            'Age_x_Hour', 'Income_x_Weekend', 'Age_x_BusinessHours',\n",
    "            'Location_x_Age', 'Location_x_Income', 'Location_x_Placement',\n",
    "            'Gender_x_AdTopic', 'Gender_x_AdType', 'AdType_x_Placement',\n",
    "            'Age_x_AdType_x_Weekend', 'Income_x_Placement_x_BizHours',\n",
    "            # Time features\n",
    "            'Weekend', 'BusinessHours', 'Evening', 'Morning',\n",
    "            # Nonlinear (now using cleaned log versions)\n",
    "            'Age_squared', 'Age_log', 'Income_log', 'Income_squared', 'Income_sqrt',\n",
    "            'Clicks_log', 'CTR_log'\n",
    "        ]\n",
    "        \n",
    "        # Combine and filter available features\n",
    "        if use_enhanced_features:\n",
    "            instrument_features = base_features + enhanced_features\n",
    "        else:\n",
    "            instrument_features = base_features\n",
    "            \n",
    "        available_features = [f for f in instrument_features if f in self.data.columns]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ML INSTRUMENT CONSTRUCTION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total features available: {len(available_features)}\")\n",
    "        print(f\"Model type: {model_type.upper()}\")\n",
    "        print(f\"Cross-validation folds: {cv_folds}\")\n",
    "\n",
    "        # Prepare data\n",
    "        X_instrument = self.data[available_features]\n",
    "        y_clicks = self.data['Clicks']\n",
    "        \n",
    "        # Standardize features\n",
    "        X_instrument_scaled = self.scaler.fit_transform(X_instrument)\n",
    "        X_instrument_scaled = pd.DataFrame(\n",
    "            X_instrument_scaled, \n",
    "            columns=available_features,\n",
    "            index=X_instrument.index\n",
    "        )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # FEATURE SELECTION\n",
    "        # =====================================================================\n",
    "        # Optional: Reduce to top N features based on importance\n",
    "        top_n = min(10, len(available_features))\n",
    "        print(f\"\\nSelecting top {top_n} features based on model importance...\")\n",
    "        \n",
    "        # Use a simple model to rank features (e.g., Random Forest)\n",
    "        feature_selector = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        feature_selector.fit(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        # Get top N features\n",
    "        importances = pd.Series(feature_selector.feature_importances_, index=X_instrument_scaled.columns)\n",
    "        top_features = importances.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "        \n",
    "        print(\"Top features selected:\")\n",
    "        for i, feat in enumerate(top_features, 1):\n",
    "            print(f\"{i}. {feat}\")\n",
    "        \n",
    "        # Filter scaled data to top features\n",
    "        X_instrument_scaled = X_instrument_scaled[top_features]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # BUILD MODEL\n",
    "        # =====================================================================\n",
    "        \n",
    "        if model_type == 'stacking':\n",
    "            print(\"\\nBuilding Stacking Ensemble (strongest option)...\")\n",
    "            \n",
    "            # Define base learners with more aggressive parameters\n",
    "            base_models = [\n",
    "                ('rf', RandomForestRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=15,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    max_features='sqrt',\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )),\n",
    "                ('gb', GradientBoostingRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ]\n",
    "            \n",
    "            # Try to import XGBoost if available\n",
    "            try:\n",
    "                from xgboost import XGBRegressor\n",
    "                base_models.append(\n",
    "                    ('xgb', XGBRegressor(\n",
    "                        n_estimators=200,\n",
    "                        max_depth=8,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    ))\n",
    "                )\n",
    "                print(\"  ‚úì Using XGBoost as additional base learner\")\n",
    "            except ImportError:\n",
    "                print(\"  ‚Ñπ XGBoost not available, using RF + GB only\")\n",
    "            \n",
    "            # Create stacking ensemble\n",
    "            self.first_stage_model = StackingRegressor(\n",
    "                estimators=base_models,\n",
    "                final_estimator=Ridge(alpha=1.0),\n",
    "                cv=cv_folds,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            print(\"\\nBuilding Random Forest...\")\n",
    "            self.first_stage_model = RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                max_features='sqrt',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'gb':\n",
    "            print(\"\\nBuilding Gradient Boosting...\")\n",
    "            self.first_stage_model = GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=7,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # GENERATE OUT-OF-FOLD PREDICTIONS\n",
    "        # =====================================================================\n",
    "        print(f\"\\nGenerating out-of-fold predictions (CV={cv_folds})...\")\n",
    "        \n",
    "        self.data['Clicks_predicted'] = cross_val_predict(\n",
    "            self.first_stage_model,\n",
    "            X_instrument_scaled,\n",
    "            y_clicks,\n",
    "            cv=cv_folds,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit final model for interpretation\n",
    "        print(\"Fitting final model...\")\n",
    "        self.first_stage_model.fit(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DIAGNOSTICS\n",
    "        # =====================================================================\n",
    "        self._enhanced_instrument_diagnostics(X_instrument_scaled, y_clicks)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _enhanced_instrument_diagnostics(self, X, y):\n",
    "        \"\"\"\n",
    "        ENHANCED: Comprehensive instrument strength testing with Stock-Yogo critical values.\n",
    "        \"\"\"\n",
    "        z = self.data['Clicks_predicted'].values\n",
    "        d = self.data['Clicks'].values\n",
    "        \n",
    "        n = len(d)\n",
    "        k = X.shape[1]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. FIRST-STAGE R-SQUARED AND F-STATISTIC\n",
    "        # =====================================================================\n",
    "        z_resid = z - z.mean()\n",
    "        d_resid = d - d.mean()\n",
    "        \n",
    "        ss_tot = np.sum(d_resid**2)\n",
    "        ss_res = np.sum((d - z)**2)\n",
    "        r_squared = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Proper F-statistic for first stage\n",
    "        f_stat = (r_squared / 1) / ((1 - r_squared) / (n - k - 1))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 2. CORRELATION\n",
    "        # =====================================================================\n",
    "        corr = np.corrcoef(z, d)[0, 1]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 3. CRAGG-DONALD STATISTIC\n",
    "        # =====================================================================\n",
    "        cragg_donald = n * r_squared\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DISPLAY RESULTS\n",
    "        # =====================================================================\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ENHANCED INSTRUMENT STRENGTH DIAGNOSTICS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nSAMPLE INFORMATION:\")\n",
    "        print(f\"  Sample size (n):              {n:,}\")\n",
    "        print(f\"  Number of features (k):       {k}\")\n",
    "        print(f\"\\nFIRST-STAGE PERFORMANCE:\")\n",
    "        print(f\"  R-squared:                    {r_squared:.4f}\")\n",
    "        print(f\"  Correlation (Z, D):           {corr:.4f}\")\n",
    "        print(f\"  F-statistic:                  {f_stat:.2f}\")\n",
    "        print(f\"  Cragg-Donald statistic:       {cragg_donald:.2f}\")\n",
    "        \n",
    "        print(f\"\\nBENCHMARKS & INTERPRETATION:\")\n",
    "        print(f\"  {'Criterion':<35} {'Threshold':<12} {'Status'}\")\n",
    "        print(f\"  {'-'*35} {'-'*12} {'-'*20}\")\n",
    "        \n",
    "        # Weak instrument test\n",
    "        weak_status = \"‚úì STRONG\" if f_stat > 10 else \"‚úó WEAK\"\n",
    "        print(f\"  {'Weak Instrument (F < 10)':<35} {'10.00':<12} {weak_status}\")\n",
    "        \n",
    "        # Stock-Yogo critical values (for single instrument, single endogenous variable)\n",
    "        sy_10_status = \"‚úì‚úì EXCELLENT\" if f_stat > 16.38 else \"‚úó Below threshold\"\n",
    "        sy_15_status = \"‚úì GOOD\" if f_stat > 8.96 else \"‚úó Below threshold\"\n",
    "        \n",
    "        print(f\"  {'Stock-Yogo 10% max bias':<35} {'16.38':<12} {sy_10_status}\")\n",
    "        print(f\"  {'Stock-Yogo 15% max bias':<35} {'8.96':<12} {sy_15_status}\")\n",
    "        \n",
    "        print(f\"\\nOVERALL ASSESSMENT:\")\n",
    "        if f_stat > 16.38:\n",
    "            print(f\"  ‚úì‚úì VERY STRONG INSTRUMENT\")\n",
    "            print(f\"     Maximum IV bias < 10% of OLS bias\")\n",
    "            print(f\"     Highly reliable causal inference\")\n",
    "        elif f_stat > 10:\n",
    "            print(f\"  ‚úì STRONG INSTRUMENT\")\n",
    "            print(f\"     Acceptable for causal inference\")\n",
    "            print(f\"     Results should be reliable\")\n",
    "        elif f_stat > 5:\n",
    "            print(f\"  ‚ö† MODERATELY WEAK INSTRUMENT\")\n",
    "            print(f\"     Proceed with caution\")\n",
    "            print(f\"     Consider sensitivity analysis\")\n",
    "        else:\n",
    "            print(f\"  ‚úó WEAK INSTRUMENT\")\n",
    "            print(f\"     Results may be unreliable\")\n",
    "            print(f\"     Consider alternative identification strategies\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # FEATURE IMPORTANCE (if available)\n",
    "        # =====================================================================\n",
    "        if hasattr(self.first_stage_model, 'feature_importances_'):\n",
    "            print(f\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING CLICKS:\")\n",
    "            importances = self.first_stage_model.feature_importances_\n",
    "            top_features = sorted(\n",
    "                zip(X.columns, importances), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:10]\n",
    "            \n",
    "            for i, (feat, imp) in enumerate(top_features, 1):\n",
    "                print(f\"  {i:2d}. {feat:35s} {imp:.4f}\")\n",
    "        \n",
    "        elif hasattr(self.first_stage_model, 'final_estimator_'):\n",
    "            print(f\"\\n‚Ñπ Stacking ensemble used - individual feature importances\")\n",
    "            print(f\"  not directly available, but all base models contribute\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def run_2sls(self, include_interactions=False):\n",
    "        \"\"\"\n",
    "        Step 3: Two-Stage Least Squares Estimation\n",
    "        \n",
    "        First Stage: D = œÄ‚ÇÄ + œÄ‚ÇÅZ + œÄ‚ÇÇX + ŒΩ\n",
    "        Second Stage: Y = Œ± + Œ≤DÃÇ + Œ≥X + Œµ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        include_interactions : bool\n",
    "            Whether to include Ad_Type √ó Ad_Placement interactions\n",
    "        \"\"\"\n",
    "        # Exogenous controls (X)\n",
    "        exog_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'CTR'\n",
    "        ]\n",
    "        \n",
    "        available_controls = [f for f in exog_controls if f in self.data.columns]\n",
    "        \n",
    "        # Add interaction terms if requested\n",
    "        if include_interactions:\n",
    "            if 'Ad_Type_encoded' in self.data.columns and 'Ad_Placement_encoded' in self.data.columns:\n",
    "                self.data['Ad_Type_x_Placement'] = (\n",
    "                    self.data['Ad_Type_encoded'] * self.data['Ad_Placement_encoded']\n",
    "                )\n",
    "                available_controls.append('Ad_Type_x_Placement')\n",
    "        \n",
    "        print('2sls data summar: ', self.data.describe(include='all'))\n",
    "\n",
    "        # FIRST STAGE: Regress D on Z and X\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FIRST STAGE REGRESSION: D ~ Z + X\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print('available controls: ', available_controls)\n",
    "\n",
    "        X_first_stage = sm.add_constant(pd.concat([\n",
    "            self.data[['Clicks_predicted']],\n",
    "            self.data[available_controls]\n",
    "        ], axis=1))\n",
    "        \n",
    "        y_first_stage = self.data['Clicks']\n",
    "        \n",
    "        self.first_stage_results = sm.OLS(y_first_stage, X_first_stage).fit()\n",
    "        \n",
    "        print(\"\\nFirst Stage Summary:\")\n",
    "        print(f\"R-squared: {self.first_stage_results.rsquared:.4f}\")\n",
    "        print(f\"F-statistic: {self.first_stage_results.fvalue:.2f}\")\n",
    "        print(f\"Instrument coefficient: {self.first_stage_results.params['Clicks_predicted']:.4f}\")\n",
    "        print(f\"Instrument p-value: {self.first_stage_results.pvalues['Clicks_predicted']:.4f}\")\n",
    "        \n",
    "        # Get fitted values from first stage\n",
    "        D_hat = self.first_stage_results.fittedvalues\n",
    "        \n",
    "        # SECOND STAGE: Regress Y on D_hat and X\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SECOND STAGE REGRESSION: Y ~ DÃÇ + X\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        X_second_stage = sm.add_constant(pd.concat([\n",
    "            pd.Series(D_hat, name='Clicks_fitted'),\n",
    "            self.data[available_controls]\n",
    "        ], axis=1))\n",
    "        \n",
    "        y_second_stage = self.data['Conversion_Rate']\n",
    "        \n",
    "        self.second_stage_results = sm.OLS(y_second_stage, X_second_stage).fit()\n",
    "        \n",
    "        # Manual calculation of correct standard errors for 2SLS\n",
    "        self._calculate_2sls_standard_errors(available_controls)\n",
    "        \n",
    "        self._display_results()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    ### stratified 2sls for subgroup effects\n",
    "    def analyze_subgroup_effects(self, subgroup_vars=None, min_subgroup_size=100):\n",
    "        \"\"\"\n",
    "        Stratified 2SLS: Run separate 2SLS regressions within subgroups to identify\n",
    "        heterogeneous treatment effects.\n",
    "\n",
    "        This helps explain why average effects may be weak - effects may be strong\n",
    "        in specific segments but cancel out in aggregate.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        subgroup_vars : list of str or dict, optional\n",
    "            Variables to stratify by. Can be:\n",
    "            - List of column names (will auto-create bins for continuous vars)\n",
    "            - Dict mapping column names to bin specifications\n",
    "            Example: ['Location', 'Ad_Type'] or \n",
    "                     {'Income': [0, 30000, 60000, np.inf], 'Age': [0, 35, 50, 65, np.inf]}\n",
    "        min_subgroup_size : int\n",
    "            Minimum observations required per subgroup (for statistical power)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        results_df : pd.DataFrame\n",
    "            Subgroup-specific causal effects with diagnostics\n",
    "        \"\"\"\n",
    "\n",
    "        if subgroup_vars is None:\n",
    "            # Default subgroups based on theory\n",
    "            subgroup_vars = {\n",
    "                'Income': [0, 30000, 50000, 70000, np.inf],  # Quartile-like bins\n",
    "                'Age': [0, 35, 50, 65, np.inf],              # Life stage bins\n",
    "                'Location': None,                             # Use as-is (categorical)\n",
    "                'Ad_Type': None                               # Use as-is (categorical)\n",
    "            }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STRATIFIED 2SLS: HETEROGENEOUS TREATMENT EFFECTS ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Store results for each subgroup\n",
    "        all_results = []\n",
    "\n",
    "        # Exogenous controls for 2SLS\n",
    "        exog_controls = [\n",
    "            'Age', 'Income',\n",
    "            'Gender_encoded', 'Location_encoded',\n",
    "            'Ad_Type_encoded', 'Ad_Topic_encoded', 'Ad_Placement_encoded',\n",
    "            'CTR'\n",
    "        ]\n",
    "        available_controls = [f for f in exog_controls if f in self.data.columns]\n",
    "\n",
    "        # =========================================================================\n",
    "        # PROCESS EACH SUBGROUP VARIABLE\n",
    "        # =========================================================================\n",
    "\n",
    "        for var in (subgroup_vars if isinstance(subgroup_vars, list) else subgroup_vars.keys()):\n",
    "\n",
    "            print(f\"\\n{'‚îÄ'*70}\")\n",
    "            print(f\"ANALYZING SUBGROUPS BY: {var.upper()}\")\n",
    "            print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "            # Create subgroups\n",
    "            if isinstance(subgroup_vars, dict) and subgroup_vars.get(var) is not None:\n",
    "                # Continuous variable with specified bins\n",
    "                bins = subgroup_vars[var]\n",
    "                labels = [f\"{var}_{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]\n",
    "                self.data[f'{var}_subgroup'] = pd.cut(\n",
    "                    self.data[var], \n",
    "                    bins=bins, \n",
    "                    labels=labels,\n",
    "                    include_lowest=True\n",
    "                )\n",
    "                subgroup_col = f'{var}_subgroup'\n",
    "            else:\n",
    "                # Categorical variable - use as is\n",
    "                subgroup_col = var\n",
    "\n",
    "            # Get unique subgroups\n",
    "            subgroups = self.data[subgroup_col].dropna().unique()\n",
    "\n",
    "            print(f\"\\nFound {len(subgroups)} subgroups: {sorted([str(s) for s in subgroups])}\")\n",
    "\n",
    "            # =====================================================================\n",
    "            # RUN 2SLS FOR EACH SUBGROUP\n",
    "            # =====================================================================\n",
    "\n",
    "            for subgroup in subgroups:\n",
    "\n",
    "                # Filter data to subgroup\n",
    "                subgroup_data = self.data[self.data[subgroup_col] == subgroup].copy()\n",
    "                n_obs = len(subgroup_data)\n",
    "\n",
    "                # Skip if too small\n",
    "                if n_obs < min_subgroup_size:\n",
    "                    print(f\"\\n  ‚ö† Skipping '{subgroup}': Only {n_obs} observations (min={min_subgroup_size})\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n  üìä Subgroup: '{subgroup}' (N={n_obs:,})\")\n",
    "\n",
    "                # Check if we have the instrument in this subgroup\n",
    "                if 'Clicks_predicted' not in subgroup_data.columns:\n",
    "                    print(f\"     ‚úó No instrument available\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                    # FIRST STAGE: D ~ Z + X (within subgroup)\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "                    X_first = sm.add_constant(pd.concat([\n",
    "                        subgroup_data[['Clicks_predicted']],\n",
    "                        subgroup_data[available_controls]\n",
    "                    ], axis=1))\n",
    "\n",
    "                    y_first = subgroup_data['Clicks']\n",
    "\n",
    "                    first_stage = sm.OLS(y_first, X_first).fit()\n",
    "\n",
    "                    # First stage diagnostics\n",
    "                    f_stat_first = first_stage.fvalue\n",
    "                    r2_first = first_stage.rsquared\n",
    "                    instrument_coef = first_stage.params['Clicks_predicted']\n",
    "                    instrument_pval = first_stage.pvalues['Clicks_predicted']\n",
    "\n",
    "                    # Weak instrument check\n",
    "                    is_weak = f_stat_first < 10\n",
    "\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                    # SECOND STAGE: Y ~ DÃÇ + X (within subgroup)\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "                    D_hat = first_stage.fittedvalues\n",
    "\n",
    "                    X_second = sm.add_constant(pd.concat([\n",
    "                        pd.Series(D_hat, name='Clicks_fitted'),\n",
    "                        subgroup_data[available_controls]\n",
    "                    ], axis=1))\n",
    "\n",
    "                    y_second = subgroup_data['Conversion_Rate']\n",
    "\n",
    "                    second_stage = sm.OLS(y_second, X_second).fit()\n",
    "\n",
    "                    # Extract causal effect\n",
    "                    causal_effect = second_stage.params['Clicks_fitted']\n",
    "                    se = second_stage.bse['Clicks_fitted']\n",
    "                    tstat = second_stage.tvalues['Clicks_fitted']\n",
    "                    pval = second_stage.pvalues['Clicks_fitted']\n",
    "                    ci_lower = causal_effect - 1.96 * se\n",
    "                    ci_upper = causal_effect + 1.96 * se\n",
    "\n",
    "                    # Statistical significance\n",
    "                    is_significant = pval < 0.05\n",
    "\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                    # DISPLAY SUBGROUP RESULTS\n",
    "                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "                    print(f\"     First Stage:\")\n",
    "                    print(f\"       F-stat: {f_stat_first:.2f} {'‚úó WEAK' if is_weak else '‚úì STRONG'}\")\n",
    "                    print(f\"       R¬≤: {r2_first:.4f}\")\n",
    "\n",
    "                    print(f\"     Second Stage (Causal Effect):\")\n",
    "                    print(f\"       Œ≤: {causal_effect:.6f}\")\n",
    "                    print(f\"       SE: {se:.6f}\")\n",
    "                    print(f\"       95% CI: [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "                    print(f\"       p-value: {pval:.4f} {'‚úì SIGNIFICANT' if is_significant else '‚úó Not significant'}\")\n",
    "\n",
    "                    # Store results\n",
    "                    all_results.append({\n",
    "                        'Variable': var,\n",
    "                        'Subgroup': str(subgroup),\n",
    "                        'N': n_obs,\n",
    "                        'First_Stage_F': f_stat_first,\n",
    "                        'First_Stage_R2': r2_first,\n",
    "                        'Instrument_Weak': is_weak,\n",
    "                        'Causal_Effect_Beta': causal_effect,\n",
    "                        'Std_Error': se,\n",
    "                        'T_Statistic': tstat,\n",
    "                        'P_Value': pval,\n",
    "                        'CI_Lower': ci_lower,\n",
    "                        'CI_Upper': ci_upper,\n",
    "                        'Significant': is_significant\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚úó Error: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "        # =========================================================================\n",
    "        # SUMMARY TABLE\n",
    "        # =========================================================================\n",
    "\n",
    "        if len(all_results) == 0:\n",
    "            print(\"\\n‚ö† No subgroups analyzed successfully\")\n",
    "            return None\n",
    "\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SUBGROUP EFFECTS SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Sort by absolute effect size\n",
    "        results_df['Abs_Effect'] = results_df['Causal_Effect_Beta'].abs()\n",
    "        results_df = results_df.sort_values('Abs_Effect', ascending=False)\n",
    "\n",
    "        # Display table\n",
    "        display_cols = [\n",
    "            'Variable', 'Subgroup', 'N', \n",
    "            'Causal_Effect_Beta', 'P_Value', 'Significant',\n",
    "            'First_Stage_F', 'Instrument_Weak'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n\" + results_df[display_cols].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # KEY INSIGHTS\n",
    "        # =========================================================================\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEY INSIGHTS\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Find strongest effects\n",
    "        significant_effects = results_df[results_df['Significant'] == True]\n",
    "\n",
    "        if len(significant_effects) > 0:\n",
    "            print(f\"\\n‚úì Found {len(significant_effects)} subgroups with SIGNIFICANT causal effects:\")\n",
    "\n",
    "            for _, row in significant_effects.head(5).iterrows():\n",
    "                print(f\"\\n  ‚Ä¢ {row['Variable']} = '{row['Subgroup']}':\")\n",
    "                print(f\"    - Causal effect: {row['Causal_Effect_Beta']:.6f}\")\n",
    "                print(f\"    - 95% CI: [{row['CI_Lower']:.6f}, {row['CI_Upper']:.6f}]\")\n",
    "                print(f\"    - p-value: {row['P_Value']:.4f}\")\n",
    "                print(f\"    - Sample size: {row['N']:,}\")\n",
    "        else:\n",
    "            print(\"\\n‚úó No subgroups with statistically significant effects found\")\n",
    "\n",
    "        # Check for weak instruments in subgroups\n",
    "        weak_instruments = results_df[results_df['Instrument_Weak'] == True]\n",
    "        if len(weak_instruments) > 0:\n",
    "            print(f\"\\n‚ö† Warning: {len(weak_instruments)} subgroups have weak instruments (F < 10)\")\n",
    "            print(\"  Consider these results with caution\")\n",
    "\n",
    "        # Effect heterogeneity\n",
    "        effect_range = results_df['Causal_Effect_Beta'].max() - results_df['Causal_Effect_Beta'].min()\n",
    "        print(f\"\\nüìä Effect Heterogeneity:\")\n",
    "        print(f\"  Range: {effect_range:.6f}\")\n",
    "        print(f\"  Max effect: {results_df['Causal_Effect_Beta'].max():.6f} ({results_df.loc[results_df['Causal_Effect_Beta'].idxmax(), 'Subgroup']})\")\n",
    "        print(f\"  Min effect: {results_df['Causal_Effect_Beta'].min():.6f} ({results_df.loc[results_df['Causal_Effect_Beta'].idxmin(), 'Subgroup']})\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Store results for later access\n",
    "        self.subgroup_results = results_df\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    # def _calculate_2sls_standard_errors(self, controls):\n",
    "    #     \"\"\"\n",
    "    #     Calculate correct 2SLS standard errors\n",
    "    #     (OLS on second stage gives incorrect SEs)\n",
    "    #     \"\"\"\n",
    "    #     # Get residuals from second stage\n",
    "    #     residuals = self.second_stage_results.resid\n",
    "        \n",
    "    #     # Calculate robust variance-covariance matrix\n",
    "    #     n = len(residuals)\n",
    "    #     k = len(self.second_stage_results.params)\n",
    "        \n",
    "    #     # Simple correction factor\n",
    "    #     correction = n / (n - k)\n",
    "        \n",
    "    #     # Store corrected standard errors\n",
    "    #     self.corrected_se = np.sqrt(np.diag(self.second_stage_results.cov_params()) * correction)\n",
    "    #     self.corrected_tvalues = self.second_stage_results.params / self.corrected_se\n",
    "    #     self.corrected_pvalues = 2 * (1 - stats.t.cdf(np.abs(self.corrected_tvalues), n - k))\n",
    "    \n",
    "    def _display_results(self):\n",
    "        \"\"\"Display 2SLS results\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TWO-STAGE LEAST SQUARES (2SLS) RESULTS\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create results table\n",
    "        results_df = pd.DataFrame({\n",
    "            'Coefficient': self.second_stage_results.params,\n",
    "            'Std Error': self.corrected_se,\n",
    "            't-statistic': self.corrected_tvalues,\n",
    "            'P-value': self.corrected_pvalues\n",
    "        })\n",
    "        \n",
    "        # Add confidence intervals\n",
    "        results_df['95% CI Lower'] = results_df['Coefficient'] - 1.96 * results_df['Std Error']\n",
    "        results_df['95% CI Upper'] = results_df['Coefficient'] + 1.96 * results_df['Std Error']\n",
    "        \n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CAUSAL INTERPRETATION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        clicks_coef = self.second_stage_results.params['Clicks_fitted']\n",
    "        clicks_se = self.corrected_se[1]  # Index 1 for Clicks_fitted (after constant)\n",
    "        clicks_pval = self.corrected_pvalues[1]\n",
    "        \n",
    "        print(f\"\\nCausal Effect of Clicks on Conversion Rate:\")\n",
    "        print(f\"  Coefficient (Œ≤): {clicks_coef:.6f}\")\n",
    "        print(f\"  Std. Error: {clicks_se:.6f}\")\n",
    "        print(f\"  95% CI: [{clicks_coef - 1.96*clicks_se:.6f}, {clicks_coef + 1.96*clicks_se:.6f}]\")\n",
    "        print(f\"  P-value: {clicks_pval:.4f}\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  A 1-unit increase in Clicks causes a {clicks_coef:.6f} change\")\n",
    "        print(f\"  in Conversion Rate (controlling for confounders)\")\n",
    "        \n",
    "        if clicks_pval < 0.05:\n",
    "            print(f\"  ‚úì Effect is statistically significant at 5% level\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Effect is NOT statistically significant at 5% level\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    def estimate_value_added(self):\n",
    "        \"\"\"\n",
    "        Step 4: Value-Added Estimation\n",
    "        \n",
    "        Estimate the incremental contribution of different ad features\n",
    "        after controlling for user characteristics and predicted clicks.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Group by Ad Type\n",
    "        if 'Ad_Type' in self.data.columns:\n",
    "            results['by_ad_type'] = self._group_value_added('Ad_Type')\n",
    "        \n",
    "        # Group by Ad Placement\n",
    "        if 'Ad_Placement' in self.data.columns:\n",
    "            results['by_ad_placement'] = self._group_value_added('Ad_Placement')\n",
    "        \n",
    "        # Group by Ad Topic\n",
    "        if 'Ad_Topic' in self.data.columns:\n",
    "            results['by_ad_topic'] = self._group_value_added('Ad_Topic')\n",
    "        \n",
    "        self._display_value_added(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _group_value_added(self, group_col):\n",
    "        \"\"\"Calculate value-added for a specific grouping variable\"\"\"\n",
    "        group_results = []\n",
    "        \n",
    "        for group in self.data[group_col].unique():\n",
    "            # Create indicator variable\n",
    "            indicator = (self.data[group_col] == group).astype(int)\n",
    "            \n",
    "            # Prepare regression with interaction\n",
    "            y = self.data['Conversion_Rate']\n",
    "            X = sm.add_constant(pd.DataFrame({\n",
    "                'Clicks_predicted': self.data['Clicks_predicted'],\n",
    "                'Age': self.data['Age'],\n",
    "                'Income': self.data['Income'],\n",
    "                'indicator': indicator,\n",
    "                'interaction': indicator * self.data['Clicks_predicted']\n",
    "            }))\n",
    "            \n",
    "            # Run OLS\n",
    "            try:\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                \n",
    "                group_results.append({\n",
    "                    'Group': str(group),\n",
    "                    'Intercept_Effect': f\"{model.params['indicator']:.6f}\",\n",
    "                    'Slope_Effect': f\"{model.params['interaction']:.6f}\",\n",
    "                    'P_value_Intercept': f\"{model.pvalues['indicator']:.4f}\",\n",
    "                    'P_value_Slope': f\"{model.pvalues['interaction']:.4f}\",\n",
    "                    'Significant': '‚úì' if model.pvalues['indicator'] < 0.05 or model.pvalues['interaction'] < 0.05 else '‚úó'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not estimate for {group}: {str(e)}\")\n",
    "        \n",
    "        return pd.DataFrame(group_results)\n",
    "    \n",
    "    def _display_value_added(self, results):\n",
    "        \"\"\"Display value-added results\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"VALUE-ADDED ESTIMATION RESULTS\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for key, df in results.items():\n",
    "            if len(df) > 0:\n",
    "                print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                print(df.to_string(index=False))\n",
    "                print(f\"{'-'*60}\\n\")\n",
    "\n",
    "    def run_chetty_value_added_analysis(self, subgroup_vars=None, split_method='time', \n",
    "                                         split_ratio=0.5, min_group_size=100):\n",
    "        \"\"\"\n",
    "        INTEGRATED PIPELINE: Combines subgroup analysis with Chetty's forecast bias framework.\n",
    "\n",
    "        This is the main method you should call. It properly chains:\n",
    "        1. analyze_subgroup_effects() on TRAINING data\n",
    "        2. Forecast validation on TESTING data  \n",
    "        3. Empirical Bayes shrinkage\n",
    "        4. Bias correction\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        subgroup_vars : list of str or dict\n",
    "            Variables to stratify by (same format as analyze_subgroup_effects)\n",
    "            Example: {'Income': [0, 30000, 60000, np.inf], 'Location': None}\n",
    "        split_method : str\n",
    "            'time' - split by Click_Time (chronological)\n",
    "            'random' - random split with seed\n",
    "        split_ratio : float\n",
    "            Proportion for training (default: 0.5)\n",
    "        min_group_size : int\n",
    "            Minimum observations per group in EACH split\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            Complete results including raw estimates, bias tests, and corrections\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTEGRATED CHETTY VALUE-ADDED ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Split method: {split_method}\")\n",
    "        print(f\"  Split ratio: {split_ratio:.1%} train / {1-split_ratio:.1%} test\")\n",
    "        print(f\"  Min group size: {min_group_size}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 1: SPLIT DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 1: DATA SPLITTING\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        original_data = self.data.copy()\n",
    "\n",
    "        if split_method == 'time':\n",
    "            self.data = self.data.sort_values('Click_Time')\n",
    "            split_idx = int(len(self.data) * split_ratio)\n",
    "            train_indices = self.data.index[:split_idx]\n",
    "            test_indices = self.data.index[split_idx:]\n",
    "\n",
    "            train_data = self.data.loc[train_indices].copy()\n",
    "            test_data = self.data.loc[test_indices].copy()\n",
    "\n",
    "            print(f\"‚úì Time-based split:\")\n",
    "            print(f\"  Training: {train_data['Click_Time'].min()} to {train_data['Click_Time'].max()}\")\n",
    "            print(f\"  Testing:  {test_data['Click_Time'].min()} to {test_data['Click_Time'].max()}\")\n",
    "\n",
    "        elif split_method == 'random':\n",
    "            train_data = self.data.sample(frac=split_ratio, random_state=42)\n",
    "            test_data = self.data.drop(train_data.index)\n",
    "            print(f\"‚úì Random split (seed=42)\")\n",
    "\n",
    "        print(f\"  Training N: {len(train_data):,}\")\n",
    "        print(f\"  Testing N:  {len(test_data):,}\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 2: RUN SUBGROUP ANALYSIS ON TRAINING DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 2: ESTIMATE VALUE-ADDED (Training Sample)\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        # Temporarily replace self.data with training data\n",
    "        self.data = train_data.copy()\n",
    "\n",
    "        # Run subgroup analysis (this uses the existing method)\n",
    "        train_results = self.analyze_subgroup_effects(\n",
    "            subgroup_vars=subgroup_vars,\n",
    "            min_subgroup_size=min_group_size\n",
    "        )\n",
    "\n",
    "        if train_results is None or len(train_results) == 0:\n",
    "            print(\"\\n‚úó No subgroups successfully estimated in training data\")\n",
    "            self.data = original_data  # Restore\n",
    "            return None\n",
    "\n",
    "        print(f\"\\n‚úì Estimated value-added for {len(train_results)} subgroups\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 3: VALIDATE FORECASTS IN TESTING DATA\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 3: FORECAST VALIDATION (Testing Sample)\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        # For each group, calculate mean outcome in test sample\n",
    "        test_outcomes = []\n",
    "\n",
    "        for _, row in train_results.iterrows():\n",
    "            var = row['Variable']\n",
    "            subgroup = row['Subgroup']\n",
    "\n",
    "            # Filter test data to this subgroup\n",
    "            if '_subgroup' in var:\n",
    "                # This was a binned continuous variable\n",
    "                # We need to recreate the bins\n",
    "                continue  # Skip for now - handle separately\n",
    "            else:\n",
    "                # Categorical variable\n",
    "                group_test = test_data[test_data[var] == subgroup]\n",
    "\n",
    "            if len(group_test) < min_group_size:\n",
    "                continue\n",
    "            \n",
    "            # Mean conversion rate in test sample\n",
    "            mean_outcome = group_test['Conversion_Rate'].mean()\n",
    "            n_test = len(group_test)\n",
    "\n",
    "            test_outcomes.append({\n",
    "                'Variable': var,\n",
    "                'Subgroup': subgroup,\n",
    "                'Value_Added_Train': row['Causal_Effect_Beta'],\n",
    "                'SE_Train': row['Std_Error'],\n",
    "                'N_Train': row['N'],\n",
    "                'Mean_Outcome_Test': mean_outcome,\n",
    "                'N_Test': n_test,\n",
    "                'First_Stage_F': row['First_Stage_F']\n",
    "            })\n",
    "\n",
    "        test_df = pd.DataFrame(test_outcomes)\n",
    "\n",
    "        if len(test_df) < 3:\n",
    "            print(f\"\\n‚úó Insufficient groups for validation (need ‚â•3, have {len(test_df)})\")\n",
    "            self.data = original_data\n",
    "            return None\n",
    "\n",
    "        print(f\"‚úì Validated {len(test_df)} groups in testing sample\")\n",
    "        print(f\"\\nTest Sample Statistics:\")\n",
    "        print(test_df[['Variable', 'Subgroup', 'Value_Added_Train', 'Mean_Outcome_Test', 'N_Test']].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 4: FORECAST UNBIASEDNESS TEST\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 4: FORECAST UNBIASEDNESS TEST\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        print(\"\\nRegression: Mean_Outcome_test = Œ± + Œ≤ * ValueAdded_train + Œµ\")\n",
    "        print(\"H‚ÇÄ: Œ≤ = 1 (unbiased forecasts)\")\n",
    "\n",
    "        # Run regression\n",
    "        X_forecast = sm.add_constant(test_df['Value_Added_Train'])\n",
    "        y_forecast = test_df['Mean_Outcome_Test']\n",
    "\n",
    "        # Weight by test sample size for precision\n",
    "        weights = np.sqrt(test_df['N_Test'])\n",
    "        forecast_model = sm.WLS(y_forecast, X_forecast, weights=weights).fit()\n",
    "\n",
    "        # Extract coefficients\n",
    "        alpha = forecast_model.params['const']\n",
    "        beta = forecast_model.params['Value_Added_Train']\n",
    "        beta_se = forecast_model.bse['Value_Added_Train']\n",
    "        r_squared = forecast_model.rsquared\n",
    "\n",
    "        # Test Œ≤ = 1\n",
    "        t_stat_bias = (beta - 1) / beta_se\n",
    "        p_val_bias = 2 * (1 - stats.t.cdf(abs(t_stat_bias), len(test_df) - 2))\n",
    "\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Œ± (intercept):     {alpha:.6f}\")\n",
    "        print(f\"  Œ≤ (slope):         {beta:.6f} (SE: {beta_se:.6f})\")\n",
    "        print(f\"  Prediction R¬≤:     {r_squared:.4f}\")\n",
    "        print(f\"\\n  Test H‚ÇÄ: Œ≤ = 1\")\n",
    "        print(f\"  t-statistic:       {t_stat_bias:.3f}\")\n",
    "        print(f\"  p-value:           {p_val_bias:.4f}\")\n",
    "\n",
    "        is_biased = p_val_bias < 0.05\n",
    "\n",
    "        if is_biased:\n",
    "            print(f\"\\n  ‚úó REJECT H‚ÇÄ: Significant forecast bias detected\")\n",
    "            if beta < 1:\n",
    "                bias_direction = \"OVERPREDICTION (regression to mean)\"\n",
    "                print(f\"    ‚Üí Training estimates overpredict test outcomes\")\n",
    "                print(f\"    ‚Üí Shrinkage is strongly recommended\")\n",
    "            else:\n",
    "                bias_direction = \"UNDERPREDICTION\"\n",
    "                print(f\"    ‚Üí Training estimates underpredict test outcomes\")\n",
    "        else:\n",
    "            bias_direction = \"MINIMAL\"\n",
    "            print(f\"\\n  ‚úì FAIL TO REJECT H‚ÇÄ: No significant bias\")\n",
    "            print(f\"    ‚Üí Training estimates predict test outcomes well\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 5: EMPIRICAL BAYES SHRINKAGE\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 5: EMPIRICAL BAYES SHRINKAGE\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        # Calculate variance components\n",
    "        va_estimates = train_results['Causal_Effect_Beta'].values\n",
    "        va_variances = train_results['Std_Error'].values ** 2\n",
    "\n",
    "        # Grand mean\n",
    "        mu = np.mean(va_estimates)\n",
    "\n",
    "        # Variance decomposition\n",
    "        var_total = np.var(va_estimates)\n",
    "        var_noise = np.mean(va_variances)\n",
    "        var_signal = max(0, var_total - var_noise)\n",
    "\n",
    "        # Reliability\n",
    "        reliability = var_signal / (var_signal + var_noise) if (var_signal + var_noise) > 0 else 0\n",
    "\n",
    "        print(f\"\\nVariance Decomposition:\")\n",
    "        print(f\"  Between-group variance (signal): {var_signal:.8f}\")\n",
    "        print(f\"  Within-group variance (noise):   {var_noise:.8f}\")\n",
    "        print(f\"  Total variance:                  {var_total:.8f}\")\n",
    "        print(f\"  Reliability (ŒªÃÑ):                 {reliability:.4f}\")\n",
    "\n",
    "        # Group-specific shrinkage\n",
    "        shrinkage_results = []\n",
    "\n",
    "        print(f\"\\nGroup-Specific Shrinkage:\")\n",
    "        print(f\"{'Variable':<15} {'Subgroup':<20} {'Raw VA':<12} {'Œª':<8} {'Shrunk VA':<12}\")\n",
    "        print(f\"{'-'*75}\")\n",
    "\n",
    "        for _, row in train_results.iterrows():\n",
    "            raw_va = row['Causal_Effect_Beta']\n",
    "            se = row['Std_Error']\n",
    "\n",
    "            # Shrinkage factor for this group\n",
    "            lambda_i = var_signal / (var_signal + se**2) if (var_signal + se**2) > 0 else 0\n",
    "\n",
    "            # Shrink toward grand mean\n",
    "            shrunk_va = mu + lambda_i * (raw_va - mu)\n",
    "\n",
    "            print(f\"{row['Variable']:<15} {str(row['Subgroup']):<20} {raw_va:>11.6f} {lambda_i:>7.4f} {shrunk_va:>11.6f}\")\n",
    "\n",
    "            shrinkage_results.append({\n",
    "                'Variable': row['Variable'],\n",
    "                'Subgroup': row['Subgroup'],\n",
    "                'Raw_VA': raw_va,\n",
    "                'Shrinkage_Factor': lambda_i,\n",
    "                'Shrunk_VA': shrunk_va,\n",
    "                'SE': se,\n",
    "                'N_Train': row['N']\n",
    "            })\n",
    "\n",
    "        shrinkage_df = pd.DataFrame(shrinkage_results)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 6: BIAS CORRECTION\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"STEP 6: FORECAST BIAS CORRECTION\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        # Bias-corrected estimates: (Shrunk_VA - Œ±) / Œ≤\n",
    "        shrinkage_df['Bias_Corrected_VA'] = (shrinkage_df['Shrunk_VA'] - alpha) / beta if beta != 0 else shrinkage_df['Shrunk_VA']\n",
    "\n",
    "        print(f\"\\nApplying correction: VA_final = (VA_shrunk - {alpha:.6f}) / {beta:.6f}\")\n",
    "        print(f\"\\nFinal Value-Added Estimates:\")\n",
    "\n",
    "        # Sort by bias-corrected VA\n",
    "        shrinkage_df = shrinkage_df.sort_values('Bias_Corrected_VA', ascending=False)\n",
    "\n",
    "        display_cols = ['Variable', 'Subgroup', 'Raw_VA', 'Shrunk_VA', 'Bias_Corrected_VA', 'N_Train']\n",
    "        print(\"\\n\" + shrinkage_df[display_cols].to_string(index=False))\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 7: SUMMARY AND INTERPRETATION\n",
    "        # =========================================================================\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SUMMARY: CORRECTIONS APPLIED\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Calculate average corrections\n",
    "        avg_shrinkage = (shrinkage_df['Shrunk_VA'] - shrinkage_df['Raw_VA']).abs().mean()\n",
    "        avg_bias_correction = (shrinkage_df['Bias_Corrected_VA'] - shrinkage_df['Shrunk_VA']).abs().mean()\n",
    "        avg_total_correction = (shrinkage_df['Bias_Corrected_VA'] - shrinkage_df['Raw_VA']).abs().mean()\n",
    "\n",
    "        print(f\"\\nAverage absolute corrections:\")\n",
    "        print(f\"  Shrinkage effect:      {avg_shrinkage:.6f}\")\n",
    "        print(f\"  Bias correction:       {avg_bias_correction:.6f}\")\n",
    "        print(f\"  Total correction:      {avg_total_correction:.6f}\")\n",
    "\n",
    "        print(f\"\\nDiagnostics:\")\n",
    "        print(f\"  Reliability:           {reliability:.4f}\")\n",
    "        print(f\"  Forecast bias (Œ≤):     {beta:.4f}\")\n",
    "        print(f\"  Prediction R¬≤:         {r_squared:.4f}\")\n",
    "        print(f\"  Bias type:             {bias_direction}\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        if reliability < 0.3:\n",
    "            print(\"  ‚ö† Low reliability - value-added estimates are very noisy\")\n",
    "            print(\"    ‚Üí Collect more data or use coarser groupings\")\n",
    "        elif reliability < 0.7:\n",
    "            print(\"  ‚úì Moderate reliability - shrinkage is important\")\n",
    "        else:\n",
    "            print(\"  ‚úì‚úì High reliability - raw estimates are fairly trustworthy\")\n",
    "\n",
    "        if is_biased:\n",
    "            print(f\"  ‚ö† Forecast bias detected - bias correction is essential\")\n",
    "        else:\n",
    "            print(f\"  ‚úì Minimal forecast bias - raw estimates predict well\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # PACKAGE RESULTS\n",
    "        # =========================================================================\n",
    "\n",
    "        results = {\n",
    "            'split_info': {\n",
    "                'method': split_method,\n",
    "                'n_train': len(train_data),\n",
    "                'n_test': len(test_data)\n",
    "            },\n",
    "            'train_estimates': train_results,\n",
    "            'test_outcomes': test_df,\n",
    "            'forecast_bias': {\n",
    "                'alpha': alpha,\n",
    "                'beta': beta,\n",
    "                'beta_se': beta_se,\n",
    "                'p_value': p_val_bias,\n",
    "                'r_squared': r_squared,\n",
    "                'is_biased': is_biased,\n",
    "                'direction': bias_direction\n",
    "            },\n",
    "            'shrinkage': {\n",
    "                'reliability': reliability,\n",
    "                'var_signal': var_signal,\n",
    "                'var_noise': var_noise,\n",
    "                'mu': mu\n",
    "            },\n",
    "            'final_estimates': shrinkage_df\n",
    "        }\n",
    "\n",
    "        # Store for later access\n",
    "        self.chetty_results = results\n",
    "\n",
    "        # Restore original data\n",
    "        self.data = original_data\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTEGRATED ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def compare_estimation_approaches(self):\n",
    "        \"\"\"\n",
    "        Create side-by-side comparison of estimation approaches.\n",
    "        Must be called after run_chetty_value_added_analysis().\n",
    "\n",
    "        Compares:\n",
    "        1. Raw 2SLS estimates (training sample)\n",
    "        2. Shrunk estimates (Empirical Bayes)\n",
    "        3. Bias-corrected estimates (Full Chetty method)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        comparison_df : pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'chetty_results'):\n",
    "            print(\"\\n‚ö† Must run run_chetty_value_added_analysis() first\")\n",
    "            return None\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARISON: ESTIMATION APPROACHES\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        final = self.chetty_results['final_estimates']\n",
    "\n",
    "        # Calculate changes\n",
    "        final['Change_from_Shrinkage'] = final['Shrunk_VA'] - final['Raw_VA']\n",
    "        final['Change_from_Bias_Corr'] = final['Bias_Corrected_VA'] - final['Shrunk_VA']\n",
    "        final['Total_Change'] = final['Bias_Corrected_VA'] - final['Raw_VA']\n",
    "        final['Pct_Change'] = (final['Total_Change'] / final['Raw_VA'].abs()) * 100\n",
    "\n",
    "        display_cols = [\n",
    "            'Variable', 'Subgroup',\n",
    "            'Raw_VA', 'Shrunk_VA', 'Bias_Corrected_VA',\n",
    "            'Total_Change', 'Pct_Change'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n\" + final[display_cols].to_string(index=False))\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "        print(f\"\\nMean absolute change:\")\n",
    "        print(f\"  From shrinkage:        {final['Change_from_Shrinkage'].abs().mean():.6f}\")\n",
    "        print(f\"  From bias correction:  {final['Change_from_Bias_Corr'].abs().mean():.6f}\")\n",
    "        print(f\"  Total:                 {final['Total_Change'].abs().mean():.6f}\")\n",
    "\n",
    "        print(f\"\\nMean % change:           {final['Pct_Change'].abs().mean():.1f}%\")\n",
    "\n",
    "        # Which groups changed most?\n",
    "        print(f\"\\nGroups with largest corrections:\")\n",
    "        top_changes = final.nlargest(3, 'Total_Change', keep='all')[['Subgroup', 'Raw_VA', 'Bias_Corrected_VA', 'Total_Change']]\n",
    "        print(top_changes.to_string(index=False))\n",
    "\n",
    "        return final\n",
    "\n",
    "    ### WITH Subgroup analysis\n",
    "    def run_complete_analysis(self, model_type='stacking', include_interactions=False, \n",
    "                            use_enhanced_features=True, analyze_subgroups=False,\n",
    "                            subgroup_vars=None, min_subgroup_size=100):\n",
    "        \"\"\"\n",
    "        ENHANCED: Run the complete causal inference pipeline with strong instruments.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            'stacking' (RECOMMENDED), 'rf', or 'gb'\n",
    "        include_interactions : bool\n",
    "            Whether to include interactions in 2SLS\n",
    "        use_enhanced_features : bool\n",
    "            Whether to use enhanced feature engineering (strongly recommended)\n",
    "        analyze_subgroups : bool\n",
    "            Whether to run stratified 2SLS to identify heterogeneous treatment effects\n",
    "        subgroup_vars : list of str or dict, optional\n",
    "            Variables to stratify by when analyze_subgroups=True\n",
    "            Example: ['Location', 'Ad_Type'] or \n",
    "                     {'Income': [0, 30000, 60000, np.inf], 'Age': [0, 35, 50, 65, np.inf]}\n",
    "        min_subgroup_size : int\n",
    "            Minimum observations required per subgroup (default: 100)\n",
    "\n",
    "        Pipeline:\n",
    "        1. Engineer time features\n",
    "        2. Encode categorical variables\n",
    "        3. Create ML instrument (with optional enhanced features)\n",
    "        4. Run 2SLS\n",
    "        5. [OPTIONAL] Analyze subgroup effects\n",
    "        6. Estimate value-added\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ENHANCED CAUSAL AD CONVERSION ANALYSIS PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Model type: {model_type.upper()}\")\n",
    "        print(f\"  Enhanced features: {'YES' if use_enhanced_features else 'NO'}\")\n",
    "        print(f\"  Include interactions in 2SLS: {'YES' if include_interactions else 'NO'}\")\n",
    "        print(f\"  Analyze subgroups: {'YES' if analyze_subgroups else 'NO'}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        self.engineer_time_features()\n",
    "        print(\"\\n‚úì Time features engineered\")\n",
    "\n",
    "        self.encode_categorical_features()\n",
    "        print(\"‚úì Categorical variables encoded\")\n",
    "\n",
    "        self.create_ml_instrument(\n",
    "            model_type=model_type,\n",
    "            use_enhanced_features=use_enhanced_features\n",
    "        )\n",
    "        print(\"‚úì ML instrument created\")\n",
    "\n",
    "        self.run_2sls(include_interactions=include_interactions)\n",
    "        print(\"‚úì 2SLS estimation complete\")\n",
    "\n",
    "        # NEW: Optional subgroup analysis\n",
    "        if analyze_subgroups:\n",
    "            subgroup_results = self.analyze_subgroup_effects(\n",
    "                subgroup_vars=subgroup_vars,\n",
    "                min_subgroup_size=min_subgroup_size\n",
    "            )\n",
    "            print(\"‚úì Subgroup effects analysis complete\")\n",
    "\n",
    "        value_added_results = self.estimate_value_added()\n",
    "        print(\"‚úì Value-added estimation complete\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANALYSIS COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return self\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE WITH COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def compare_instrument_approaches(data):\n",
    "    \"\"\"\n",
    "    Compare weak vs strong instruments to demonstrate improvement\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON: WEAK vs STRONG INSTRUMENTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Approach 1: Basic features, single model (WEAK)\n",
    "    print(\"\\n\\n\" + \"üî¥ APPROACH 1: BASIC (LIKELY WEAK)\")\n",
    "    print(\"=\"*70)\n",
    "    analyzer_weak = CausalAdAnalyzer(data.copy())\n",
    "    analyzer_weak.engineer_time_features()\n",
    "    analyzer_weak.encode_categorical_features()\n",
    "    analyzer_weak.create_ml_instrument(\n",
    "        model_type='rf',\n",
    "        use_enhanced_features=False  # No enhanced features\n",
    "    )\n",
    "    \n",
    "    # Approach 2: Enhanced features, stacking ensemble (STRONG)\n",
    "    print(\"\\n\\n\" + \"üü¢ APPROACH 2: ENHANCED (STRONG)\")\n",
    "    print(\"=\"*70)\n",
    "    analyzer_strong = CausalAdAnalyzer(data.copy())\n",
    "    analyzer_strong.engineer_time_features()\n",
    "    analyzer_strong.encode_categorical_features()\n",
    "    analyzer_strong.create_ml_instrument(\n",
    "        model_type='stacking',\n",
    "        use_enhanced_features=True  # Enhanced features\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract F-statistics for comparison\n",
    "    z_weak = analyzer_weak.data['Clicks_predicted'].values\n",
    "    d_weak = analyzer_weak.data['Clicks'].values\n",
    "    corr_weak = np.corrcoef(z_weak, d_weak)[0, 1]\n",
    "    f_weak = (corr_weak**2 / (1 - corr_weak**2)) * (len(d_weak) - 2)\n",
    "    \n",
    "    z_strong = analyzer_strong.data['Clicks_predicted'].values\n",
    "    d_strong = analyzer_strong.data['Clicks'].values\n",
    "    ss_tot = np.sum((d_strong - d_strong.mean())**2)\n",
    "    ss_res = np.sum((d_strong - z_strong)**2)\n",
    "    r2_strong = 1 - (ss_res / ss_tot)\n",
    "    n = len(d_strong)\n",
    "    k = len([col for col in analyzer_strong.data.columns if 'x' in col.lower() or 'squared' in col.lower()]) + 9\n",
    "    f_strong = (r2_strong / 1) / ((1 - r2_strong) / (n - k - 1))\n",
    "    \n",
    "    print(f\"\\nApproach 1 (Basic):\")\n",
    "    print(f\"  F-statistic: {f_weak:.2f}\")\n",
    "    print(f\"  Status: {'‚úó WEAK' if f_weak < 10 else '‚úì STRONG'}\")\n",
    "    \n",
    "    print(f\"\\nApproach 2 (Enhanced):\")\n",
    "    print(f\"  F-statistic: {f_strong:.2f}\")\n",
    "    print(f\"  Status: {'‚úó WEAK' if f_strong < 10 else '‚úì STRONG'}\")\n",
    "    \n",
    "    improvement = ((f_strong - f_weak) / f_weak) * 100\n",
    "    print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDATION: Use Approach 2 (Enhanced) for reliable causal inference\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return analyzer_weak, analyzer_strong\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # reading in the df\n",
    "    df = pd.read_csv('../datasets/project/Dataset_Ads.csv')\n",
    "\n",
    "    # # default setting with stacking, and enhanced machine learning.\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "\n",
    "    # # # Run complete pipeline\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',          # Use stacking ensemble\n",
    "    #     include_interactions=True,       # Include interactions in 2SLS\n",
    "    #     use_enhanced_features=True      # Use enhanced feature engineering\n",
    "    # )\n",
    "\n",
    "    # compares the methods, weak vs strong.\n",
    "    # analyzer_weak, analyzer_strong = compare_instrument_approaches(df)\n",
    "\n",
    "    # 2sls subgroup effect section\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "\n",
    "    # analyzer.run_2sls(include_interactions=True)\n",
    "\n",
    "    # # NEW: Analyze subgroup effects\n",
    "    # subgroup_results = analyzer.analyze_subgroup_effects(\n",
    "    #     subgroup_vars={\n",
    "    #         'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "    #         'Age': [0, 35, 50, 65, np.inf],\n",
    "    #         'Location': None,  # Categorical\n",
    "    #         'Ad_Type': None    # Categorical\n",
    "    #     },\n",
    "    #     min_subgroup_size=100\n",
    "    # )\n",
    "\n",
    "    # # Then continue with value-added\n",
    "    # analyzer.estimate_value_added()\n",
    "\n",
    "    # # Example 1: Run WITHOUT subgroup analysis (default behavior)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True\n",
    "    # )\n",
    "\n",
    "    # Example 2: Run WITH subgroup analysis (using default subgroups)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True  # Toggle this on\n",
    "    # )\n",
    "\n",
    "    # # Example 3: Run WITH subgroup analysis (custom subgroups)\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     include_interactions=True,\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True,\n",
    "    #     subgroup_vars={\n",
    "    #         'Income': [0, 35000, 55000, 75000, np.inf],  # Custom income bins\n",
    "    #         'Age': [0, 40, 60, np.inf],                   # Custom age bins\n",
    "    #         'Location': None,                              # Use as-is\n",
    "    #         'Ad_Type': None                                # Use as-is\n",
    "    #     },\n",
    "    #     min_subgroup_size=150  # Require at least 150 obs per subgroup\n",
    "    # )\n",
    "\n",
    "    # # Example 4: Only specific categorical subgroups\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     use_enhanced_features=True,\n",
    "    #     analyze_subgroups=True,\n",
    "    #     subgroup_vars=['Location', 'Ad_Type', 'Gender']  # Just these three\n",
    "    # )\n",
    "\n",
    "    # Trying to add Raj Chetty forecast bias's approach.\n",
    "    # After running your analysis This would run independently from the stratified gorup results..\n",
    "    # analyzer = CausalAdAnalyzer(df)\n",
    "    # analyzer.run_complete_analysis(\n",
    "    #     model_type='stacking',\n",
    "    #     use_enhanced_features=True\n",
    "    # )\n",
    "\n",
    "    # # Run Chetty-style forecast bias test\n",
    "    # bias_results = analyzer.test_forecast_bias(\n",
    "    #     split_method='time',      # or 'random'\n",
    "    #     split_ratio=0.5,\n",
    "    #     subgroup_var='Ad_Type',   # or 'Ad_Placement', 'Location'\n",
    "    #     min_group_size=50\n",
    "    # )\n",
    "\n",
    "    # df = generate_example_data(n=2000)    # # Compare methods\n",
    "    # comparison = analyzer.compare_value_added_methods(subgroup_var='Ad_Type')\n",
    "    # Initialize and run basic pipeline\n",
    "    analyzer = CausalAdAnalyzer(df)\n",
    "    analyzer.engineer_time_features()\n",
    "    analyzer.encode_categorical_features()\n",
    "    analyzer.create_ml_instrument(model_type='stacking', use_enhanced_features=True)\n",
    "\n",
    "    # Run integrated Chetty analysis\n",
    "    results = analyzer.run_chetty_value_added_analysis(\n",
    "        subgroup_vars={\n",
    "            'Income': [0, 30000, 50000, 70000, np.inf],\n",
    "            'Age': [0, 35, 50, 65, np.inf],\n",
    "            'Location': None,\n",
    "            'Ad_Type': None\n",
    "        },\n",
    "        split_method='time',      # or 'random'\n",
    "        split_ratio=0.5,\n",
    "        min_group_size=100\n",
    "    )\n",
    "\n",
    "    # Compare approaches\n",
    "    comparison = analyzer.compare_estimation_approaches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
